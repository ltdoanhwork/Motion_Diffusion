{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af10b203",
   "metadata": {},
   "source": [
    "### Chu·∫©n b·ªã data\n",
    "\n",
    "```text\n",
    "datasets/\n",
    "‚îú‚îÄ‚îÄ BEAT/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 1/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 2/\n",
    "|   ‚îú‚îÄ‚îÄ ...\n",
    "|   ‚îî‚îÄ‚îÄ ...\n",
    "‚îú‚îÄ‚îÄ BEAT_numpy/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ npy/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ txt/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9e75213",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/BEAT\"\n",
    "out_dir = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/BEAT_numpy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918dc43a",
   "metadata": {},
   "source": [
    "### T√≠nh mean/std cho to√†n b·ªô dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991e8b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/step1_fit_scaler.py\" --parent-dir \"{base_dir}\" --start 1 --end 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3acb60",
   "metadata": {},
   "source": [
    "### Chuy·ªÉn dataset bvh sang npy theo t·ª´ng segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98171b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/preprocess_data.py\" --parent-dir \"{base_dir}\" --out-root \"{out_dir}\" --start 1 --end 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7d2957",
   "metadata": {},
   "source": [
    "### Train MotionDiffuse nh∆∞ b√¨nh th∆∞·ªùng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6c81c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python \"/home/serverai/ltdoanh/Motion_Diffusion/tools/train.py\" --dataset_name beat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf77ac5",
   "metadata": {},
   "source": [
    "### Train VQ-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ed13924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-19 04:10:51.590459: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Using device: cuda:0\n",
      "[INFO] Loading stats from /home/serverai/ltdoanh/Motion_Diffusion/global_pipeline.pkl\n",
      "[INFO] Mean and Std loaded successfully from pipeline.\n",
      "[INFO] ./datasets/BEAT_numpy/train.txt already exists\n",
      "Loading motion‚Äëtext pairs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 223/223 [00:00<00:00, 1349.61it/s]\n",
      "Train dataset: 223 samples\n",
      "VQ-VAE Model parameters: 27.57M\n",
      "Starting VQ-VAE Training...\n",
      "Total Epochs: 10, Total Iters: 30\n",
      "Epoch: 000 | Iter: 000001 | Loss: 0.0883 | Rec: 0.0881 | Vel: 0.0002 | Commit: 0.0006 | LR: 0.000000\n",
      "Epoch: 000 | Iter: 000002 | Loss: 0.0883 | Rec: 0.0882 | Vel: 0.0002 | Commit: 0.0006 | LR: 0.000000\n",
      "Epoch: 000 | Iter: 000003 | Loss: 0.0883 | Rec: 0.0882 | Vel: 0.0002 | Commit: 0.0005 | LR: 0.000000\n",
      "[INFO] Validation skipped (val_loader=None)\n",
      "Epoch: 001 | Iter: 000004 | Loss: 0.0883 | Rec: 0.0882 | Vel: 0.0002 | Commit: 0.0005 | LR: 0.000000\n",
      "Epoch: 001 | Iter: 000005 | Loss: 0.0884 | Rec: 0.0883 | Vel: 0.0002 | Commit: 0.0005 | LR: 0.000000\n",
      "Epoch: 001 | Iter: 000006 | Loss: 0.0880 | Rec: 0.0879 | Vel: 0.0002 | Commit: 0.0006 | LR: 0.000000\n",
      "[INFO] Validation skipped (val_loader=None)\n",
      "Epoch: 002 | Iter: 000007 | Loss: 0.0881 | Rec: 0.0879 | Vel: 0.0002 | Commit: 0.0006 | LR: 0.000000\n",
      "Epoch: 002 | Iter: 000008 | Loss: 0.0881 | Rec: 0.0880 | Vel: 0.0002 | Commit: 0.0007 | LR: 0.000000\n",
      "Epoch: 002 | Iter: 000009 | Loss: 0.0880 | Rec: 0.0879 | Vel: 0.0002 | Commit: 0.0009 | LR: 0.000000\n",
      "[INFO] Validation skipped (val_loader=None)\n",
      "Epoch: 003 | Iter: 000010 | Loss: 0.0879 | Rec: 0.0877 | Vel: 0.0002 | Commit: 0.0010 | LR: 0.000000\n",
      "Epoch: 003 | Iter: 000011 | Loss: 0.0880 | Rec: 0.0878 | Vel: 0.0003 | Commit: 0.0011 | LR: 0.000000\n",
      "Epoch: 003 | Iter: 000012 | Loss: 0.0875 | Rec: 0.0873 | Vel: 0.0003 | Commit: 0.0013 | LR: 0.000000\n",
      "[INFO] Validation skipped (val_loader=None)\n",
      "Epoch: 004 | Iter: 000013 | Loss: 0.0874 | Rec: 0.0872 | Vel: 0.0003 | Commit: 0.0015 | LR: 0.000000\n",
      "Epoch: 004 | Iter: 000014 | Loss: 0.0867 | Rec: 0.0865 | Vel: 0.0003 | Commit: 0.0018 | LR: 0.000000\n",
      "Epoch: 004 | Iter: 000015 | Loss: 0.0863 | Rec: 0.0861 | Vel: 0.0004 | Commit: 0.0019 | LR: 0.000000\n",
      "[INFO] Validation skipped (val_loader=None)\n",
      "Epoch: 005 | Iter: 000016 | Loss: 0.0855 | Rec: 0.0852 | Vel: 0.0004 | Commit: 0.0025 | LR: 0.000000\n",
      "Epoch: 005 | Iter: 000017 | Loss: 0.0844 | Rec: 0.0841 | Vel: 0.0005 | Commit: 0.0033 | LR: 0.000000\n",
      "Epoch: 005 | Iter: 000018 | Loss: 0.0828 | Rec: 0.0824 | Vel: 0.0006 | Commit: 0.0041 | LR: 0.000000\n",
      "[INFO] Validation skipped (val_loader=None)\n",
      "Epoch: 006 | Iter: 000019 | Loss: 0.0810 | Rec: 0.0805 | Vel: 0.0008 | Commit: 0.0055 | LR: 0.000000\n",
      "Epoch: 006 | Iter: 000020 | Loss: 0.0794 | Rec: 0.0788 | Vel: 0.0010 | Commit: 0.0068 | LR: 0.000000\n",
      "Epoch: 006 | Iter: 000021 | Loss: 0.0797 | Rec: 0.0788 | Vel: 0.0013 | Commit: 0.0092 | LR: 0.000000\n",
      "[INFO] Validation skipped (val_loader=None)\n",
      "Epoch: 007 | Iter: 000022 | Loss: 0.0812 | Rec: 0.0802 | Vel: 0.0015 | Commit: 0.0107 | LR: 0.000000\n",
      "Epoch: 007 | Iter: 000023 | Loss: 0.0825 | Rec: 0.0814 | Vel: 0.0017 | Commit: 0.0111 | LR: 0.000000\n",
      "Epoch: 007 | Iter: 000024 | Loss: 0.0835 | Rec: 0.0824 | Vel: 0.0019 | Commit: 0.0121 | LR: 0.000000\n",
      "[INFO] Validation skipped (val_loader=None)\n",
      "Epoch: 008 | Iter: 000025 | Loss: 0.0829 | Rec: 0.0817 | Vel: 0.0019 | Commit: 0.0124 | LR: 0.000001\n",
      "Epoch: 008 | Iter: 000026 | Loss: 0.0814 | Rec: 0.0802 | Vel: 0.0018 | Commit: 0.0123 | LR: 0.000001\n",
      "Epoch: 008 | Iter: 000027 | Loss: 0.0784 | Rec: 0.0774 | Vel: 0.0017 | Commit: 0.0076 | LR: 0.000001\n",
      "[INFO] Validation skipped (val_loader=None)\n",
      "Epoch: 009 | Iter: 000028 | Loss: 0.0752 | Rec: 0.0743 | Vel: 0.0015 | Commit: 0.0062 | LR: 0.000001\n",
      "Epoch: 009 | Iter: 000029 | Loss: 0.0727 | Rec: 0.0719 | Vel: 0.0013 | Commit: 0.0052 | LR: 0.000001\n",
      "Epoch: 009 | Iter: 000030 | Loss: 0.0718 | Rec: 0.0712 | Vel: 0.0011 | Commit: 0.0046 | LR: 0.000001\n",
      "[INFO] Validation skipped (val_loader=None)\n"
     ]
    }
   ],
   "source": [
    "!python \"/home/serverai/ltdoanh/Motion_Diffusion/tools/train_vq.py\" --dataset_name beat --codebook_size 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab74aaf",
   "metadata": {},
   "source": [
    "### Train MotionDiffuse tr√™n Latent Space do VQ-VAE ·ªü tr√™n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cf31078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-19 04:16:31.416993: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Using device: cuda:0\n",
      "[INFO] Mean and Std extracted successfully from pipeline.\n",
      "[INFO] Mean and Std loaded successfully from .pkl file.\n",
      "Loading motion‚Äëtext pairs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 223/223 [00:02<00:00, 91.15it/s]\n",
      "Train dataset: 223 samples\n",
      "Loading VQ-VAE from ./checkpoints/beat/VQVAE_BEAT/model/finest.tar\n",
      "VQ-VAE loaded successfully\n",
      "VQ-VAE frozen\n",
      "/srv/conda/envs/serverai/layout/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "VQLatentDiffusion initialized:\n",
      "  - Input features (latent): 512\n",
      "  - Latent sequence length: 45\n",
      "  - Transformer latent dim: 512\n",
      "  - Use continuous latent: True\n",
      "Model parameters:\n",
      "  Total: 266.18M\n",
      "  Trainable: 87.33M\n",
      "Diffusion steps: 1000\n",
      "==================================================\n",
      "Starting VQ Latent Diffusion Training\n",
      "==================================================\n",
      "Epochs: 10\n",
      "Batches per epoch: 3\n",
      "Total steps: 30\n",
      "==================================================\n",
      "Epoch: 000 | Step: 000001 | Loss: 1.0008 | LR: 0.00019946\n",
      "Epoch: 000 | Step: 000002 | Loss: 1.0108 | LR: 0.00019784\n",
      "Epoch: 000 | Step: 000003 | Loss: 1.0074 | LR: 0.00019515\n",
      "Checkpoint saved to ./checkpoints/beat/vq_diffusion/model/epoch_0000.pt\n",
      "Generating samples at epoch 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [01:29<00:00, 11.23it/s]\n",
      "Generated samples shape: torch.Size([4, 360, 264])\n",
      "Epoch: 001 | Step: 000004 | Loss: 0.9985 | LR: 0.00019144\n",
      "Epoch: 001 | Step: 000005 | Loss: 0.9988 | LR: 0.00018674\n",
      "Epoch: 001 | Step: 000006 | Loss: 0.9981 | LR: 0.00018109\n",
      "Epoch: 002 | Step: 000007 | Loss: 0.9976 | LR: 0.00017457\n",
      "Epoch: 002 | Step: 000008 | Loss: 0.9975 | LR: 0.00016724\n",
      "Epoch: 002 | Step: 000009 | Loss: 0.9966 | LR: 0.00015919\n",
      "Epoch: 003 | Step: 000010 | Loss: 0.9939 | LR: 0.00015050\n",
      "Epoch: 003 | Step: 000011 | Loss: 0.9957 | LR: 0.00014127\n",
      "Epoch: 003 | Step: 000012 | Loss: 0.9946 | LR: 0.00013159\n",
      "Epoch: 004 | Step: 000013 | Loss: 0.9942 | LR: 0.00012158\n",
      "Epoch: 004 | Step: 000014 | Loss: 0.9935 | LR: 0.00011135\n",
      "Epoch: 004 | Step: 000015 | Loss: 0.9926 | LR: 0.00010100\n",
      "Epoch: 005 | Step: 000016 | Loss: 0.9888 | LR: 0.00009065\n",
      "Epoch: 005 | Step: 000017 | Loss: 0.9892 | LR: 0.00008042\n",
      "Epoch: 005 | Step: 000018 | Loss: 0.9897 | LR: 0.00007041\n",
      "Checkpoint saved to ./checkpoints/beat/vq_diffusion/model/epoch_0005.pt\n",
      "Epoch: 006 | Step: 000019 | Loss: 0.9895 | LR: 0.00006073\n",
      "Epoch: 006 | Step: 000020 | Loss: 0.9894 | LR: 0.00005150\n",
      "Epoch: 006 | Step: 000021 | Loss: 0.9895 | LR: 0.00004281\n",
      "Epoch: 007 | Step: 000022 | Loss: 0.9890 | LR: 0.00003476\n",
      "Epoch: 007 | Step: 000023 | Loss: 0.9890 | LR: 0.00002743\n",
      "Epoch: 007 | Step: 000024 | Loss: 0.9894 | LR: 0.00002091\n",
      "Epoch: 008 | Step: 000025 | Loss: 0.9888 | LR: 0.00001526\n",
      "Epoch: 008 | Step: 000026 | Loss: 0.9867 | LR: 0.00001056\n",
      "Epoch: 008 | Step: 000027 | Loss: 0.9874 | LR: 0.00000685\n",
      "Epoch: 009 | Step: 000028 | Loss: 0.9876 | LR: 0.00000416\n",
      "Epoch: 009 | Step: 000029 | Loss: 0.9881 | LR: 0.00000254\n",
      "Epoch: 009 | Step: 000030 | Loss: 0.9878 | LR: 0.00000200\n",
      "==================================================\n",
      "Training completed!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "!python \"/home/serverai/ltdoanh/Motion_Diffusion/tools/train_vq_diffusion.py\" --dataset_name beat --vqvae_name VQVAE_BEAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e1052a",
   "metadata": {},
   "source": [
    "### Evaluation - ƒêang fix l·ªói ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca744de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python \"/home/serverai/ltdoanh/Motion_Diffusion/run_evaluation.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9950bdf",
   "metadata": {},
   "source": [
    "### Visual data b·∫±ng mean/std chu·∫©n t√≠nh t·ª´ b·ªô d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9f0d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Setup path\n",
    "PYOM_DIR = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/pymo\"\n",
    "if PYOM_DIR not in sys.path:\n",
    "    sys.path.insert(0, PYOM_DIR)\n",
    "\n",
    "from pymo.parsers import BVHParser\n",
    "from pymo.preprocessing import *\n",
    "from pymo.viz_tools import draw_stickfigure, draw_stickfigure3d\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "\n",
    "# ===== CELL 2: Load Pipeline =====\n",
    "pipeline_path = \"/home/serverai/ltdoanh/Motion_Diffusion/global_pipeline.pkl\"\n",
    "\n",
    "print(f\"üì¶ Loading pipeline from: {pipeline_path}\")\n",
    "\n",
    "pipeline = joblib.load(pipeline_path)\n",
    "print(f\"‚úÖ Pipeline loaded!\")\n",
    "print(f\"   Mean shape: {pipeline.named_steps['stdscale'].data_mean_.shape}, First 5 values: {pipeline.named_steps['stdscale'].data_mean_[:5]}\")\n",
    "print(f\"   Std shape: {pipeline.named_steps['stdscale'].data_std_.shape},  First 5 values: {pipeline.named_steps['stdscale'].data_std_[:5]}\")\n",
    "\n",
    "# ===== CELL 3: Load and Visualize Motion =====\n",
    "npy_path = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/BEAT_numpy/npy/1/1_wayne_0_1_1_sentence_000.npy\"\n",
    "frame = 50\n",
    "\n",
    "print(f\"\\nüé¨ Visualizing: {os.path.basename(npy_path)}\")\n",
    "print(f\"   Frame: {frame}\")\n",
    "\n",
    "# Load motion data\n",
    "motion_data = np.load(npy_path)\n",
    "print(f\"   Motion shape: {motion_data.shape}\")\n",
    "\n",
    "# Inverse transform\n",
    "print(\"   Performing inverse transform...\")\n",
    "reconstructed = pipeline.inverse_transform([motion_data])\n",
    "print(f\"   ‚úÖ Reconstructed shape: {reconstructed[0].values.shape}\")\n",
    "\n",
    "# Visualize 2D\n",
    "print(\"\\n   Creating 2D visualization...\")\n",
    "fig1 = plt.figure(figsize=(10, 8))\n",
    "draw_stickfigure(reconstructed[0], frame=frame)\n",
    "plt.title(f\"2D Stick Figure - Frame {frame}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"   ‚úÖ 2D plot displayed!\")\n",
    "\n",
    "# Visualize 3D\n",
    "print(\"\\n   Creating 3D visualization...\")\n",
    "fig2 = plt.figure(figsize=(10, 8))\n",
    "draw_stickfigure3d(reconstructed[0], frame=frame)\n",
    "plt.title(f\"3D Stick Figure - Frame {frame}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"   ‚úÖ 3D plot displayed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9286cdc0",
   "metadata": {},
   "source": [
    "### Visual data b·∫±ng mean/std t·ª´ model ƒë∆∞·ª£c hu·∫•n luy·ªán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65c2f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "\n",
    "# Setup path\n",
    "PYOM_DIR = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/pymo\"\n",
    "if PYOM_DIR not in sys.path:\n",
    "    sys.path.insert(0, PYOM_DIR)\n",
    "\n",
    "from pymo.parsers import BVHParser\n",
    "from pymo.preprocessing import *\n",
    "from pymo.viz_tools import draw_stickfigure, draw_stickfigure3d\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "\n",
    "# ===== CREATE PIPELINE WITH META =====\n",
    "print(\"\\nüî® Creating pipeline from scratch...\")\n",
    "\n",
    "# 1. Create empty pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('param', MocapParameterizer('position')),\n",
    "    ('rcpn', RootCentricPositionNormalizer()),\n",
    "    ('delta', RootTransformer('abdolute_translation_deltas')),\n",
    "    ('const', ConstantsRemover()),\n",
    "    ('np', Numpyfier()),\n",
    "    ('down', DownSampler(2)),\n",
    "    ('stdscale', ListStandardScaler())\n",
    "])\n",
    "\n",
    "# 2. Fit pipeline on sample BVH (to learn structure)\n",
    "bvh_sample_path = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/BEAT/1/1_wayne_0_1_1.bvh\"\n",
    "print(f\"   Fitting on BVH sample: {os.path.basename(bvh_sample_path)}\")\n",
    "\n",
    "parser = BVHParser()\n",
    "parsed_data = parser.parse(bvh_sample_path)\n",
    "pipeline.fit([parsed_data])\n",
    "print(\"   ‚úÖ Pipeline fitted (structure learned)\")\n",
    "\n",
    "# 3. Load mean/std from meta directory\n",
    "# meta_dir = \"/home/serverai/ltdoanh/Motion_Diffusion/checkpoints/beat/test/meta\"\n",
    "# mean_path = os.path.join(meta_dir, \"mean.npy\")\n",
    "# std_path = os.path.join(meta_dir, \"std.npy\")\n",
    "ckpt_path = \"/home/serverai/ltdoanh/Motion_Diffusion/checkpoints/beat/vq_diffusion/model/epoch_0005.pt\"  # Thay ƒë∆∞·ªùng d·∫´n file c·ªßa b·∫°n v√†o ƒë√¢y\n",
    "\n",
    "try:\n",
    "    # 2. Load checkpoint\n",
    "    # map_location='cpu' gi√∫p tr√°nh l·ªói n·∫øu m√°y b·∫°n kh√¥ng c√≥ GPU gi·ªëng l√∫c train\n",
    "    checkpoint = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
    "\n",
    "    # 3. Ki·ªÉm tra v√† l·∫•y mean, std\n",
    "    if 'mean' in checkpoint and 'std' in checkpoint:\n",
    "        mean = checkpoint['mean']\n",
    "        std = checkpoint['std']\n",
    "\n",
    "        print(\"--- ƒê√£ t√¨m th·∫•y Mean v√† Std ---\")\n",
    "        print(f\"Shape c·ªßa Mean: {mean.shape}\")\n",
    "        print(f\"Shape c·ªßa Std: {std.shape}\")\n",
    "        \n",
    "        # In th·ª≠ v√†i gi√° tr·ªã ƒë·∫ßu\n",
    "        print(f\"Mean (5 gi√° tr·ªã ƒë·∫ßu): {mean[:5]}\")\n",
    "        print(f\"Std (5 gi√° tr·ªã ƒë·∫ßu): {std[:5]}\")\n",
    "        \n",
    "        # 4. (T√πy ch·ªçn) L∆∞u l·∫°i ra file .npy ƒë·ªÉ d√πng vi·ªác kh√°c n·∫øu c·∫ßn\n",
    "        # np.save('mean.npy', mean)\n",
    "        # np.save('std.npy', std)\n",
    "        # print(\"ƒê√£ l∆∞u ra file .npy\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Kh√¥ng t√¨m th·∫•y key 'mean' ho·∫∑c 'std' trong file .pt n√†y.\")\n",
    "        print(\"C√°c keys hi·ªán c√≥:\", checkpoint.keys())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"C√≥ l·ªói x·∫£y ra: {e}\")\n",
    "\n",
    "# print(f\"\\nüìä Loading meta statistics:\")\n",
    "# print(f\"   Mean: {mean_path}\")\n",
    "# print(f\"   Std: {std_path}\")\n",
    "\n",
    "# mean_val = np.load(mean_path)\n",
    "# std_val = np.load(std_path)\n",
    "\n",
    "# print(f\"   ‚úÖ Mean shape: {mean_val.shape}, Mean first 5 values: {mean_val[:5]}\")\n",
    "# print(f\"   ‚úÖ Std shape: {std_val.shape}, Std first 5 values: {std_val[:5]}\")\n",
    "\n",
    "# 4. Override pipeline's mean/std with meta values\n",
    "print(\"\\nüîß Overriding pipeline statistics with meta values...\")\n",
    "pipeline.named_steps['stdscale'].data_mean_ = mean\n",
    "pipeline.named_steps['stdscale'].data_std_ = std\n",
    "print(\"   ‚úÖ Pipeline updated with meta statistics!\")\n",
    "\n",
    "# ===== VISUALIZE MOTION =====\n",
    "npy_path = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/BEAT_numpy/npy/3/1_wayne_0_25_25_sentence_004.npy\"\n",
    "frame = 50\n",
    "\n",
    "print(f\"\\nüé¨ Visualizing: {os.path.basename(npy_path)}\")\n",
    "print(f\"   Frame: {frame}\")\n",
    "\n",
    "# Load motion data\n",
    "motion_data = np.load(npy_path)\n",
    "print(f\"   Motion shape: {motion_data.shape}\")\n",
    "\n",
    "# Inverse transform\n",
    "print(\"   Performing inverse transform...\")\n",
    "reconstructed = pipeline.inverse_transform([motion_data])\n",
    "print(f\"   ‚úÖ Reconstructed shape: {reconstructed[0].values.shape}\")\n",
    "\n",
    "# Visualize 2D\n",
    "print(\"\\n   Creating 2D visualization...\")\n",
    "fig1 = plt.figure(figsize=(10, 8))\n",
    "draw_stickfigure(reconstructed[0], frame=frame)\n",
    "plt.title(f\"2D Stick Figure (Meta) - Frame {frame}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"   ‚úÖ 2D plot displayed!\")\n",
    "\n",
    "# Visualize 3D\n",
    "print(\"\\n   Creating 3D visualization...\")\n",
    "fig2 = plt.figure(figsize=(10, 8))\n",
    "draw_stickfigure3d(reconstructed[0], frame=frame)\n",
    "plt.title(f\"3D Stick Figure (Meta) - Frame {frame}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"   ‚úÖ 3D plot displayed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83329b36",
   "metadata": {},
   "source": [
    "### Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3d1766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# ƒê·∫£m b·∫£o ƒë∆∞·ªùng d·∫´n ƒë√∫ng ƒë·ªÉ import c√°c module c·ªßa b·∫°n\n",
    "# sys.path.insert(0, \"/path/to/your/project_root\") \n",
    "\n",
    "from models import MotionTransformer\n",
    "from trainers import DDPMTrainer\n",
    "# Import ƒë√∫ng class t·ª´ file model.py b·∫°n ƒë√£ upload\n",
    "from models.vq.model import RVQVAE \n",
    "\n",
    "# ==========================================\n",
    "# 1. C·∫•u h√¨nh Inference\n",
    "# ==========================================\n",
    "class InferenceConfig:\n",
    "    def __init__(self):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        self.is_train = False\n",
    "        self.schedule_sampler = 'uniform'   \n",
    "\n",
    "        # --- C·∫•u h√¨nh Diffusion ---\n",
    "        self.input_feats = 512     # Latent Dimension\n",
    "        self.num_frames = 24       # Latent Length (360 / 8)\n",
    "        self.num_layers = 8\n",
    "        self.latent_dim = 512\n",
    "        self.ff_size = 1024\n",
    "        self.num_heads = 8\n",
    "        self.dropout = 0.1\n",
    "        self.activation = \"gelu\"\n",
    "        self.dataset_name = 'beat' \n",
    "        self.do_denoise = True\n",
    "        self.noise_schedule = 'cosine'\n",
    "        self.diffusion_steps = 1000\n",
    "        self.no_clip = False\n",
    "        self.no_eff = False\n",
    "        self.result_dir = \"/home/serverai/ltdoanh/Motion_Diffusion/results\"\n",
    "\n",
    "# Class gi·∫£ l·∫≠p args cho RVQVAE\n",
    "class VQArgs:\n",
    "    def __init__(self):\n",
    "        # C√°c gi√° tr·ªã m·∫∑c ƒë·ªãnh, s·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t logic b√™n d∆∞·ªõi\n",
    "        self.num_quantizers = 1 \n",
    "        self.shared_codebook = False\n",
    "        self.quantize_dropout_prob = 0.0\n",
    "        self.mu = 0.99 # Cho QuantizerEMA\n",
    "\n",
    "opt = InferenceConfig()\n",
    "vq_args = VQArgs()\n",
    "\n",
    "# ==========================================\n",
    "# 2. Load Checkpoint & T√°ch Weights\n",
    "# ==========================================\n",
    "ckpt_path = \"/home/serverai/ltdoanh/Motion_Diffusion/checkpoints/beat/vq_diffusion/model/best.pt\"\n",
    "print(f\"üìÇ Loading checkpoint: {ckpt_path}\")\n",
    "\n",
    "# Load to√†n b·ªô checkpoint\n",
    "checkpoint = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
    "state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "# --- T·ª± ƒë·ªông ph√°t hi·ªán s·ªë l∆∞·ª£ng Quantizers t·ª´ Checkpoint ---\n",
    "# ƒêi·ªÅu n√†y gi√∫p tr√°nh l·ªói sai l·ªách key khi kh·ªüi t·∫°o VQ-VAE\n",
    "max_layer_idx = 0\n",
    "for k in state_dict.keys():\n",
    "    if \"vqvae.quantizer.layers.\" in k:\n",
    "        # Parse t√¨m s·ªë l·ªõn nh·∫•t trong 'layers.X.'\n",
    "        try:\n",
    "            parts = k.split('.')\n",
    "            layer_idx = int(parts[parts.index('layers') + 1])\n",
    "            if layer_idx > max_layer_idx:\n",
    "                max_layer_idx = layer_idx\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "vq_args.num_quantizers = max_layer_idx + 1\n",
    "print(f\"üîç Detected num_quantizers: {vq_args.num_quantizers}\")\n",
    "\n",
    "# --- T√°ch Dictionary ---\n",
    "trans_dict = {}\n",
    "vqvae_dict = {}\n",
    "\n",
    "for k, v in state_dict.items():\n",
    "    if k.startswith('transformer.'):\n",
    "        trans_dict[k[12:]] = v  \n",
    "    elif k.startswith('vqvae.'):\n",
    "        vqvae_dict[k[6:]] = v   \n",
    "\n",
    "# ==========================================\n",
    "# 3. Kh·ªüi t·∫°o Models\n",
    "# ==========================================\n",
    "\n",
    "# A. Motion Transformer\n",
    "print(\"üîß Initializing MotionTransformer...\")\n",
    "encoder = MotionTransformer(\n",
    "    input_feats=opt.input_feats,\n",
    "    num_frames=opt.num_frames,\n",
    "    num_layers=opt.num_layers,\n",
    "    latent_dim=opt.latent_dim,\n",
    "    num_heads=opt.num_heads,\n",
    "    ff_size=opt.ff_size,\n",
    "    no_clip=opt.no_clip,\n",
    "    no_eff=opt.no_eff\n",
    ")\n",
    "encoder.load_state_dict(trans_dict, strict=True)\n",
    "encoder.to(opt.device).eval()\n",
    "\n",
    "# B. RVQVAE\n",
    "print(\"üîß Initializing RVQVAE...\")\n",
    "# L∆∞u √Ω: C√°c tham s·ªë d∆∞·ªõi ƒë√¢y ph·∫£i kh·ªõp v·ªõi file config l√∫c train VQVAE c·ªßa b·∫°n.\n",
    "# T√¥i ƒëang ƒë·ªÉ c√°c gi√° tr·ªã ph·ªï bi·∫øn d·ª±a tr√™n file model.py\n",
    "vqvae_model = RVQVAE(\n",
    "    args=vq_args,\n",
    "    input_width=264,       # BEAT dataset th∆∞·ªùng l√† 264\n",
    "    nb_code=512,           # Ki·ªÉm tra l·∫°i config train c≈© n·∫øu l·ªói\n",
    "    code_dim=512, \n",
    "    output_emb_width=512, \n",
    "    down_t=3, \n",
    "    stride_t=2, \n",
    "    width=512, \n",
    "    depth=3, \n",
    "    dilation_growth_rate=3,\n",
    "    activation='relu',\n",
    "    norm=None\n",
    ")\n",
    "vqvae_model.load_state_dict(vqvae_dict, strict=True)\n",
    "vqvae_model.to(opt.device).eval()\n",
    "\n",
    "# ==========================================\n",
    "# 4. Inference\n",
    "# ==========================================\n",
    "trainer = DDPMTrainer(opt, encoder)\n",
    "\n",
    "# Inject mean/std (Quan tr·ªçng cho qu√° tr√¨nh decode cu·ªëi c√πng)\n",
    "trainer.mean = checkpoint['mean']\n",
    "trainer.std = checkpoint['std']\n",
    "\n",
    "print(\"üöÄ Starting Inference...\")\n",
    "os.makedirs(opt.result_dir, exist_ok=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    caption = [\"the first thing i like to do on weekends is relaxing\"]\n",
    "    \n",
    "    # ƒê·ªô d√†i Latent (45)\n",
    "    m_lens = torch.LongTensor([45]).to(opt.device) \n",
    "    \n",
    "    # 1. Sinh Latent (Diffusion) -> Output: (Batch, Length, Dim) = (1, 45, 512)\n",
    "    pred_latent_list = trainer.generate(caption, m_lens, dim_pose=512)\n",
    "    pred_latent = pred_latent_list[0]\n",
    "\n",
    "    if pred_latent.dim() == 2:\n",
    "        pred_latent = pred_latent.unsqueeze(0)  # Th√™m batch dim n·∫øu c·∫ßn\n",
    "\n",
    "    print(f\"   Latent generated shape: {pred_latent.shape}\")\n",
    "\n",
    "    # 2. Decode b·∫±ng RVQVAE\n",
    "    # RVQVAE Decoder c·∫ßn input: (Batch, Channel, Length) -> C·∫ßn permute\n",
    "    latent_input = pred_latent.permute(0, 2, 1) # -> (1, 512, 45)\n",
    "    \n",
    "    print(\"   Decoding with RVQVAE...\")\n",
    "    # G·ªçi tr·ª±c ti·∫øp decoder (b·ªè qua quantizer v√¨ Diffusion ƒë√£ sinh ra latent r·ªìi)\n",
    "    decoded_motion = vqvae_model.decoder(latent_input)\n",
    "    \n",
    "    # 3. Post-process (Permute l·∫°i v·ªÅ: Batch, Length, Channel)\n",
    "    # H√†m postprocess trong model.py: (B, C, T) -> (B, T, C)\n",
    "    motion = vqvae_model.postprocess(decoded_motion).cpu().numpy()\n",
    "\n",
    "    if motion.shape[1] == 264 and motion.shape[2] == 360:\n",
    "        motion = motion.transpose(0, 2, 1)\n",
    "\n",
    "    # motion = motion.cpu().numpy()\n",
    "    \n",
    "    # 4. Denormalize (Gi·∫£i chu·∫©n h√≥a)\n",
    "    # Output c·ªßa VQVAE th∆∞·ªùng v·∫´n l√† normalized data\n",
    "    mean = checkpoint['mean']\n",
    "    std = checkpoint['std']\n",
    "    \n",
    "    # ƒê·∫£m b·∫£o shape kh·ªõp ƒë·ªÉ broadcast\n",
    "    # Motion: (1, 360, 264), Mean: (264,), Std: (264,)\n",
    "    motion = motion * std + mean\n",
    "    \n",
    "    print(f\"üéâ Final Motion Shape: {motion.shape}\")\n",
    "\n",
    "# L∆∞u k·∫øt qu·∫£\n",
    "save_path = os.path.join(opt.result_dir, 'motion_inference.npy')\n",
    "np.save(save_path, motion)\n",
    "print(f\"üíæ Saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95738b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "\n",
    "# Setup path\n",
    "PYOM_DIR = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/pymo\"\n",
    "if PYOM_DIR not in sys.path:\n",
    "    sys.path.insert(0, PYOM_DIR)\n",
    "\n",
    "from pymo.parsers import BVHParser\n",
    "from pymo.preprocessing import *\n",
    "from pymo.viz_tools import draw_stickfigure, draw_stickfigure3d\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "\n",
    "# ===== CREATE PIPELINE WITH META =====\n",
    "print(\"\\nüî® Creating pipeline from scratch...\")\n",
    "\n",
    "# 1. Create empty pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('param', MocapParameterizer('position')),\n",
    "    ('rcpn', RootCentricPositionNormalizer()),\n",
    "    ('delta', RootTransformer('abdolute_translation_deltas')),\n",
    "    ('const', ConstantsRemover()),\n",
    "    ('np', Numpyfier()),\n",
    "    ('down', DownSampler(2)),\n",
    "    ('stdscale', ListStandardScaler())\n",
    "])\n",
    "\n",
    "# 2. Fit pipeline on sample BVH (to learn structure)\n",
    "bvh_sample_path = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/BEAT/1/1_wayne_0_1_1.bvh\"\n",
    "print(f\"   Fitting on BVH sample: {os.path.basename(bvh_sample_path)}\")\n",
    "\n",
    "parser = BVHParser()\n",
    "parsed_data = parser.parse(bvh_sample_path)\n",
    "pipeline.fit([parsed_data])\n",
    "print(\"   ‚úÖ Pipeline fitted (structure learned)\")\n",
    "\n",
    "# 3. Load mean/std from meta directory\n",
    "# meta_dir = \"/home/serverai/ltdoanh/Motion_Diffusion/checkpoints/beat/test/meta\"\n",
    "# mean_path = os.path.join(meta_dir, \"mean.npy\")\n",
    "# std_path = os.path.join(meta_dir, \"std.npy\")\n",
    "ckpt_path = \"/home/serverai/ltdoanh/Motion_Diffusion/checkpoints/beat/vq_diffusion/model/best.pt\"  # Thay ƒë∆∞·ªùng d·∫´n file c·ªßa b·∫°n v√†o ƒë√¢y\n",
    "\n",
    "try:\n",
    "    # 2. Load checkpoint\n",
    "    # map_location='cpu' gi√∫p tr√°nh l·ªói n·∫øu m√°y b·∫°n kh√¥ng c√≥ GPU gi·ªëng l√∫c train\n",
    "    checkpoint = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
    "\n",
    "    # 3. Ki·ªÉm tra v√† l·∫•y mean, std\n",
    "    if 'mean' in checkpoint and 'std' in checkpoint:\n",
    "        mean = checkpoint['mean']\n",
    "        std = checkpoint['std']\n",
    "\n",
    "        print(\"--- ƒê√£ t√¨m th·∫•y Mean v√† Std ---\")\n",
    "        print(f\"Shape c·ªßa Mean: {mean.shape}\")\n",
    "        print(f\"Shape c·ªßa Std: {std.shape}\")\n",
    "        \n",
    "        # In th·ª≠ v√†i gi√° tr·ªã ƒë·∫ßu\n",
    "        print(f\"Mean (5 gi√° tr·ªã ƒë·∫ßu): {mean[:5]}\")\n",
    "        print(f\"Std (5 gi√° tr·ªã ƒë·∫ßu): {std[:5]}\")\n",
    "        \n",
    "        # 4. (T√πy ch·ªçn) L∆∞u l·∫°i ra file .npy ƒë·ªÉ d√πng vi·ªác kh√°c n·∫øu c·∫ßn\n",
    "        # np.save('mean.npy', mean)\n",
    "        # np.save('std.npy', std)\n",
    "        # print(\"ƒê√£ l∆∞u ra file .npy\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Kh√¥ng t√¨m th·∫•y key 'mean' ho·∫∑c 'std' trong file .pt n√†y.\")\n",
    "        print(\"C√°c keys hi·ªán c√≥:\", checkpoint.keys())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"C√≥ l·ªói x·∫£y ra: {e}\")\n",
    "\n",
    "# print(f\"\\nüìä Loading meta statistics:\")\n",
    "# print(f\"   Mean: {mean_path}\")\n",
    "# print(f\"   Std: {std_path}\")\n",
    "\n",
    "# mean_val = np.load(mean_path)\n",
    "# std_val = np.load(std_path)\n",
    "\n",
    "# print(f\"   ‚úÖ Mean shape: {mean_val.shape}, Mean first 5 values: {mean_val[:5]}\")\n",
    "# print(f\"   ‚úÖ Std shape: {std_val.shape}, Std first 5 values: {std_val[:5]}\")\n",
    "\n",
    "# 4. Override pipeline's mean/std with meta values\n",
    "print(\"\\nüîß Overriding pipeline statistics with meta values...\")\n",
    "pipeline.named_steps['stdscale'].data_mean_ = mean\n",
    "pipeline.named_steps['stdscale'].data_std_ = std\n",
    "print(\"   ‚úÖ Pipeline updated with meta statistics!\")\n",
    "\n",
    "# ===== VISUALIZE MOTION =====\n",
    "npy_path = \"/home/serverai/ltdoanh/Motion_Diffusion/results/motion_inference.npy\"\n",
    "frame = 50\n",
    "\n",
    "print(f\"\\nüé¨ Visualizing: {os.path.basename(npy_path)}\")\n",
    "print(f\"   Frame: {frame}\")\n",
    "\n",
    "# Load motion data\n",
    "motion_data = np.load(npy_path)\n",
    "print(f\"   Motion shape: {motion_data.shape}\")\n",
    "if motion_data.ndim == 3:\n",
    "    motion_data = motion_data[0] # L·∫•y m·∫´u ƒë·∫ßu ti√™n -> (360, 264)\n",
    "    print(f\"   Squeezed Motion shape: {motion_data.shape}\")\n",
    "\n",
    "# Inverse transform\n",
    "print(\"   Performing inverse transform...\")\n",
    "reconstructed = pipeline.inverse_transform([motion_data])\n",
    "print(f\"   ‚úÖ Reconstructed shape: {reconstructed[0].values.shape}\")\n",
    "\n",
    "# Visualize 2D\n",
    "print(\"\\n   Creating 2D visualization...\")\n",
    "fig1 = plt.figure(figsize=(10, 8))\n",
    "draw_stickfigure(reconstructed[0], frame=frame)\n",
    "plt.title(f\"2D Stick Figure (Meta) - Frame {frame}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"   ‚úÖ 2D plot displayed!\")\n",
    "\n",
    "# Visualize 3D\n",
    "print(\"\\n   Creating 3D visualization...\")\n",
    "fig2 = plt.figure(figsize=(10, 8))\n",
    "draw_stickfigure3d(reconstructed[0], frame=frame)\n",
    "plt.title(f\"3D Stick Figure (Meta) - Frame {frame}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"   ‚úÖ 3D plot displayed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "layout",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
