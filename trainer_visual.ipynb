{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af10b203",
   "metadata": {},
   "source": [
    "### Chuáº©n bá»‹ data\n",
    "\n",
    "```text\n",
    "datasets/\n",
    "â”œâ”€â”€ BEAT/\n",
    "â”‚   â”œâ”€â”€ 1/\n",
    "â”‚   â”œâ”€â”€ 2/\n",
    "|   â”œâ”€â”€ ...\n",
    "|   â””â”€â”€ ...\n",
    "â”œâ”€â”€ BEAT_numpy/\n",
    "â”‚   â”œâ”€â”€ npy/\n",
    "â”‚   â””â”€â”€ txt/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2487c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/BEAT\"\n",
    "out_dir = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/BEAT_numpy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a287d95",
   "metadata": {},
   "source": [
    "### TÃ­nh mean/std cho toÃ n bá»™ dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d812d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ¯ MODE: HYBRID\n",
      "ğŸ“‚ Folders to process: 30\n",
      "============================================================\n",
      "âœ… Found 1627 BVH files across 30 folders\n",
      "   Min files/folder: 31\n",
      "   Max files/folder: 130\n",
      "   Avg files/folder: 54\n",
      "\n",
      "============================================================\n",
      "ğŸš€ PASS 1: Parse + Transform (to before ConstantsRemover)\n",
      "============================================================\n",
      "\n",
      "[1/30] \n",
      "ğŸ“ Processing folder: 1\n",
      "   Files: 130 | Cores: 12\n",
      "   â†’ 1:   0%|                                                               | 0/130 [00:00<?, ?it/s]Process ForkPoolWorker-5:\n",
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-11:\n",
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-12:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/serverai/layout/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/srv/conda/envs/serverai/layout/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/srv/conda/envs/serverai/layout/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/step1_fit_scaler.py\", line 71, in worker_parse_and_transform\n",
      "    parsed_data = g_parser.parse(bvh_path)\n",
      "  File \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/pymo/pymo/parsers.py\", line 81, in parse\n",
      "    tokens, remainder = self.scanner.scan(raw_contents)\n",
      "  File \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/pymo/pymo/parsers.py\", line 49, in scan\n",
      "    return self.scanner.scan(stuff)\n",
      "  File \"/srv/conda/envs/serverai/layout/lib/python3.10/re.py\", line 377, in scan\n",
      "    if callable(action):\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-1:\n",
      "Process ForkPoolWorker-7:\n",
      "   â†’ 1:   0%|                                                               | 0/130 [00:03<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/serverai/layout/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/srv/conda/envs/serverai/layout/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/serverai/layout/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/step1_fit_scaler.py\", line 71, in worker_parse_and_transform\n",
      "    parsed_data = g_parser.parse(bvh_path)\n",
      "  File \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/pymo/pymo/parsers.py\", line 81, in parse\n",
      "    tokens, remainder = self.scanner.scan(raw_contents)\n",
      "  File \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/pymo/pymo/parsers.py\", line 49, in scan\n",
      "    return self.scanner.scan(stuff)\n",
      "  File \"/srv/conda/envs/serverai/layout/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/srv/conda/envs/serverai/layout/lib/python3.10/re.py\", line 377, in scan\n",
      "    if callable(action):\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/serverai/layout/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/srv/conda/envs/serverai/layout/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/srv/conda/envs/serverai/layout/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "Process ForkPoolWorker-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/serverai/layout/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/srv/conda/envs/serverai/layout/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/srv/conda/envs/serverai/layout/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/step1_fit_scaler.py\", line 71, in worker_parse_and_transform\n",
      "    parsed_data = g_parser.parse(bvh_path)\n",
      "  File \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/pymo/pymo/parsers.py\", line 81, in parse\n",
      "    tokens, remainder = self.scanner.scan(raw_contents)\n",
      "  File \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/pymo/pymo/parsers.py\", line 49, in scan\n",
      "    return self.scanner.scan(stuff)\n",
      "  File \"/srv/conda/envs/serverai/layout/lib/python3.10/re.py\",^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/serverai/layout/lib/python3.10/multiprocessing/pool.py\", line 856, in next\n",
      "    item = self._items.popleft()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/step1_fit_scaler.py\", line 91, in process_folder_multiprocessing\n",
      "    results = list(tqdm(\n",
      "  File \"/srv/conda/envs/serverai/layout/lib/python3.10/site-packages/tqdm/std.py\", line 1181, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"/srv/conda/envs/serverai/layout/lib/python3.10/multiprocessing/pool.py\", line 861, in next\n",
      "    self._cond.wait(timeout)\n",
      "  File \"/srv/conda/envs/serverai/layout/lib/python3.10/threading.py\", line 320, in wait\n",
      "    waiter.acquire()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/step1_fit_scaler.py\", line 331, in <module>\n",
      "    main_fit_scaler(\n",
      "  File \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/step1_fit_scaler.py\", line 187, in main_fit_scaler\n",
      "    folder_results = process_folder_multiprocessing(bvh_paths, folder)\n",
      "  File \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/step1_fit_scaler.py\", line 90, in process_folder_multiprocessing\n",
      "    with multiprocessing.Pool(processes=num_cores, initializer=init_worker) as pool:\n",
      "  File \"/srv/conda/envs/serverai/layout/lib/python3.10/multiprocessing/pool.py\", line 739, in __exit__\n",
      "    self.terminate()\n",
      "  File \"/srv/conda/envs/serverai/layout/lib/python3.10/multiprocessing/pool.py\", line 657, in terminate\n",
      "    self._terminate()\n",
      "  File \"/srv/conda/envs/serverai/layout/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/srv/conda/envs/serverai/layout/lib/python3.10/multiprocessing/pool.py\", line 732, in _terminate_pool\n",
      "    p.join()\n",
      "  File \"/srv/conda/envs/serverai/layout/lib/python3.10/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/srv/conda/envs/serverai/layout/lib/python3.10/multiprocessing/popen_fork.py\", line 43, in wait\n",
      "    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\n",
      "  File \"/srv/conda/envs/serverai/layout/lib/python3.10/multiprocessing/popen_fork.py\", line 27, in poll\n",
      "    pid, sts = os.waitpid(self.pid, flag)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/step1_fit_scaler.py\" --parent-dir \"{base_dir}\" --start 1 --end 30 --mode hybrid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3acb60",
   "metadata": {},
   "source": [
    "### Chuyá»ƒn dataset bvh sang npy theo tá»«ng segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98171b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/preprocess_data.py\" --parent-dir \"{base_dir}\" --out-root \"{out_dir}\" --folders \"6\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7d2957",
   "metadata": {},
   "source": [
    "### Train MotionDiffuse nhÆ° bÃ¬nh thÆ°á»ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6c81c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python \"/home/serverai/ltdoanh/Motion_Diffusion/tools/train.py\" --dataset_name beat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf77ac5",
   "metadata": {},
   "source": [
    "### Train VQ-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed13924",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python \"/home/serverai/ltdoanh/Motion_Diffusion/tools/train_vq.py\" --dataset_name beat --codebook_size 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab74aaf",
   "metadata": {},
   "source": [
    "### Train MotionDiffuse trÃªn Latent Space do VQ-VAE á»Ÿ trÃªn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf31078",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python \"/home/serverai/ltdoanh/Motion_Diffusion/tools/train_vq_diffusion.py\" --dataset_name beat --vqvae_name VQVAE_BEAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e1052a",
   "metadata": {},
   "source": [
    "### Evaluation - Äang fix lá»—i ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca744de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python \"/home/serverai/ltdoanh/Motion_Diffusion/run_evaluation.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9950bdf",
   "metadata": {},
   "source": [
    "### Visual data báº±ng mean/std chuáº©n tÃ­nh tá»« bá»™ dá»¯ liá»‡u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9f0d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Setup path\n",
    "PYOM_DIR = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/pymo\"\n",
    "if PYOM_DIR not in sys.path:\n",
    "    sys.path.insert(0, PYOM_DIR)\n",
    "\n",
    "from pymo.parsers import BVHParser\n",
    "from pymo.preprocessing import *\n",
    "from pymo.viz_tools import draw_stickfigure, draw_stickfigure3d\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"âœ… Imports successful!\")\n",
    "\n",
    "# ===== CELL 2: Load Pipeline =====\n",
    "pipeline_path = \"/home/serverai/ltdoanh/Motion_Diffusion/global_pipeline.pkl\"\n",
    "\n",
    "print(f\"ğŸ“¦ Loading pipeline from: {pipeline_path}\")\n",
    "\n",
    "pipeline = joblib.load(pipeline_path)\n",
    "print(f\"âœ… Pipeline loaded!\")\n",
    "print(f\"   Mean shape: {pipeline.named_steps['stdscale'].data_mean_.shape}, First 5 values: {pipeline.named_steps['stdscale'].data_mean_[:5]}\")\n",
    "print(f\"   Std shape: {pipeline.named_steps['stdscale'].data_std_.shape},  First 5 values: {pipeline.named_steps['stdscale'].data_std_[:5]}\")\n",
    "\n",
    "# ===== CELL 3: Load and Visualize Motion =====\n",
    "npy_path = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/BEAT_numpy/npy/3/1_wayne_0_5_5_sentence_000.npy\"\n",
    "frame = 50\n",
    "\n",
    "print(f\"\\nğŸ¬ Visualizing: {os.path.basename(npy_path)}\")\n",
    "print(f\"   Frame: {frame}\")\n",
    "\n",
    "# Load motion data\n",
    "motion_data = np.load(npy_path)\n",
    "print(f\"   Motion shape: {motion_data.shape}\")\n",
    "\n",
    "# Inverse transform\n",
    "print(\"   Performing inverse transform...\")\n",
    "reconstructed = pipeline.inverse_transform([motion_data])\n",
    "print(f\"   âœ… Reconstructed shape: {reconstructed[0].values.shape}\")\n",
    "\n",
    "# Visualize 2D\n",
    "print(\"\\n   Creating 2D visualization...\")\n",
    "fig1 = plt.figure(figsize=(10, 8))\n",
    "draw_stickfigure(reconstructed[0], frame=frame)\n",
    "plt.title(f\"2D Stick Figure - Frame {frame}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"   âœ… 2D plot displayed!\")\n",
    "\n",
    "# Visualize 3D\n",
    "print(\"\\n   Creating 3D visualization...\")\n",
    "fig2 = plt.figure(figsize=(10, 8))\n",
    "draw_stickfigure3d(reconstructed[0], frame=frame)\n",
    "plt.title(f\"3D Stick Figure - Frame {frame}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"   âœ… 3D plot displayed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9286cdc0",
   "metadata": {},
   "source": [
    "### Visual data báº±ng mean/std tá»« model Ä‘Æ°á»£c huáº¥n luyá»‡n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65c2f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "\n",
    "# Setup path\n",
    "PYOM_DIR = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/pymo\"\n",
    "if PYOM_DIR not in sys.path:\n",
    "    sys.path.insert(0, PYOM_DIR)\n",
    "\n",
    "from pymo.parsers import BVHParser\n",
    "from pymo.preprocessing import *\n",
    "from pymo.viz_tools import draw_stickfigure, draw_stickfigure3d\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "print(\"âœ… Imports successful!\")\n",
    "\n",
    "# ===== CREATE PIPELINE WITH META =====\n",
    "print(\"\\nğŸ”¨ Creating pipeline from scratch...\")\n",
    "\n",
    "# 1. Create empty pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('param', MocapParameterizer('position')),\n",
    "    ('rcpn', RootCentricPositionNormalizer()),\n",
    "    ('delta', RootTransformer('abdolute_translation_deltas')),\n",
    "    ('const', ConstantsRemover()),\n",
    "    ('np', Numpyfier()),\n",
    "    ('down', DownSampler(2)),\n",
    "    ('stdscale', ListStandardScaler())\n",
    "])\n",
    "\n",
    "# 2. Fit pipeline on sample BVH (to learn structure)\n",
    "bvh_sample_path = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/BEAT/1/1_wayne_0_1_1.bvh\"\n",
    "print(f\"   Fitting on BVH sample: {os.path.basename(bvh_sample_path)}\")\n",
    "\n",
    "parser = BVHParser()\n",
    "parsed_data = parser.parse(bvh_sample_path)\n",
    "pipeline.fit([parsed_data])\n",
    "print(\"   âœ… Pipeline fitted (structure learned)\")\n",
    "\n",
    "# 3. Load mean/std from meta directory\n",
    "# meta_dir = \"/home/serverai/ltdoanh/Motion_Diffusion/checkpoints/beat/test/meta\"\n",
    "# mean_path = os.path.join(meta_dir, \"mean.npy\")\n",
    "# std_path = os.path.join(meta_dir, \"std.npy\")\n",
    "ckpt_path = \"/home/serverai/ltdoanh/Motion_Diffusion/checkpoints/beat/vq_diffusion/model/latest.pt\"  # Thay Ä‘Æ°á»ng dáº«n file cá»§a báº¡n vÃ o Ä‘Ã¢y\n",
    "\n",
    "try:\n",
    "    # 2. Load checkpoint\n",
    "    # map_location='cpu' giÃºp trÃ¡nh lá»—i náº¿u mÃ¡y báº¡n khÃ´ng cÃ³ GPU giá»‘ng lÃºc train\n",
    "    checkpoint = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
    "\n",
    "    # 3. Kiá»ƒm tra vÃ  láº¥y mean, std\n",
    "    if 'mean' in checkpoint and 'std' in checkpoint:\n",
    "        mean = checkpoint['mean']\n",
    "        std = checkpoint['std']\n",
    "\n",
    "        print(\"--- ÄÃ£ tÃ¬m tháº¥y Mean vÃ  Std ---\")\n",
    "        print(f\"Shape cá»§a Mean: {mean.shape}\")\n",
    "        print(f\"Shape cá»§a Std: {std.shape}\")\n",
    "        \n",
    "        # In thá»­ vÃ i giÃ¡ trá»‹ Ä‘áº§u\n",
    "        print(f\"Mean (5 giÃ¡ trá»‹ Ä‘áº§u): {mean[:5]}\")\n",
    "        print(f\"Std (5 giÃ¡ trá»‹ Ä‘áº§u): {std[:5]}\")\n",
    "        \n",
    "        # 4. (TÃ¹y chá»n) LÆ°u láº¡i ra file .npy Ä‘á»ƒ dÃ¹ng viá»‡c khÃ¡c náº¿u cáº§n\n",
    "        # np.save('mean.npy', mean)\n",
    "        # np.save('std.npy', std)\n",
    "        # print(\"ÄÃ£ lÆ°u ra file .npy\")\n",
    "        \n",
    "    else:\n",
    "        print(\"KhÃ´ng tÃ¬m tháº¥y key 'mean' hoáº·c 'std' trong file .pt nÃ y.\")\n",
    "        print(\"CÃ¡c keys hiá»‡n cÃ³:\", checkpoint.keys())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"CÃ³ lá»—i xáº£y ra: {e}\")\n",
    "\n",
    "# 4. Override pipeline's mean/std with meta values\n",
    "print(\"\\nğŸ”§ Overriding pipeline statistics with meta values...\")\n",
    "pipeline.named_steps['stdscale'].data_mean_ = mean\n",
    "pipeline.named_steps['stdscale'].data_std_ = std\n",
    "print(\"   âœ… Pipeline updated with meta statistics!\")\n",
    "\n",
    "# ===== VISUALIZE MOTION =====\n",
    "npy_path = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/BEAT_numpy/npy/2/2_scott_0_2_2_sentence_000.npy\"\n",
    "frame = 50\n",
    "\n",
    "print(f\"\\nğŸ¬ Visualizing: {os.path.basename(npy_path)}\")\n",
    "print(f\"   Frame: {frame}\")\n",
    "\n",
    "# Load motion data\n",
    "motion_data = np.load(npy_path)\n",
    "print(f\"   Motion shape: {motion_data.shape}\")\n",
    "\n",
    "# Inverse transform\n",
    "print(\"   Performing inverse transform...\")\n",
    "reconstructed = pipeline.inverse_transform([motion_data])\n",
    "print(f\"   âœ… Reconstructed shape: {reconstructed[0].values.shape}\")\n",
    "\n",
    "# Visualize 2D\n",
    "print(\"\\n   Creating 2D visualization...\")\n",
    "fig1 = plt.figure(figsize=(10, 8))\n",
    "draw_stickfigure(reconstructed[0], frame=frame)\n",
    "plt.title(f\"2D Stick Figure (Meta) - Frame {frame}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"   âœ… 2D plot displayed!\")\n",
    "\n",
    "# Visualize 3D\n",
    "print(\"\\n   Creating 3D visualization...\")\n",
    "fig2 = plt.figure(figsize=(10, 8))\n",
    "draw_stickfigure3d(reconstructed[0], frame=frame)\n",
    "plt.title(f\"3D Stick Figure (Meta) - Frame {frame}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"   âœ… 3D plot displayed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83329b36",
   "metadata": {},
   "source": [
    "### Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3d1766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Äáº£m báº£o Ä‘Æ°á»ng dáº«n Ä‘Ãºng Ä‘á»ƒ import cÃ¡c module cá»§a báº¡n\n",
    "# sys.path.insert(0, \"/path/to/your/project_root\") \n",
    "\n",
    "from models import MotionTransformer\n",
    "from trainers import DDPMTrainer\n",
    "# Import Ä‘Ãºng class tá»« file model.py báº¡n Ä‘Ã£ upload\n",
    "from models.vq.model import RVQVAE \n",
    "\n",
    "# ==========================================\n",
    "# 1. Cáº¥u hÃ¬nh Inference\n",
    "# ==========================================\n",
    "class InferenceConfig:\n",
    "    def __init__(self):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        self.is_train = False\n",
    "        self.schedule_sampler = 'uniform'   \n",
    "\n",
    "        # --- Cáº¥u hÃ¬nh Diffusion ---\n",
    "        self.input_feats = 512     # Latent Dimension\n",
    "        self.num_frames = 45      # Latent Length (360 / 8)\n",
    "        self.num_layers = 8\n",
    "        self.latent_dim = 512\n",
    "        self.ff_size = 1024\n",
    "        self.num_heads = 8\n",
    "        self.dropout = 0.1\n",
    "        self.activation = \"gelu\"\n",
    "        self.dataset_name = 'beat' \n",
    "        self.do_denoise = True\n",
    "        self.noise_schedule = 'cosine'\n",
    "        self.diffusion_steps = 1000\n",
    "        self.no_clip = False\n",
    "        self.no_eff = False\n",
    "        self.result_dir = \"/home/serverai/ltdoanh/Motion_Diffusion/results\"\n",
    "\n",
    "# Class giáº£ láº­p args cho RVQVAE\n",
    "class VQArgs:\n",
    "    def __init__(self):\n",
    "        # CÃ¡c giÃ¡ trá»‹ máº·c Ä‘á»‹nh, sáº½ Ä‘Æ°á»£c cáº­p nháº­t logic bÃªn dÆ°á»›i\n",
    "        self.num_quantizers = 1 \n",
    "        self.shared_codebook = False\n",
    "        self.quantize_dropout_prob = 0.0\n",
    "        self.mu = 0.99 # Cho QuantizerEMA\n",
    "\n",
    "opt = InferenceConfig()\n",
    "vq_args = VQArgs()\n",
    "\n",
    "# ==========================================\n",
    "# 2. Load Checkpoint & TÃ¡ch Weights\n",
    "# ==========================================\n",
    "ckpt_path = \"/home/serverai/ltdoanh/Motion_Diffusion/checkpoints/beat/vq_diffusion/model/latest.pt\"\n",
    "print(f\"ğŸ“‚ Loading checkpoint: {ckpt_path}\")\n",
    "\n",
    "# Load toÃ n bá»™ checkpoint\n",
    "checkpoint = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
    "state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "# --- Tá»± Ä‘á»™ng phÃ¡t hiá»‡n sá»‘ lÆ°á»£ng Quantizers tá»« Checkpoint ---\n",
    "# Äiá»u nÃ y giÃºp trÃ¡nh lá»—i sai lá»‡ch key khi khá»Ÿi táº¡o VQ-VAE\n",
    "max_layer_idx = 0\n",
    "for k in state_dict.keys():\n",
    "    if \"vqvae.quantizer.layers.\" in k:\n",
    "        # Parse tÃ¬m sá»‘ lá»›n nháº¥t trong 'layers.X.'\n",
    "        try:\n",
    "            parts = k.split('.')\n",
    "            layer_idx = int(parts[parts.index('layers') + 1])\n",
    "            if layer_idx > max_layer_idx:\n",
    "                max_layer_idx = layer_idx\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "vq_args.num_quantizers = max_layer_idx + 1\n",
    "print(f\"ğŸ” Detected num_quantizers: {vq_args.num_quantizers}\")\n",
    "\n",
    "# --- TÃ¡ch Dictionary ---\n",
    "trans_dict = {}\n",
    "vqvae_dict = {}\n",
    "\n",
    "for k, v in state_dict.items():\n",
    "    if k.startswith('transformer.'):\n",
    "        trans_dict[k[12:]] = v  \n",
    "    elif k.startswith('vqvae.'):\n",
    "        vqvae_dict[k[6:]] = v   \n",
    "\n",
    "# ==========================================\n",
    "# 3. Khá»Ÿi táº¡o Models\n",
    "# ==========================================\n",
    "\n",
    "# A. Motion Transformer\n",
    "print(\"ğŸ”§ Initializing MotionTransformer...\")\n",
    "encoder = MotionTransformer(\n",
    "    input_feats=opt.input_feats,\n",
    "    num_frames=opt.num_frames,\n",
    "    num_layers=opt.num_layers,\n",
    "    latent_dim=opt.latent_dim,\n",
    "    num_heads=opt.num_heads,\n",
    "    ff_size=opt.ff_size,\n",
    "    no_clip=opt.no_clip,\n",
    "    no_eff=opt.no_eff\n",
    ")\n",
    "encoder.load_state_dict(trans_dict, strict=True)\n",
    "encoder.to(opt.device).eval()\n",
    "\n",
    "# B. RVQVAE\n",
    "print(\"ğŸ”§ Initializing RVQVAE...\")\n",
    "# LÆ°u Ã½: CÃ¡c tham sá»‘ dÆ°á»›i Ä‘Ã¢y pháº£i khá»›p vá»›i file config lÃºc train VQVAE cá»§a báº¡n.\n",
    "# TÃ´i Ä‘ang Ä‘á»ƒ cÃ¡c giÃ¡ trá»‹ phá»• biáº¿n dá»±a trÃªn file model.py\n",
    "vqvae_model = RVQVAE(\n",
    "    args=vq_args,\n",
    "    input_width=264,       # BEAT dataset thÆ°á»ng lÃ  264\n",
    "    nb_code=512,           # Kiá»ƒm tra láº¡i config train cÅ© náº¿u lá»—i\n",
    "    code_dim=512, \n",
    "    output_emb_width=512, \n",
    "    down_t=3, \n",
    "    stride_t=2, \n",
    "    width=512, \n",
    "    depth=3, \n",
    "    dilation_growth_rate=3,\n",
    "    activation='relu',\n",
    "    norm=None\n",
    ")\n",
    "vqvae_model.load_state_dict(vqvae_dict, strict=True)\n",
    "vqvae_model.to(opt.device).eval()\n",
    "\n",
    "# ==========================================\n",
    "# 4. Inference\n",
    "# ==========================================\n",
    "trainer = DDPMTrainer(opt, encoder)\n",
    "\n",
    "# Inject mean/std (Quan trá»ng cho quÃ¡ trÃ¬nh decode cuá»‘i cÃ¹ng)\n",
    "trainer.mean = checkpoint['mean']\n",
    "trainer.std = checkpoint['std']\n",
    "\n",
    "print(\"ğŸš€ Starting Inference...\")\n",
    "os.makedirs(opt.result_dir, exist_ok=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    caption = [\"the one thing i miss when i'm away from home is my mum for example when i'm studying in the university which is 1400 miles away from my birthplace the food of my school's cafeteria is awful\"]\n",
    "    \n",
    "    # Äá»™ dÃ i Latent (45)\n",
    "    m_lens = torch.LongTensor([45]).to(opt.device) \n",
    "    \n",
    "    # 1. Sinh Latent (Diffusion) -> Output: (Batch, Length, Dim) = (1, 45, 512)\n",
    "    pred_latent_list = trainer.generate(caption, m_lens, dim_pose=512)\n",
    "    pred_latent = pred_latent_list[0]\n",
    "\n",
    "    if pred_latent.dim() == 2:\n",
    "        pred_latent = pred_latent.unsqueeze(0)  # ThÃªm batch dim náº¿u cáº§n\n",
    "\n",
    "    print(f\"   Latent generated shape: {pred_latent.shape}\")\n",
    "\n",
    "    # 2. Decode báº±ng RVQVAE\n",
    "    # RVQVAE Decoder cáº§n input: (Batch, Channel, Length) -> Cáº§n permute\n",
    "    latent_input = pred_latent.permute(0, 2, 1) # -> (1, 512, 45)\n",
    "    \n",
    "    print(\"   Decoding with RVQVAE...\")\n",
    "    # Gá»i trá»±c tiáº¿p decoder (bá» qua quantizer vÃ¬ Diffusion Ä‘Ã£ sinh ra latent rá»“i)\n",
    "    decoded_motion = vqvae_model.decoder(latent_input)\n",
    "    \n",
    "    # 3. Post-process (Permute láº¡i vá»: Batch, Length, Channel)\n",
    "    # HÃ m postprocess trong model.py: (B, C, T) -> (B, T, C)\n",
    "    motion = vqvae_model.postprocess(decoded_motion).cpu().numpy()\n",
    "\n",
    "    if motion.shape[1] == 264 and motion.shape[2] == 360:\n",
    "        motion = motion.transpose(0, 2, 1)\n",
    "\n",
    "    # motion = motion.cpu().numpy()\n",
    "    \n",
    "    # 4. Denormalize (Giáº£i chuáº©n hÃ³a)\n",
    "    # Output cá»§a VQVAE thÆ°á»ng váº«n lÃ  normalized data\n",
    "    mean = checkpoint['mean']\n",
    "    std = checkpoint['std']\n",
    "    \n",
    "    # Äáº£m báº£o shape khá»›p Ä‘á»ƒ broadcast\n",
    "    # Motion: (1, 360, 264), Mean: (264,), Std: (264,)\n",
    "    motion = motion * std + mean\n",
    "    \n",
    "    print(f\"ğŸ‰ Final Motion Shape: {motion.shape}\")\n",
    "\n",
    "# LÆ°u káº¿t quáº£\n",
    "save_path = os.path.join(opt.result_dir, 'motion_inference-01.npy')\n",
    "np.save(save_path, motion)\n",
    "print(f\"ğŸ’¾ Saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95738b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "\n",
    "# Setup path\n",
    "PYOM_DIR = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/pymo\"\n",
    "if PYOM_DIR not in sys.path:\n",
    "    sys.path.insert(0, PYOM_DIR)\n",
    "\n",
    "from pymo.parsers import BVHParser\n",
    "from pymo.preprocessing import *\n",
    "from pymo.viz_tools import draw_stickfigure, draw_stickfigure3d\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "print(\"âœ… Imports successful!\")\n",
    "\n",
    "# ===== CREATE PIPELINE WITH META =====\n",
    "print(\"\\nğŸ”¨ Creating pipeline from scratch...\")\n",
    "\n",
    "# 1. Create empty pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('param', MocapParameterizer('position')),\n",
    "    ('rcpn', RootCentricPositionNormalizer()),\n",
    "    ('delta', RootTransformer('abdolute_translation_deltas')),\n",
    "    ('const', ConstantsRemover()),\n",
    "    ('np', Numpyfier()),\n",
    "    ('down', DownSampler(2)),\n",
    "    ('stdscale', ListStandardScaler())\n",
    "])\n",
    "\n",
    "# 2. Fit pipeline on sample BVH (to learn structure)\n",
    "bvh_sample_path = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/BEAT/1/1_wayne_0_1_1.bvh\"\n",
    "print(f\"   Fitting on BVH sample: {os.path.basename(bvh_sample_path)}\")\n",
    "\n",
    "parser = BVHParser()\n",
    "parsed_data = parser.parse(bvh_sample_path)\n",
    "pipeline.fit([parsed_data])\n",
    "print(\"   âœ… Pipeline fitted (structure learned)\")\n",
    "\n",
    "# 3. Load mean/std from meta directory\n",
    "# meta_dir = \"/home/serverai/ltdoanh/Motion_Diffusion/checkpoints/beat/test/meta\"\n",
    "# mean_path = os.path.join(meta_dir, \"mean.npy\")\n",
    "# std_path = os.path.join(meta_dir, \"std.npy\")\n",
    "ckpt_path = \"/home/serverai/ltdoanh/Motion_Diffusion/checkpoints/beat/vq_diffusion/model/latest.pt\"  # Thay Ä‘Æ°á»ng dáº«n file cá»§a báº¡n vÃ o Ä‘Ã¢y\n",
    "\n",
    "try:\n",
    "    # 2. Load checkpoint\n",
    "    # map_location='cpu' giÃºp trÃ¡nh lá»—i náº¿u mÃ¡y báº¡n khÃ´ng cÃ³ GPU giá»‘ng lÃºc train\n",
    "    checkpoint = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
    "\n",
    "    # 3. Kiá»ƒm tra vÃ  láº¥y mean, std\n",
    "    if 'mean' in checkpoint and 'std' in checkpoint:\n",
    "        mean = checkpoint['mean']\n",
    "        std = checkpoint['std']\n",
    "\n",
    "        print(\"--- ÄÃ£ tÃ¬m tháº¥y Mean vÃ  Std ---\")\n",
    "        print(f\"Shape cá»§a Mean: {mean.shape}\")\n",
    "        print(f\"Shape cá»§a Std: {std.shape}\")\n",
    "        \n",
    "        # In thá»­ vÃ i giÃ¡ trá»‹ Ä‘áº§u\n",
    "        print(f\"Mean (5 giÃ¡ trá»‹ Ä‘áº§u): {mean[:5]}\")\n",
    "        print(f\"Std (5 giÃ¡ trá»‹ Ä‘áº§u): {std[:5]}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"KhÃ´ng tÃ¬m tháº¥y key 'mean' hoáº·c 'std' trong file .pt nÃ y.\")\n",
    "        print(\"CÃ¡c keys hiá»‡n cÃ³:\", checkpoint.keys())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"CÃ³ lá»—i xáº£y ra: {e}\")\n",
    "\n",
    "# 4. Override pipeline's mean/std with meta values\n",
    "print(\"\\nğŸ”§ Overriding pipeline statistics with meta values...\")\n",
    "pipeline.named_steps['stdscale'].data_mean_ = mean\n",
    "pipeline.named_steps['stdscale'].data_std_ = std\n",
    "print(\"   âœ… Pipeline updated with meta statistics!\")\n",
    "\n",
    "# ===== VISUALIZE MOTION =====\n",
    "npy_path = \"/home/serverai/ltdoanh/Motion_Diffusion/results/motion_inference-01.npy\"\n",
    "frame = 50\n",
    "\n",
    "print(f\"\\nğŸ¬ Visualizing: {os.path.basename(npy_path)}\")\n",
    "print(f\"   Frame: {frame}\")\n",
    "\n",
    "# Load motion data\n",
    "motion_data = np.load(npy_path)\n",
    "print(f\"   Motion shape: {motion_data.shape}\")\n",
    "if motion_data.ndim == 3:\n",
    "    motion_data = motion_data[0] # Láº¥y máº«u Ä‘áº§u tiÃªn -> (360, 264)\n",
    "    print(f\"   Squeezed Motion shape: {motion_data.shape}\")\n",
    "\n",
    "# Inverse transform\n",
    "print(\"   Performing inverse transform...\")\n",
    "reconstructed = pipeline.inverse_transform([motion_data])\n",
    "print(f\"   âœ… Reconstructed shape: {reconstructed[0].values.shape}\")\n",
    "\n",
    "# Visualize 2D\n",
    "print(\"\\n   Creating 2D visualization...\")\n",
    "fig1 = plt.figure(figsize=(10, 8))\n",
    "draw_stickfigure(reconstructed[0], frame=frame)\n",
    "plt.title(f\"2D Stick Figure (Meta) - Frame {frame}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"   âœ… 2D plot displayed!\")\n",
    "\n",
    "# Visualize 3D\n",
    "print(\"\\n   Creating 3D visualization...\")\n",
    "fig2 = plt.figure(figsize=(10, 8))\n",
    "draw_stickfigure3d(reconstructed[0], frame=frame)\n",
    "plt.title(f\"3D Stick Figure (Meta) - Frame {frame}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"   âœ… 3D plot displayed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "layout",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
