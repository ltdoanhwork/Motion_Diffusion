{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a287d95",
   "metadata": {},
   "source": [
    "### T√≠nh mean/std cho to√†n b·ªô dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d812d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/step1_fit_scaler.py\" --parent-dir \"{base_dir}\" --start 1 --end 30 --mode hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5645ae96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pipeline loaded successfully.\n",
      "----------------------------------------\n",
      "Pipeline Steps Inspection:\n",
      "----------------------------------------\n",
      "Step: 'param' -> Type: <class 'pymo.preprocessing.MocapParameterizer'>\n",
      "Step: 'rcpn' -> Type: <class 'pymo.preprocessing.RootCentricPositionNormalizer'>\n",
      "Step: 'delta' -> Type: <class 'pymo.preprocessing.RootTransformer'>\n",
      "Step: 'const' -> Type: <class 'pymo.preprocessing.ConstantsRemover'>\n",
      "Step: 'np' -> Type: <class 'pymo.preprocessing.Numpyfier'>\n",
      "Step: 'down' -> Type: <class 'pymo.preprocessing.DownSampler'>\n",
      "\n",
      "  FOUND DownSampler object!\n",
      "   Listing all attributes of this object:\n",
      "    rate: 2\n",
      "\n",
      "    Analysis: Found 'rate' = 2\n",
      "   N·∫øu d·ªØ li·ªáu g·ªëc l√† 120 FPS, th√¨ Target FPS = 120 / 2 = 60.0\n",
      "Step: 'stdscale' -> Type: <class 'pymo.preprocessing.ListStandardScaler'>\n",
      "----------------------------------------\n",
      "   Skipped file test (path not set or file not found).\n",
      "   Please rely on the 'Attribute' info above.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import joblib\n",
    "import numpy as np\n",
    "PYOM_DIR = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/pymo\"\n",
    "if PYOM_DIR not in sys.path:\n",
    "    sys.path.insert(0, PYOM_DIR)\n",
    "from pymo.parsers import BVHParser\n",
    "\n",
    "try:\n",
    "    pipeline = joblib.load('global_pipeline.pkl')\n",
    "    print(\" Pipeline loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\" Error: 'global_pipeline.pkl' not found.\")\n",
    "    exit()\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(\"Pipeline Steps Inspection:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "found_downsampler = False\n",
    "\n",
    "for name, step in pipeline.steps:\n",
    "    step_type = str(type(step))\n",
    "    print(f\"Step: '{name}' -> Type: {step_type}\")\n",
    "    \n",
    "    if 'DownSampler' in step_type:\n",
    "        found_downsampler = True\n",
    "        print(f\"\\n  FOUND DownSampler object!\")\n",
    "        print(f\"   Listing all attributes of this object:\")\n",
    "        \n",
    "        for attr, value in step.__dict__.items():\n",
    "            if not attr.startswith('_'): \n",
    "                print(f\"    {attr}: {value}\")\n",
    "                \n",
    "        if hasattr(step, 'rate'):\n",
    "            print(f\"\\n    Analysis: Found 'rate' = {step.rate}\")\n",
    "            print(f\"   N·∫øu d·ªØ li·ªáu g·ªëc l√† 120 FPS, th√¨ Target FPS = 120 / {step.rate} = {120/step.rate}\")\n",
    "        elif hasattr(step, 'target_fps'):\n",
    "             print(f\"\\n    Analysis: Found 'target_fps' = {step.target_fps}\")\n",
    "\n",
    "print(\"-\" * 40)\n",
    "\n",
    "bvh_file = \"./datasets/BEAT/10/10_kieks_0_1_1.bvh\"\n",
    "import os\n",
    "if os.path.exists(bvh_file):\n",
    "    print(f\"Testing on file: {bvh_file}\")\n",
    "    parser = BVHParser()\n",
    "    data = parser.parse(bvh_file)\n",
    "    print(f\"Original frames: {data.values.shape[0]}\")\n",
    "    processed = pipeline.transform([data])\n",
    "    new_frames = processed[0].shape[0]\n",
    "    ratio = data.values.shape[0] / new_frames\n",
    "    print(f\"Reduction Ratio: {ratio}\")\n",
    "    print(f\"==> ESTIMATED FPS (Assuming 120Hz input): {120 / ratio}\")\n",
    "else:\n",
    "    print(\"   Skipped file test (path not set or file not found).\")\n",
    "    print(\"   Please rely on the 'Attribute' info above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3acb60",
   "metadata": {},
   "source": [
    "### Chuy·ªÉn dataset bvh sang npy theo t·ª´ng segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb50064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# for i in range(1, 31):\n",
    "#     print(\"ƒêang ch·∫°y preprocess_data.py...\")\n",
    "#     command2 = f'python \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/preprocess_data.py\" --parent-dir \"{base_dir}\" --out-root \"{out_dir}\" --folders \"{i}\" --fps 60'\n",
    "#     os.system(command2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98171b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/preprocess_data.py\" --parent-dir \"{base_dir}\" --out-root \"{out_dir}\" --folders \"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df8ec042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from textgrid import TextGrid\n",
    "\n",
    "# tg = TextGrid.fromFile(\"/home/serverai/ltdoanh/Motion_Diffusion/datasets/BEAT/5/5_stewart_0_82_82.TextGrid\")\n",
    "# print(f\"Number of tiers: {len(tg)}\")\n",
    "# for i, tier in enumerate(tg):\n",
    "#     print(f\"Tier {i}: {tier.name}, intervals: {len(tier.intervals)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74444f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T√¨m th·∫•y 30 th∆∞ m·ª•c con: ['1', '2', '3', '4', '5'] ... 30\n",
      "\n",
      "--- Ki·ªÉm tra chi ti·∫øt th∆∞ m·ª•c 1 ---\n",
      "File m·∫´u: 1_wayne_0_49_49_sentence_002.npy\n",
      "Shape: (465, 264)\n",
      "S·ªë chi·ªÅu: 264\n",
      "\n",
      "========================================\n",
      "K·∫æT QU·∫¢ KI·ªÇM TRA TO√ÄN B·ªò\n",
      "========================================\n",
      "‚úÖ D·ªÆ LI·ªÜU NH·∫§T QU√ÅN. To√†n b·ªô dataset c√≥ s·ªë chi·ªÅu: 264\n",
      "\n",
      "=> K·∫æT LU·∫¨N: D·ªØ li·ªáu chu·∫©n BEAT ƒë·∫ßy ƒë·ªß (264 chi·ªÅu).\n",
      "   1. Code t√≠nh index c≈© c·ªßa t√¥i KH√îNG d√πng ƒë∆∞·ª£c.\n",
      "   2. B·∫°n c·∫ßn gi·ªØ nguy√™n 'dim_pose = 264'.\n",
      "   3. B√°o l·∫°i t√¥i ƒë·ªÉ t√¥i vi·∫øt h√†m map index m·ªõi cho chu·∫©n 264.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n g·ªëc ch·ª©a c√°c th∆∞ m·ª•c con 1, 2, ..., 30\n",
    "base_path = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/BEAT_numpy/npy\"\n",
    "\n",
    "# L·∫•y danh s√°ch t·∫•t c·∫£ c√°c th∆∞ m·ª•c con (1, 2, 3...)\n",
    "sub_dirs = sorted([d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))], key=lambda x: int(x) if x.isdigit() else 0)\n",
    "\n",
    "print(f\"T√¨m th·∫•y {len(sub_dirs)} th∆∞ m·ª•c con: {sub_dirs[:5]} ... {sub_dirs[-1]}\")\n",
    "\n",
    "dims_found = set()\n",
    "consistent = True\n",
    "\n",
    "for sub_dir in sub_dirs:\n",
    "    dir_path = os.path.join(base_path, sub_dir)\n",
    "    \n",
    "    # L·∫•y danh s√°ch file npy trong folder n√†y\n",
    "    files = [f for f in os.listdir(dir_path) if f.endswith('.npy')]\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"[C·∫¢NH B√ÅO] Th∆∞ m·ª•c {sub_dir} r·ªóng!\")\n",
    "        continue\n",
    "        \n",
    "    # L·∫•y ng·∫´u nhi√™n 1 file ƒë·ªÉ ki·ªÉm tra\n",
    "    sample_file = random.choice(files)\n",
    "    file_path = os.path.join(dir_path, sample_file)\n",
    "    \n",
    "    try:\n",
    "        data = np.load(file_path)\n",
    "        feature_dim = data.shape[-1]\n",
    "        dims_found.add(feature_dim)\n",
    "        \n",
    "        # Ch·ªâ in chi ti·∫øt cho th∆∞ m·ª•c ƒë·∫ßu ti√™n ƒë·ªÉ ƒë·ª° r·ªëi\n",
    "        if sub_dir == '1':\n",
    "            print(f\"\\n--- Ki·ªÉm tra chi ti·∫øt th∆∞ m·ª•c {sub_dir} ---\")\n",
    "            print(f\"File m·∫´u: {sample_file}\")\n",
    "            print(f\"Shape: {data.shape}\")\n",
    "            print(f\"S·ªë chi·ªÅu: {feature_dim}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"L·ªói ƒë·ªçc file {file_path}: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"K·∫æT QU·∫¢ KI·ªÇM TRA TO√ÄN B·ªò\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "if len(dims_found) == 1:\n",
    "    dim = dims_found.pop()\n",
    "    print(f\"‚úÖ D·ªÆ LI·ªÜU NH·∫§T QU√ÅN. To√†n b·ªô dataset c√≥ s·ªë chi·ªÅu: {dim}\")\n",
    "    \n",
    "    if dim == 171:\n",
    "        print(\"\\n=> K·∫æT LU·∫¨N: D·ªØ li·ªáu c·ªßa b·∫°n ch·ªâ c√≥ Position (57 kh·ªõp x 3).\")\n",
    "        print(\"   1. Code t√≠nh index 'joint_idx * 3' c·ªßa t√¥i l√† ƒê√öNG.\")\n",
    "        print(\"   2. B·∫°n C·∫¶N s·ª≠a file 'vq_diffusion.py': ƒë·∫∑t dim_pose = 171\")\n",
    "        print(\"   3. ƒê·ª´ng qu√™n truy·ªÅn hand_indices, body_indices v√†o model.\")\n",
    "        \n",
    "    elif dim == 264:\n",
    "        print(\"\\n=> K·∫æT LU·∫¨N: D·ªØ li·ªáu chu·∫©n BEAT ƒë·∫ßy ƒë·ªß (264 chi·ªÅu).\")\n",
    "        print(\"   1. Code t√≠nh index c≈© c·ªßa t√¥i KH√îNG d√πng ƒë∆∞·ª£c.\")\n",
    "        print(\"   2. B·∫°n c·∫ßn gi·ªØ nguy√™n 'dim_pose = 264'.\")\n",
    "        print(\"   3. B√°o l·∫°i t√¥i ƒë·ªÉ t√¥i vi·∫øt h√†m map index m·ªõi cho chu·∫©n 264.\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\n=> K·∫æT LU·∫¨N: S·ªë chi·ªÅu l·∫° ({dim}). C·∫ßn ki·ªÉm tra l·∫°i pipeline x·ª≠ l√Ω d·ªØ li·ªáu.\")\n",
    "else:\n",
    "    print(f\"‚ùå D·ªÆ LI·ªÜU KH√îNG NH·∫§T QU√ÅN! T√¨m th·∫•y c√°c s·ªë chi·ªÅu kh√°c nhau: {dims_found}\")\n",
    "    print(\"=> B·∫°n c·∫ßn x·ª≠ l√Ω l·∫°i d·ªØ li·ªáu ƒë·ªÉ ƒë·∫£m b·∫£o ƒë·ªìng b·ªô.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9950bdf",
   "metadata": {},
   "source": [
    "### Visual data b·∫±ng mean/std chu·∫©n t√≠nh t·ª´ b·ªô d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e9f0d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful!\n",
      "üì¶ Loading pipeline from: /home/serverai/ltdoanh/Motion_Diffusion/global_pipeline.pkl\n",
      "‚úÖ Pipeline loaded!\n",
      "   Mean shape: (264,), First 5 values: [-0.05385426  8.01535389 -3.75700843  0.02310581 18.43449509]\n",
      "   Std shape: (264,),  First 5 values: [1.17785947 2.32203438 1.24202695 1.89584987 2.06571074]\n",
      "\n",
      "üé¨ Visualizing: 1_wayne_0_1_1_sentence_010.npy\n",
      "   Frame: 50\n",
      "   Motion shape: (261, 264)\n",
      "   Performing inverse transform...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Numpyfier' object has no attribute 'org_mocap_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Inverse transform\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Performing inverse transform...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m reconstructed \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmotion_data\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   ‚úÖ Reconstructed shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreconstructed[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Visualize 2D\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/pipeline.py:1138\u001b[0m, in \u001b[0;36mPipeline.inverse_transform\u001b[0;34m(self, X, **params)\u001b[0m\n\u001b[1;32m   1136\u001b[0m reverse_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter()))\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m reverse_iter:\n\u001b[0;32m-> 1138\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m~/ltdoanh/Motion_Diffusion/datasets/pymo/pymo/preprocessing.py:313\u001b[0m, in \u001b[0;36mNumpyfier.inverse_transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m    309\u001b[0m Q \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m track \u001b[38;5;129;01min\u001b[39;00m X:\n\u001b[0;32m--> 313\u001b[0m     new_mocap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morg_mocap_\u001b[49m\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m    314\u001b[0m     time_index \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_timedelta([f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(track\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])], unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    316\u001b[0m     new_df \u001b[38;5;241m=\u001b[39m  pd\u001b[38;5;241m.\u001b[39mDataFrame(data\u001b[38;5;241m=\u001b[39mtrack, index\u001b[38;5;241m=\u001b[39mtime_index, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morg_mocap_\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Numpyfier' object has no attribute 'org_mocap_'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Setup path\n",
    "PYOM_DIR = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/pymo\"\n",
    "if PYOM_DIR not in sys.path:\n",
    "    sys.path.insert(0, PYOM_DIR)\n",
    "\n",
    "from pymo.parsers import BVHParser\n",
    "from pymo.preprocessing import *\n",
    "from pymo.viz_tools import draw_stickfigure, draw_stickfigure3d\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "\n",
    "# ===== CELL 2: Load Pipeline =====\n",
    "pipeline_path = \"/home/serverai/ltdoanh/Motion_Diffusion/global_pipeline.pkl\"\n",
    "\n",
    "print(f\"üì¶ Loading pipeline from: {pipeline_path}\")\n",
    "\n",
    "pipeline = joblib.load(pipeline_path)\n",
    "print(f\"‚úÖ Pipeline loaded!\")\n",
    "print(f\"   Mean shape: {pipeline.named_steps['stdscale'].data_mean_.shape}, First 5 values: {pipeline.named_steps['stdscale'].data_mean_[:5]}\")\n",
    "print(f\"   Std shape: {pipeline.named_steps['stdscale'].data_std_.shape},  First 5 values: {pipeline.named_steps['stdscale'].data_std_[:5]}\")\n",
    "\n",
    "# ===== CELL 3: Load and Visualize Motion =====\n",
    "npy_path = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/BEAT_numpy/npy/1/1_wayne_0_1_1_sentence_010.npy\"\n",
    "frame = 50\n",
    "\n",
    "print(f\"\\nüé¨ Visualizing: {os.path.basename(npy_path)}\")\n",
    "print(f\"   Frame: {frame}\")\n",
    "\n",
    "# Load motion data\n",
    "motion_data = np.load(npy_path)\n",
    "print(f\"   Motion shape: {motion_data.shape}\")\n",
    "\n",
    "# Inverse transform\n",
    "print(\"   Performing inverse transform...\")\n",
    "reconstructed = pipeline.inverse_transform([motion_data])\n",
    "print(f\"   ‚úÖ Reconstructed shape: {reconstructed[0].values.shape}\")\n",
    "\n",
    "# Visualize 2D\n",
    "print(\"\\n   Creating 2D visualization...\")\n",
    "fig1 = plt.figure(figsize=(10, 8))\n",
    "draw_stickfigure(reconstructed[0], frame=frame)\n",
    "plt.title(f\"2D Stick Figure - Frame {frame}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"   ‚úÖ 2D plot displayed!\")\n",
    "\n",
    "# Visualize 3D\n",
    "print(\"\\n   Creating 3D visualization...\")\n",
    "fig2 = plt.figure(figsize=(10, 8))\n",
    "draw_stickfigure3d(reconstructed[0], frame=frame)\n",
    "plt.title(f\"3D Stick Figure - Frame {frame}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"   ‚úÖ 3D plot displayed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9286cdc0",
   "metadata": {},
   "source": [
    "### Visual data b·∫±ng mean/std t·ª´ model ƒë∆∞·ª£c hu·∫•n luy·ªán - Latent MotionDiffuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65c2f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful!\n",
      "\n",
      "üî® Creating pipeline from scratch...\n",
      "   Fitting on BVH sample: 1_wayne_0_1_1.bvh\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/serverai/ltdoanh/Motion_Diffusion/datasets/BEAT/1/1_wayne_0_1_1.bvh'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Fitting on BVH sample: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(bvh_sample_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m parser \u001b[38;5;241m=\u001b[39m BVHParser()\n\u001b[0;32m---> 43\u001b[0m parsed_data \u001b[38;5;241m=\u001b[39m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbvh_sample_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mfit([parsed_data])\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   ‚úÖ Pipeline fitted (structure learned)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/ltdoanh/Motion_Diffusion/datasets/pymo/pymo/parsers.py:79\u001b[0m, in \u001b[0;36mBVHParser.parse\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename):\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m bvh_file:\n\u001b[1;32m     80\u001b[0m         raw_contents \u001b[38;5;241m=\u001b[39m bvh_file\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     81\u001b[0m     tokens, remainder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscanner\u001b[38;5;241m.\u001b[39mscan(raw_contents)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/serverai/ltdoanh/Motion_Diffusion/datasets/BEAT/1/1_wayne_0_1_1.bvh'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "\n",
    "# Setup path\n",
    "PYOM_DIR = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/pymo\"\n",
    "if PYOM_DIR not in sys.path:\n",
    "    sys.path.insert(0, PYOM_DIR)\n",
    "\n",
    "from pymo.parsers import BVHParser\n",
    "from pymo.preprocessing import *\n",
    "from pymo.viz_tools import draw_stickfigure, draw_stickfigure3d\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "\n",
    "# ===== CREATE PIPELINE WITH META =====\n",
    "print(\"\\nüî® Creating pipeline from scratch...\")\n",
    "\n",
    "# 1. Create empty pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('param', MocapParameterizer('position')),\n",
    "    ('rcpn', RootCentricPositionNormalizer()),\n",
    "    ('delta', RootTransformer('abdolute_translation_deltas')),\n",
    "    ('const', ConstantsRemover()),\n",
    "    ('np', Numpyfier()),\n",
    "    ('down', DownSampler(2)),\n",
    "    ('stdscale', ListStandardScaler())\n",
    "])\n",
    "\n",
    "# 2. Fit pipeline on sample BVH (to learn structure)\n",
    "bvh_sample_path = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/BEAT/1/1_wayne_0_1_1.bvh\"\n",
    "print(f\"   Fitting on BVH sample: {os.path.basename(bvh_sample_path)}\")\n",
    "\n",
    "parser = BVHParser()\n",
    "parsed_data = parser.parse(bvh_sample_path)\n",
    "pipeline.fit([parsed_data])\n",
    "print(\"   ‚úÖ Pipeline fitted (structure learned)\")\n",
    "\n",
    "# 3. Load mean/std from meta directory\n",
    "# meta_dir = \"/home/serverai/ltdoanh/Motion_Diffusion/checkpoints/beat/test/meta\"\n",
    "# mean_path = os.path.join(meta_dir, \"mean.npy\")\n",
    "# std_path = os.path.join(meta_dir, \"std.npy\")\n",
    "ckpt_path = \"/home/serverai/ltdoanh/Motion_Diffusion/checkpoints/beat/vq_diffusion/model/latest.pt\"  # Thay ƒë∆∞·ªùng d·∫´n file c·ªßa b·∫°n v√†o ƒë√¢y\n",
    "\n",
    "try:\n",
    "    # 2. Load checkpoint\n",
    "    # map_location='cpu' gi√∫p tr√°nh l·ªói n·∫øu m√°y b·∫°n kh√¥ng c√≥ GPU gi·ªëng l√∫c train\n",
    "    checkpoint = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
    "\n",
    "    # 3. Ki·ªÉm tra v√† l·∫•y mean, std\n",
    "    if 'mean' in checkpoint and 'std' in checkpoint:\n",
    "        mean = checkpoint['mean']\n",
    "        std = checkpoint['std']\n",
    "\n",
    "        print(\"--- ƒê√£ t√¨m th·∫•y Mean v√† Std ---\")\n",
    "        print(f\"Shape c·ªßa Mean: {mean.shape}\")\n",
    "        print(f\"Shape c·ªßa Std: {std.shape}\")\n",
    "        \n",
    "        # In th·ª≠ v√†i gi√° tr·ªã ƒë·∫ßu\n",
    "        print(f\"Mean (5 gi√° tr·ªã ƒë·∫ßu): {mean[:5]}\")\n",
    "        print(f\"Std (5 gi√° tr·ªã ƒë·∫ßu): {std[:5]}\")\n",
    "        \n",
    "        # 4. (T√πy ch·ªçn) L∆∞u l·∫°i ra file .npy ƒë·ªÉ d√πng vi·ªác kh√°c n·∫øu c·∫ßn\n",
    "        # np.save('mean.npy', mean)\n",
    "        # np.save('std.npy', std)\n",
    "        # print(\"ƒê√£ l∆∞u ra file .npy\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Kh√¥ng t√¨m th·∫•y key 'mean' ho·∫∑c 'std' trong file .pt n√†y.\")\n",
    "        print(\"C√°c keys hi·ªán c√≥:\", checkpoint.keys())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"C√≥ l·ªói x·∫£y ra: {e}\")\n",
    "\n",
    "# 4. Override pipeline's mean/std with meta values\n",
    "print(\"\\nüîß Overriding pipeline statistics with meta values...\")\n",
    "pipeline.named_steps['stdscale'].data_mean_ = mean\n",
    "pipeline.named_steps['stdscale'].data_std_ = std\n",
    "print(\"   ‚úÖ Pipeline updated with meta statistics!\")\n",
    "\n",
    "# ===== VISUALIZE MOTION =====\n",
    "npy_path = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/BEAT_numpy/npy/2/2_scott_0_2_2_sentence_000.npy\"\n",
    "frame = 50\n",
    "\n",
    "print(f\"\\nüé¨ Visualizing: {os.path.basename(npy_path)}\")\n",
    "print(f\"   Frame: {frame}\")\n",
    "\n",
    "# Load motion data\n",
    "motion_data = np.load(npy_path)\n",
    "print(f\"   Motion shape: {motion_data.shape}\")\n",
    "\n",
    "# Inverse transform\n",
    "print(\"   Performing inverse transform...\")\n",
    "reconstructed = pipeline.inverse_transform([motion_data])\n",
    "print(f\"   ‚úÖ Reconstructed shape: {reconstructed[0].values.shape}\")\n",
    "\n",
    "# Visualize 2D\n",
    "print(\"\\n   Creating 2D visualization...\")\n",
    "fig1 = plt.figure(figsize=(10, 8))\n",
    "draw_stickfigure(reconstructed[0], frame=frame)\n",
    "plt.title(f\"2D Stick Figure (Meta) - Frame {frame}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"   ‚úÖ 2D plot displayed!\")\n",
    "\n",
    "# Visualize 3D\n",
    "print(\"\\n   Creating 3D visualization...\")\n",
    "fig2 = plt.figure(figsize=(10, 8))\n",
    "draw_stickfigure3d(reconstructed[0], frame=frame)\n",
    "plt.title(f\"3D Stick Figure (Meta) - Frame {frame}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"   ‚úÖ 3D plot displayed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6dfb53",
   "metadata": {},
   "source": [
    "### Visual data b·∫±ng mean/std t·ª´ model ƒë∆∞·ª£c hu·∫•n luy·ªán - MotionDiffuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94aa49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /home/serverai/ltdoanh/Motion_Diffusion/checkpoints/beat/test/opt.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:16<00:00, 58.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 264)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from models import MotionTransformer\n",
    "from utils.get_opt import get_opt\n",
    "import numpy as np\n",
    "from os.path import join as pjoin\n",
    "from trainers import DDPMTrainer\n",
    "\n",
    "encoder = MotionTransformer(\n",
    "        input_feats=264,\n",
    "        num_frames=360,\n",
    "        num_layers=8,\n",
    "        latent_dim=512,\n",
    "        no_clip=False,\n",
    "        no_eff=False)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "opt_path = \"/home/serverai/ltdoanh/Motion_Diffusion/checkpoints/beat/test/opt.txt\"\n",
    "opt = get_opt(opt_path, device)\n",
    "opt.do_denoise = True\n",
    "encoder.to(opt.device)\n",
    "mean = np.load(pjoin(opt.meta_dir, 'mean.npy'))\n",
    "std = np.load(pjoin(opt.meta_dir, 'std.npy'))\n",
    "trainer = DDPMTrainer(opt, encoder)\n",
    "trainer.load(pjoin('/home/serverai/ltdoanh/Motion_Diffusion/checkpoints/beat/test/model/ckpt_e045.tar'))\n",
    "\n",
    "trainer.eval_mode()\n",
    "\n",
    "\n",
    "result_dict = {}\n",
    "with torch.no_grad():\n",
    "        caption = [\"the first thing i like to do on weekends is relaxing and i'll go shopping if i'm not that tired\"]\n",
    "        m_lens = torch.LongTensor([64]).to(device)\n",
    "        pred_motions = trainer.generate(caption, m_lens, 264)\n",
    "        motion = pred_motions[0].cpu().numpy()\n",
    "        print(motion.shape)\n",
    "opt.result_dir = \"/home/serverai/ltdoanh/Motion_Diffusion/results\"\n",
    "np.save(pjoin(opt.result_dir, 'motion_v0.npy'), motion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc408604",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "\n",
    "# Setup path\n",
    "PYOM_DIR = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/pymo\"\n",
    "if PYOM_DIR not in sys.path:\n",
    "    sys.path.insert(0, PYOM_DIR)\n",
    "\n",
    "from pymo.parsers import BVHParser\n",
    "from pymo.preprocessing import *\n",
    "from pymo.viz_tools import draw_stickfigure, draw_stickfigure3d\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "\n",
    "# ===== 1. CREATE PIPELINE WITH META =====\n",
    "print(\"\\nüî® Creating pipeline from scratch...\")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('param', MocapParameterizer('position')),\n",
    "    ('rcpn', RootCentricPositionNormalizer()),\n",
    "    ('delta', RootTransformer('abdolute_translation_deltas')),\n",
    "    ('const', ConstantsRemover()),\n",
    "    ('np', Numpyfier()),\n",
    "    ('down', DownSampler(2)),\n",
    "    ('stdscale', ListStandardScaler())\n",
    "])\n",
    "\n",
    "# ===== 2. FIT PIPELINE (Structure Learning) =====\n",
    "# C·∫ßn fit v√†o m·ªôt file BVH m·∫´u ƒë·ªÉ pipeline h·ªçc ƒë∆∞·ª£c c·∫•u tr√∫c x∆∞∆°ng (skeleton)\n",
    "bvh_sample_path = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/2_scott_0_55_55.bvh\"\n",
    "print(f\"   Fitting on BVH sample: {os.path.basename(bvh_sample_path)}\")\n",
    "\n",
    "if os.path.exists(bvh_sample_path):\n",
    "    parser = BVHParser()\n",
    "    parsed_data = parser.parse(bvh_sample_path)\n",
    "    pipeline.fit([parsed_data])\n",
    "    print(\"   ‚úÖ Pipeline fitted (structure learned)\")\n",
    "else:\n",
    "    print(f\"‚ùå Error: BVH file not found at {bvh_sample_path}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ===== 3. LOAD MEAN/STD FROM META (.npy files) =====\n",
    "meta_dir = \"/home/serverai/ltdoanh/Motion_Diffusion/checkpoints/beat/test/meta\"\n",
    "mean_path = os.path.join(meta_dir, \"mean.npy\")\n",
    "std_path = os.path.join(meta_dir, \"std.npy\")\n",
    "\n",
    "print(\"\\nüì• Loading Mean/Std from Meta files...\")\n",
    "try:\n",
    "    # Load tr·ª±c ti·∫øp t·ª´ file .npy\n",
    "    mean = np.load(mean_path)\n",
    "    std = np.load(std_path)\n",
    "\n",
    "    print(\"   ‚úÖ ƒê√£ t√¨m th·∫•y Mean v√† Std t·ª´ file .npy\")\n",
    "    print(f\"   Shape c·ªßa Mean: {mean.shape}\")\n",
    "    print(f\"   Shape c·ªßa Std: {std.shape}\")\n",
    "    \n",
    "    # In th·ª≠ v√†i gi√° tr·ªã ƒë·∫ßu ƒë·ªÉ ki·ªÉm tra\n",
    "    print(f\"   Mean (5 gi√° tr·ªã ƒë·∫ßu): {mean[:5]}\")\n",
    "    \n",
    "    # Override pipeline statistics\n",
    "    print(\"\\nüîß Overriding pipeline statistics with meta values...\")\n",
    "    pipeline.named_steps['stdscale'].data_mean_ = mean\n",
    "    pipeline.named_steps['stdscale'].data_std_ = std\n",
    "    print(\"   ‚úÖ Pipeline updated with meta statistics!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå C√≥ l·ªói x·∫£y ra khi load meta: {e}\")\n",
    "    # N·∫øu kh√¥ng c√≥ mean/std th√¨ visualization s·∫Ω b·ªã sai, n√™n d·ª´ng ho·∫∑c c·∫£nh b√°o\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0593a582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 4. VISUALIZE MOTION =====\n",
    "npy_path = \"/home/serverai/ltdoanh/Motion_Diffusion/results/motion_v0.npy\"\n",
    "frame = 0\n",
    "\n",
    "print(f\"\\nüé¨ Visualizing: {os.path.basename(npy_path)}\")\n",
    "\n",
    "if os.path.exists(npy_path):\n",
    "    # Load motion data\n",
    "    motion_data = np.load(npy_path)\n",
    "    print(f\"   Motion shape: {motion_data.shape}\")\n",
    "\n",
    "    # Inverse transform\n",
    "    print(\"   Performing inverse transform...\")\n",
    "    # L∆∞u √Ω: inverse_transform tr·∫£ v·ªÅ list c√°c object MocapData\n",
    "    reconstructed = pipeline.inverse_transform([motion_data])\n",
    "    print(f\"   ‚úÖ Reconstructed shape: {reconstructed[0].values.shape}\")\n",
    "\n",
    "    # Visualize 2D\n",
    "    print(\"\\n   Creating 2D visualization...\")\n",
    "    fig1 = plt.figure(figsize=(10, 8))\n",
    "    draw_stickfigure(reconstructed[0], frame=frame)\n",
    "    plt.title(f\"2D Stick Figure (Meta Mean/Std) - Frame {frame}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Visualize 3D\n",
    "    print(\"\\n   Creating 3D visualization...\")\n",
    "    fig2 = plt.figure(figsize=(10, 8))\n",
    "    draw_stickfigure3d(reconstructed[0], frame=frame)\n",
    "    plt.title(f\"3D Stick Figure (Meta Mean/Std) - Frame {frame}\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"   ‚úÖ Done!\")\n",
    "else:\n",
    "    print(f\"‚ùå Error: Motion file not found at {npy_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27914097",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83329b36",
   "metadata": {},
   "source": [
    "### Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3d1766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading checkpoint: /home/serverai/ltdoanh/Motion_Diffusion/checkpoints/beat/vqkl_diffusion_hierarchical/model/best_model.pt\n",
      "üîç Sample keys in checkpoint:\n",
      "   vqkl.encoder.model.0.weight\n",
      "   vqkl.encoder.model.0.bias\n",
      "   vqkl.encoder.model.2.0.weight\n",
      "   vqkl.encoder.model.2.0.bias\n",
      "   vqkl.encoder.model.2.1.model.0.conv1.weight\n",
      "üîç Detected num_quantizers: 10\n",
      "üìä Loaded 685 keys for Transformer\n",
      "üìä Loaded 128 keys for RVQVAE\n",
      "üîß Initializing MotionTransformer...\n",
      "üîß Initializing RVQVAE...\n",
      "üöÄ Starting Inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:17<00:00, 57.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Latent generated shape: torch.Size([1, 45, 512])\n",
      "   Decoding with RVQVAE...\n",
      "üéâ Final Motion Shape: (1, 360, 264)\n",
      "üíæ Saved to: /home/serverai/ltdoanh/Motion_Diffusion/results/inference-latest.npy\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from models import MotionTransformer\n",
    "from trainers import DDPMTrainer\n",
    "from models.vq.model import RVQVAE \n",
    "\n",
    "# ==========================================\n",
    "# 1. C·∫•u h√¨nh Inference\n",
    "# ==========================================\n",
    "class InferenceConfig:\n",
    "    def __init__(self):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        self.is_train = False\n",
    "        self.schedule_sampler = 'uniform'   \n",
    "\n",
    "        # --- C·∫•u h√¨nh Diffusion ---\n",
    "        self.input_feats = 512     # Latent Dimension\n",
    "        self.num_frames = 45      # Latent Length (360 / 8)\n",
    "        self.num_layers = 8\n",
    "        self.latent_dim = 512\n",
    "        self.ff_size = 1024\n",
    "        self.num_heads = 8\n",
    "        self.dropout = 0.1\n",
    "        self.activation = \"gelu\"\n",
    "        self.dataset_name = 'beat' \n",
    "        self.do_denoise = True\n",
    "        self.noise_schedule = 'cosine'\n",
    "        self.diffusion_steps = 1000\n",
    "        self.no_clip = False\n",
    "        self.no_eff = False\n",
    "        self.result_dir = \"/home/serverai/ltdoanh/Motion_Diffusion/results\"\n",
    "\n",
    "# Class gi·∫£ l·∫≠p args cho RVQVAE\n",
    "class VQArgs:\n",
    "    def __init__(self):\n",
    "        # C√°c gi√° tr·ªã m·∫∑c ƒë·ªãnh, s·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t logic b√™n d∆∞·ªõi\n",
    "        self.num_quantizers = 1 \n",
    "        self.shared_codebook = False\n",
    "        self.quantize_dropout_prob = 0.0\n",
    "        self.mu = 0.99 # Cho QuantizerEMA\n",
    "\n",
    "opt = InferenceConfig()\n",
    "vq_args = VQArgs()\n",
    "\n",
    "# ==========================================\n",
    "# 2. Load Checkpoint & T√°ch Weights\n",
    "# ==========================================\n",
    "ckpt_path = \"/home/serverai/ltdoanh/Motion_Diffusion/checkpoints/beat/vqkl_diffusion_hierarchical/model/best_model.pt\"\n",
    "print(f\"üìÇ Loading checkpoint: {ckpt_path}\")\n",
    "\n",
    "# Load to√†n b·ªô checkpoint\n",
    "checkpoint = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
    "\n",
    "state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "# DEBUG: In ra m·ªôt v√†i key ƒë·ªÉ ki·ªÉm tra prefix th·ª±c t·∫ø\n",
    "print(\"üîç Sample keys in checkpoint:\")\n",
    "for i, k in enumerate(list(state_dict.keys())[:5]):\n",
    "    print(f\"   {k}\")\n",
    "\n",
    "# --- T·ª± ƒë·ªông ph√°t hi·ªán s·ªë l∆∞·ª£ng Quantizers t·ª´ Checkpoint ---\n",
    "max_layer_idx = 0\n",
    "found_quantizer = False\n",
    "for k in state_dict.keys():\n",
    "    # FIX: Check prefix 'vqkl.' instead of 'vqvae.'\n",
    "    if \"quantizer.layers.\" in k: \n",
    "        found_quantizer = True\n",
    "        try:\n",
    "            # T√¨m s·ªë n·∫±m sau 'layers.'\n",
    "            parts = k.split('.')\n",
    "            layer_idx = int(parts[parts.index('layers') + 1])\n",
    "            if layer_idx > max_layer_idx:\n",
    "                max_layer_idx = layer_idx\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "if found_quantizer:\n",
    "    vq_args.num_quantizers = max_layer_idx + 1\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Warning: Could not detect quantizer layers. Defaulting to 1.\")\n",
    "    vq_args.num_quantizers = 1\n",
    "    \n",
    "print(f\"üîç Detected num_quantizers: {vq_args.num_quantizers}\")\n",
    "\n",
    "# --- T√°ch Dictionary ---\n",
    "trans_dict = {}\n",
    "vqvae_dict = {}\n",
    "\n",
    "for k, v in state_dict.items():\n",
    "    if k.startswith('transformer.'):\n",
    "        trans_dict[k[12:]] = v  \n",
    "    # FIX: Prefix l√† 'vqkl.' (5 chars) ch·ª© kh√¥ng ph·∫£i 'vqvae.' (6 chars)\n",
    "    elif k.startswith('vqkl.'):\n",
    "        vqvae_dict[k[5:]] = v   \n",
    "\n",
    "print(f\"üìä Loaded {len(trans_dict)} keys for Transformer\")\n",
    "print(f\"üìä Loaded {len(vqvae_dict)} keys for RVQVAE\")\n",
    "\n",
    "# A. Motion Transformer\n",
    "print(\"üîß Initializing MotionTransformer...\")\n",
    "encoder = MotionTransformer(\n",
    "    input_feats=opt.input_feats,\n",
    "    num_frames=opt.num_frames,\n",
    "    num_layers=opt.num_layers,\n",
    "    latent_dim=opt.latent_dim,\n",
    "    num_heads=opt.num_heads,\n",
    "    ff_size=opt.ff_size,\n",
    "    no_clip=opt.no_clip,\n",
    "    no_eff=opt.no_eff\n",
    ")\n",
    "encoder.load_state_dict(trans_dict, strict=True)\n",
    "encoder.to(opt.device).eval()\n",
    "\n",
    "# B. RVQVAE\n",
    "print(\"üîß Initializing RVQVAE...\")\n",
    "# L∆∞u √Ω: C√°c tham s·ªë d∆∞·ªõi ƒë√¢y ph·∫£i kh·ªõp v·ªõi file config l√∫c train VQVAE c·ªßa b·∫°n.\n",
    "# T√¥i ƒëang ƒë·ªÉ c√°c gi√° tr·ªã ph·ªï bi·∫øn d·ª±a tr√™n file model.py\n",
    "vqvae_model = RVQVAE(\n",
    "    args=vq_args,\n",
    "    input_width=264,       # BEAT dataset th∆∞·ªùng l√† 264\n",
    "    nb_code=1024,          # FIX: Updated from 512 to 1024 to match checkpoint\n",
    "    code_dim=512, \n",
    "    output_emb_width=512, \n",
    "    down_t=3, \n",
    "    stride_t=2, \n",
    "    width=512, \n",
    "    depth=3, \n",
    "    dilation_growth_rate=3,\n",
    "    activation='relu',\n",
    "    norm=None,\n",
    "    embed_dim=512,         # FIX: Added for VQ-KL (matches checkpoint)\n",
    "    double_z=True          # FIX: Enable VQ-KL mode (loads quant_conv/post_quant_conv)\n",
    ")\n",
    "vqvae_model.load_state_dict(vqvae_dict, strict=True)\n",
    "vqvae_model.to(opt.device).eval()\n",
    "\n",
    "# ==========================================\n",
    "# 4. Inference\n",
    "# ==========================================\n",
    "trainer = DDPMTrainer(opt, encoder)\n",
    "\n",
    "# Inject mean/std (Quan tr·ªçng cho qu√° tr√¨nh decode cu·ªëi c√πng)\n",
    "trainer.mean = checkpoint['mean']\n",
    "trainer.std = checkpoint['std']\n",
    "\n",
    "print(\"üöÄ Starting Inference...\")\n",
    "os.makedirs(opt.result_dir, exist_ok=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    caption = [\"the first thing i like to do on weekends is relaxing and i'll go shopping if i'm not that tired\"]\n",
    "    \n",
    "    # ƒê·ªô d√†i Latent (45)\n",
    "    m_lens = torch.LongTensor([45]).to(opt.device) \n",
    "    \n",
    "    # 1. Sinh Latent (Diffusion) -> Output: (Batch, Length, Dim) = (1, 45, 512)\n",
    "    pred_latent_list = trainer.generate(caption, m_lens, dim_pose=512)\n",
    "    pred_latent = pred_latent_list[0]\n",
    "\n",
    "    if pred_latent.dim() == 2:\n",
    "        pred_latent = pred_latent.unsqueeze(0)  # Th√™m batch dim n·∫øu c·∫ßn\n",
    "\n",
    "    print(f\"   Latent generated shape: {pred_latent.shape}\")\n",
    "\n",
    "    # 2. Decode b·∫±ng RVQVAE\n",
    "    # RVQVAE Decoder c·∫ßn input: (Batch, Channel, Length) -> C·∫ßn permute\n",
    "    latent_input = pred_latent.permute(0, 2, 1) # -> (1, 512, 45)\n",
    "    \n",
    "    print(\"   Decoding with RVQVAE...\")\n",
    "    # G·ªçi tr·ª±c ti·∫øp decoder (b·ªè qua quantizer v√¨ Diffusion ƒë√£ sinh ra latent r·ªìi)\n",
    "    decoded_motion = vqvae_model.decoder(latent_input)\n",
    "    \n",
    "    # 3. Post-process (Permute l·∫°i v·ªÅ: Batch, Length, Channel)\n",
    "    # H√†m postprocess trong model.py: (B, C, T) -> (B, T, C)\n",
    "    motion = vqvae_model.postprocess(decoded_motion).cpu().numpy()\n",
    "\n",
    "    if motion.shape[1] == 264 and motion.shape[2] == 360:\n",
    "        motion = motion.transpose(0, 2, 1)\n",
    "\n",
    "    # motion = motion.cpu().numpy()\n",
    "    \n",
    "    # 4. Denormalize (Gi·∫£i chu·∫©n h√≥a)\n",
    "    # Output c·ªßa VQVAE th∆∞·ªùng v·∫´n l√† normalized data\n",
    "    mean = checkpoint['mean']\n",
    "    std = checkpoint['std']\n",
    "    \n",
    "    # ƒê·∫£m b·∫£o shape kh·ªõp ƒë·ªÉ broadcast\n",
    "    # Motion: (1, 360, 264), Mean: (264,), Std: (264,)\n",
    "    motion = motion * std + mean\n",
    "    \n",
    "    print(f\"üéâ Final Motion Shape: {motion.shape}\")\n",
    "\n",
    "# L∆∞u k·∫øt qu·∫£\n",
    "save_path = os.path.join(opt.result_dir, 'inference-latest.npy')\n",
    "np.save(save_path, motion)\n",
    "print(f\"üíæ Saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95738b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "\n",
    "# Setup path\n",
    "PYOM_DIR = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/pymo\"\n",
    "if PYOM_DIR not in sys.path:\n",
    "    sys.path.insert(0, PYOM_DIR)\n",
    "\n",
    "from pymo.parsers import BVHParser\n",
    "from pymo.preprocessing import *\n",
    "from pymo.viz_tools import draw_stickfigure, draw_stickfigure3d\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "\n",
    "# ===== CREATE PIPELINE WITH META =====\n",
    "print(\"\\nüî® Creating pipeline from scratch...\")\n",
    "\n",
    "# 1. Create empty pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('param', MocapParameterizer('position')),\n",
    "    ('rcpn', RootCentricPositionNormalizer()),\n",
    "    ('delta', RootTransformer('abdolute_translation_deltas')),\n",
    "    ('const', ConstantsRemover()),\n",
    "    ('np', Numpyfier()),\n",
    "    ('down', DownSampler(2)),\n",
    "    ('stdscale', ListStandardScaler())\n",
    "])\n",
    "\n",
    "# 2. Fit pipeline on sample BVH (to learn structure)\n",
    "bvh_sample_path = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/BEAT/1/1_wayne_0_1_1.bvh\"\n",
    "print(f\"   Fitting on BVH sample: {os.path.basename(bvh_sample_path)}\")\n",
    "\n",
    "parser = BVHParser()\n",
    "parsed_data = parser.parse(bvh_sample_path)\n",
    "pipeline.fit([parsed_data])\n",
    "print(\"   ‚úÖ Pipeline fitted (structure learned)\")\n",
    "\n",
    "# 3. Load mean/std from meta directory\n",
    "ckpt_path = \"/home/serverai/ltdoanh/Motion_Diffusion/checkpoints/beat/vqkl_diffusion/model/best_model.pt\"  \n",
    "\n",
    "try:\n",
    "    # 2. Load checkpoint\n",
    "    # map_location='cpu' gi√∫p tr√°nh l·ªói n·∫øu m√°y b·∫°n kh√¥ng c√≥ GPU gi·ªëng l√∫c train\n",
    "    checkpoint = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
    "\n",
    "    # 3. Ki·ªÉm tra v√† l·∫•y mean, std\n",
    "    if 'mean' in checkpoint and 'std' in checkpoint:\n",
    "        mean = checkpoint['mean']\n",
    "        std = checkpoint['std']\n",
    "\n",
    "        print(\"--- ƒê√£ t√¨m th·∫•y Mean v√† Std ---\")\n",
    "        print(f\"Shape c·ªßa Mean: {mean.shape}\")\n",
    "        print(f\"Shape c·ªßa Std: {std.shape}\")\n",
    "        \n",
    "        # In th·ª≠ v√†i gi√° tr·ªã ƒë·∫ßu\n",
    "        print(f\"Mean (5 gi√° tr·ªã ƒë·∫ßu): {mean[:5]}\")\n",
    "        print(f\"Std (5 gi√° tr·ªã ƒë·∫ßu): {std[:5]}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Kh√¥ng t√¨m th·∫•y key 'mean' ho·∫∑c 'std' trong file .pt n√†y.\")\n",
    "        print(\"C√°c keys hi·ªán c√≥:\", checkpoint.keys())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"C√≥ l·ªói x·∫£y ra: {e}\")\n",
    "\n",
    "# 4. Override pipeline's mean/std with meta values\n",
    "print(\"\\nüîß Overriding pipeline statistics with meta values...\")\n",
    "pipeline.named_steps['stdscale'].data_mean_ = mean\n",
    "pipeline.named_steps['stdscale'].data_std_ = std\n",
    "print(\"   ‚úÖ Pipeline updated with meta statistics!\")\n",
    "\n",
    "# ===== VISUALIZE MOTION =====\n",
    "npy_path = \"/home/serverai/ltdoanh/Motion_Diffusion/results/inference-latest.npy\"\n",
    "frame = 50\n",
    "\n",
    "print(f\"\\nüé¨ Visualizing: {os.path.basename(npy_path)}\")\n",
    "print(f\"   Frame: {frame}\")\n",
    "\n",
    "# Load motion data\n",
    "motion_data = np.load(npy_path)\n",
    "print(f\"   Motion shape: {motion_data.shape}\")\n",
    "if motion_data.ndim == 3:\n",
    "    motion_data = motion_data[0] # L·∫•y m·∫´u ƒë·∫ßu ti√™n -> (360, 264)\n",
    "    print(f\"   Squeezed Motion shape: {motion_data.shape}\")\n",
    "\n",
    "# Inverse transform\n",
    "print(\"   Performing inverse transform...\")\n",
    "reconstructed = pipeline.inverse_transform([motion_data])\n",
    "print(f\"   ‚úÖ Reconstructed shape: {reconstructed[0].values.shape}\")\n",
    "\n",
    "# Visualize 2D\n",
    "print(\"\\n   Creating 2D visualization...\")\n",
    "fig1 = plt.figure(figsize=(10, 8))\n",
    "draw_stickfigure(reconstructed[0], frame=frame)\n",
    "plt.title(f\"2D Stick Figure (Meta) - Frame {frame}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"   ‚úÖ 2D plot displayed!\")\n",
    "\n",
    "# Visualize 3D\n",
    "print(\"\\n   Creating 3D visualization...\")\n",
    "fig2 = plt.figure(figsize=(10, 8))\n",
    "draw_stickfigure3d(reconstructed[0], frame=frame)\n",
    "plt.title(f\"3D Stick Figure (Meta) - Frame {frame}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"   ‚úÖ 3D plot displayed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4e6e98",
   "metadata": {},
   "source": [
    "### Inference CFG"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motion_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
