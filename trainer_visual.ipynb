{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af10b203",
   "metadata": {},
   "source": [
    "### Chu·∫©n b·ªã data\n",
    "\n",
    "```text\n",
    "datasets/\n",
    "‚îú‚îÄ‚îÄ BEAT/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 1/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 2/\n",
    "|   ‚îú‚îÄ‚îÄ ...\n",
    "|   ‚îî‚îÄ‚îÄ ...\n",
    "‚îú‚îÄ‚îÄ BEAT_numpy/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ npy/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ txt/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b0d3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/BEAT\"\n",
    "out_dir = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/BEAT_numpy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a287d95",
   "metadata": {},
   "source": [
    "### T√≠nh mean/std cho to√†n b·ªô dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d812d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/step1_fit_scaler.py\" --parent-dir \"{base_dir}\" --start 1 --end 30 --mode hybrid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3acb60",
   "metadata": {},
   "source": [
    "### Chuy·ªÉn dataset bvh sang npy theo t·ª´ng segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb50064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# for i in range(6, 31):\n",
    "#     print(\"ƒêang ch·∫°y preprocess_data.py...\")\n",
    "#     command2 = f'python \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/preprocess_data.py\" --parent-dir \"{base_dir}\" --out-root \"{out_dir}\" --folders \"{i}\"'\n",
    "#     os.system(command2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98171b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/preprocess_data.py\" --parent-dir \"{base_dir}\" --out-root \"{out_dir}\" --folders \"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8ec042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from textgrid import TextGrid\n",
    "\n",
    "# tg = TextGrid.fromFile(\"/home/serverai/ltdoanh/Motion_Diffusion/datasets/BEAT/5/5_stewart_0_82_82.TextGrid\")\n",
    "# print(f\"Number of tiers: {len(tg)}\")\n",
    "# for i, tier in enumerate(tg):\n",
    "#     print(f\"Tier {i}: {tier.name}, intervals: {len(tier.intervals)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7d2957",
   "metadata": {},
   "source": [
    "### Train MotionDiffuse nh∆∞ b√¨nh th∆∞·ªùng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6c81c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python \"/home/serverai/ltdoanh/Motion_Diffusion/tools/train.py\" --dataset_name beat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf77ac5",
   "metadata": {},
   "source": [
    "### Train VQ-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed13924",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python \"/home/serverai/ltdoanh/Motion_Diffusion/tools/train_vq.py\" --dataset_name beat --codebook_size 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dcce1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./tools/scale_factor.py --data_root ./datasets/BEAT_numpy --checkpoints_dir ./checkpoints --vqvae_name VQVAE_BEAT --batch_size 32 --max_samples 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab74aaf",
   "metadata": {},
   "source": [
    "### Train MotionDiffuse tr√™n Latent Space do VQ-VAE ·ªü tr√™n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf31078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python \"/home/serverai/ltdoanh/Motion_Diffusion/tools/train_vq_diffusion.py\" --dataset_name beat --vqvae_name VQVAE_BEAT --sampler ddim --max_epoch 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69db7618",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./tools/train_vq_diffusion.py --dataset_name beat --data_root ./datasets/BEAT_numpy --vqvae_name VQVAE_BEAT --scale_factor 7.53435087 --batch_size 64 --max_epoch 100 --cond_drop_prob 0.1 --guidance_scale 2.0 --hand_loss_weight 10.0 --lr 2e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e1052a",
   "metadata": {},
   "source": [
    "### Evaluation - ƒêang fix l·ªói ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca744de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python \"/home/serverai/ltdoanh/Motion_Diffusion/run_evaluation.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9950bdf",
   "metadata": {},
   "source": [
    "### Visual data b·∫±ng mean/std chu·∫©n t√≠nh t·ª´ b·ªô d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9f0d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Setup path\n",
    "PYOM_DIR = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/pymo\"\n",
    "if PYOM_DIR not in sys.path:\n",
    "    sys.path.insert(0, PYOM_DIR)\n",
    "\n",
    "from pymo.parsers import BVHParser\n",
    "from pymo.preprocessing import *\n",
    "from pymo.viz_tools import draw_stickfigure, draw_stickfigure3d\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "\n",
    "# ===== CELL 2: Load Pipeline =====\n",
    "pipeline_path = \"/home/serverai/ltdoanh/Motion_Diffusion/global_pipeline.pkl\"\n",
    "\n",
    "print(f\"üì¶ Loading pipeline from: {pipeline_path}\")\n",
    "\n",
    "pipeline = joblib.load(pipeline_path)\n",
    "print(f\"‚úÖ Pipeline loaded!\")\n",
    "print(f\"   Mean shape: {pipeline.named_steps['stdscale'].data_mean_.shape}, First 5 values: {pipeline.named_steps['stdscale'].data_mean_[:5]}\")\n",
    "print(f\"   Std shape: {pipeline.named_steps['stdscale'].data_std_.shape},  First 5 values: {pipeline.named_steps['stdscale'].data_std_[:5]}\")\n",
    "\n",
    "# ===== CELL 3: Load and Visualize Motion =====\n",
    "npy_path = \"/home/serverai/ltdoanh/Motion_Diffusion/results/motion_inference-02.npy\"\n",
    "frame = 50\n",
    "\n",
    "print(f\"\\nüé¨ Visualizing: {os.path.basename(npy_path)}\")\n",
    "print(f\"   Frame: {frame}\")\n",
    "\n",
    "# Load motion data\n",
    "motion_data = np.load(npy_path)\n",
    "print(f\"   Motion shape: {motion_data.shape}\")\n",
    "\n",
    "# Inverse transform\n",
    "print(\"   Performing inverse transform...\")\n",
    "reconstructed = pipeline.inverse_transform([motion_data])\n",
    "print(f\"   ‚úÖ Reconstructed shape: {reconstructed[0].values.shape}\")\n",
    "\n",
    "# Visualize 2D\n",
    "print(\"\\n   Creating 2D visualization...\")\n",
    "fig1 = plt.figure(figsize=(10, 8))\n",
    "draw_stickfigure(reconstructed[0], frame=frame)\n",
    "plt.title(f\"2D Stick Figure - Frame {frame}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"   ‚úÖ 2D plot displayed!\")\n",
    "\n",
    "# Visualize 3D\n",
    "print(\"\\n   Creating 3D visualization...\")\n",
    "fig2 = plt.figure(figsize=(10, 8))\n",
    "draw_stickfigure3d(reconstructed[0], frame=frame)\n",
    "plt.title(f\"3D Stick Figure - Frame {frame}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"   ‚úÖ 3D plot displayed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9286cdc0",
   "metadata": {},
   "source": [
    "### Visual data b·∫±ng mean/std t·ª´ model ƒë∆∞·ª£c hu·∫•n luy·ªán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65c2f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "\n",
    "# Setup path\n",
    "PYOM_DIR = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/pymo\"\n",
    "if PYOM_DIR not in sys.path:\n",
    "    sys.path.insert(0, PYOM_DIR)\n",
    "\n",
    "from pymo.parsers import BVHParser\n",
    "from pymo.preprocessing import *\n",
    "from pymo.viz_tools import draw_stickfigure, draw_stickfigure3d\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "\n",
    "# ===== CREATE PIPELINE WITH META =====\n",
    "print(\"\\nüî® Creating pipeline from scratch...\")\n",
    "\n",
    "# 1. Create empty pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('param', MocapParameterizer('position')),\n",
    "    ('rcpn', RootCentricPositionNormalizer()),\n",
    "    ('delta', RootTransformer('abdolute_translation_deltas')),\n",
    "    ('const', ConstantsRemover()),\n",
    "    ('np', Numpyfier()),\n",
    "    ('down', DownSampler(2)),\n",
    "    ('stdscale', ListStandardScaler())\n",
    "])\n",
    "\n",
    "# 2. Fit pipeline on sample BVH (to learn structure)\n",
    "bvh_sample_path = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/BEAT/1/1_wayne_0_1_1.bvh\"\n",
    "print(f\"   Fitting on BVH sample: {os.path.basename(bvh_sample_path)}\")\n",
    "\n",
    "parser = BVHParser()\n",
    "parsed_data = parser.parse(bvh_sample_path)\n",
    "pipeline.fit([parsed_data])\n",
    "print(\"   ‚úÖ Pipeline fitted (structure learned)\")\n",
    "\n",
    "# 3. Load mean/std from meta directory\n",
    "# meta_dir = \"/home/serverai/ltdoanh/Motion_Diffusion/checkpoints/beat/test/meta\"\n",
    "# mean_path = os.path.join(meta_dir, \"mean.npy\")\n",
    "# std_path = os.path.join(meta_dir, \"std.npy\")\n",
    "ckpt_path = \"/home/serverai/ltdoanh/Motion_Diffusion/checkpoints/beat/vq_diffusion/model/latest.pt\"  # Thay ƒë∆∞·ªùng d·∫´n file c·ªßa b·∫°n v√†o ƒë√¢y\n",
    "\n",
    "try:\n",
    "    # 2. Load checkpoint\n",
    "    # map_location='cpu' gi√∫p tr√°nh l·ªói n·∫øu m√°y b·∫°n kh√¥ng c√≥ GPU gi·ªëng l√∫c train\n",
    "    checkpoint = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
    "\n",
    "    # 3. Ki·ªÉm tra v√† l·∫•y mean, std\n",
    "    if 'mean' in checkpoint and 'std' in checkpoint:\n",
    "        mean = checkpoint['mean']\n",
    "        std = checkpoint['std']\n",
    "\n",
    "        print(\"--- ƒê√£ t√¨m th·∫•y Mean v√† Std ---\")\n",
    "        print(f\"Shape c·ªßa Mean: {mean.shape}\")\n",
    "        print(f\"Shape c·ªßa Std: {std.shape}\")\n",
    "        \n",
    "        # In th·ª≠ v√†i gi√° tr·ªã ƒë·∫ßu\n",
    "        print(f\"Mean (5 gi√° tr·ªã ƒë·∫ßu): {mean[:5]}\")\n",
    "        print(f\"Std (5 gi√° tr·ªã ƒë·∫ßu): {std[:5]}\")\n",
    "        \n",
    "        # 4. (T√πy ch·ªçn) L∆∞u l·∫°i ra file .npy ƒë·ªÉ d√πng vi·ªác kh√°c n·∫øu c·∫ßn\n",
    "        # np.save('mean.npy', mean)\n",
    "        # np.save('std.npy', std)\n",
    "        # print(\"ƒê√£ l∆∞u ra file .npy\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Kh√¥ng t√¨m th·∫•y key 'mean' ho·∫∑c 'std' trong file .pt n√†y.\")\n",
    "        print(\"C√°c keys hi·ªán c√≥:\", checkpoint.keys())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"C√≥ l·ªói x·∫£y ra: {e}\")\n",
    "\n",
    "# 4. Override pipeline's mean/std with meta values\n",
    "print(\"\\nüîß Overriding pipeline statistics with meta values...\")\n",
    "pipeline.named_steps['stdscale'].data_mean_ = mean\n",
    "pipeline.named_steps['stdscale'].data_std_ = std\n",
    "print(\"   ‚úÖ Pipeline updated with meta statistics!\")\n",
    "\n",
    "# ===== VISUALIZE MOTION =====\n",
    "npy_path = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/BEAT_numpy/npy/2/2_scott_0_2_2_sentence_000.npy\"\n",
    "frame = 50\n",
    "\n",
    "print(f\"\\nüé¨ Visualizing: {os.path.basename(npy_path)}\")\n",
    "print(f\"   Frame: {frame}\")\n",
    "\n",
    "# Load motion data\n",
    "motion_data = np.load(npy_path)\n",
    "print(f\"   Motion shape: {motion_data.shape}\")\n",
    "\n",
    "# Inverse transform\n",
    "print(\"   Performing inverse transform...\")\n",
    "reconstructed = pipeline.inverse_transform([motion_data])\n",
    "print(f\"   ‚úÖ Reconstructed shape: {reconstructed[0].values.shape}\")\n",
    "\n",
    "# Visualize 2D\n",
    "print(\"\\n   Creating 2D visualization...\")\n",
    "fig1 = plt.figure(figsize=(10, 8))\n",
    "draw_stickfigure(reconstructed[0], frame=frame)\n",
    "plt.title(f\"2D Stick Figure (Meta) - Frame {frame}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"   ‚úÖ 2D plot displayed!\")\n",
    "\n",
    "# Visualize 3D\n",
    "print(\"\\n   Creating 3D visualization...\")\n",
    "fig2 = plt.figure(figsize=(10, 8))\n",
    "draw_stickfigure3d(reconstructed[0], frame=frame)\n",
    "plt.title(f\"3D Stick Figure (Meta) - Frame {frame}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"   ‚úÖ 3D plot displayed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83329b36",
   "metadata": {},
   "source": [
    "### Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3d1766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from models import MotionTransformer\n",
    "from trainers import DDPMTrainer\n",
    "from models.vq.model import RVQVAE \n",
    "\n",
    "# ==========================================\n",
    "# 1. C·∫•u h√¨nh Inference\n",
    "# ==========================================\n",
    "class InferenceConfig:\n",
    "    def __init__(self):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        self.is_train = False\n",
    "        self.schedule_sampler = 'uniform'   \n",
    "\n",
    "        # --- C·∫•u h√¨nh Diffusion ---\n",
    "        self.input_feats = 512     # Latent Dimension\n",
    "        self.num_frames = 45      # Latent Length (360 / 8)\n",
    "        self.num_layers = 8\n",
    "        self.latent_dim = 512\n",
    "        self.ff_size = 1024\n",
    "        self.num_heads = 8\n",
    "        self.dropout = 0.1\n",
    "        self.activation = \"gelu\"\n",
    "        self.dataset_name = 'beat' \n",
    "        self.do_denoise = True\n",
    "        self.noise_schedule = 'cosine'\n",
    "        self.diffusion_steps = 1000\n",
    "        self.no_clip = False\n",
    "        self.no_eff = False\n",
    "        self.result_dir = \"/home/serverai/ltdoanh/Motion_Diffusion/results\"\n",
    "\n",
    "# Class gi·∫£ l·∫≠p args cho RVQVAE\n",
    "class VQArgs:\n",
    "    def __init__(self):\n",
    "        # C√°c gi√° tr·ªã m·∫∑c ƒë·ªãnh, s·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t logic b√™n d∆∞·ªõi\n",
    "        self.num_quantizers = 1 \n",
    "        self.shared_codebook = False\n",
    "        self.quantize_dropout_prob = 0.0\n",
    "        self.mu = 0.99 # Cho QuantizerEMA\n",
    "\n",
    "opt = InferenceConfig()\n",
    "vq_args = VQArgs()\n",
    "\n",
    "# ==========================================\n",
    "# 2. Load Checkpoint & T√°ch Weights\n",
    "# ==========================================\n",
    "ckpt_path = \"/home/serverai/ltdoanh/Motion_Diffusion/checkpoints/beat/vq_diffusion/model/best_model.pt\"\n",
    "print(f\"üìÇ Loading checkpoint: {ckpt_path}\")\n",
    "\n",
    "# Load to√†n b·ªô checkpoint\n",
    "checkpoint = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
    "state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "# --- T·ª± ƒë·ªông ph√°t hi·ªán s·ªë l∆∞·ª£ng Quantizers t·ª´ Checkpoint ---\n",
    "# ƒêi·ªÅu n√†y gi√∫p tr√°nh l·ªói sai l·ªách key khi kh·ªüi t·∫°o VQ-VAE\n",
    "max_layer_idx = 0\n",
    "for k in state_dict.keys():\n",
    "    if \"vqvae.quantizer.layers.\" in k:\n",
    "        # Parse t√¨m s·ªë l·ªõn nh·∫•t trong 'layers.X.'\n",
    "        try:\n",
    "            parts = k.split('.')\n",
    "            layer_idx = int(parts[parts.index('layers') + 1])\n",
    "            if layer_idx > max_layer_idx:\n",
    "                max_layer_idx = layer_idx\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "vq_args.num_quantizers = max_layer_idx + 1\n",
    "print(f\"üîç Detected num_quantizers: {vq_args.num_quantizers}\")\n",
    "\n",
    "# --- T√°ch Dictionary ---\n",
    "trans_dict = {}\n",
    "vqvae_dict = {}\n",
    "\n",
    "for k, v in state_dict.items():\n",
    "    if k.startswith('transformer.'):\n",
    "        trans_dict[k[12:]] = v  \n",
    "    elif k.startswith('vqvae.'):\n",
    "        vqvae_dict[k[6:]] = v   \n",
    "\n",
    "# ==========================================\n",
    "# 3. Kh·ªüi t·∫°o Models\n",
    "# ==========================================\n",
    "\n",
    "# A. Motion Transformer\n",
    "print(\"üîß Initializing MotionTransformer...\")\n",
    "encoder = MotionTransformer(\n",
    "    input_feats=opt.input_feats,\n",
    "    num_frames=opt.num_frames,\n",
    "    num_layers=opt.num_layers,\n",
    "    latent_dim=opt.latent_dim,\n",
    "    num_heads=opt.num_heads,\n",
    "    ff_size=opt.ff_size,\n",
    "    no_clip=opt.no_clip,\n",
    "    no_eff=opt.no_eff\n",
    ")\n",
    "encoder.load_state_dict(trans_dict, strict=True)\n",
    "encoder.to(opt.device).eval()\n",
    "\n",
    "# B. RVQVAE\n",
    "print(\"üîß Initializing RVQVAE...\")\n",
    "# L∆∞u √Ω: C√°c tham s·ªë d∆∞·ªõi ƒë√¢y ph·∫£i kh·ªõp v·ªõi file config l√∫c train VQVAE c·ªßa b·∫°n.\n",
    "# T√¥i ƒëang ƒë·ªÉ c√°c gi√° tr·ªã ph·ªï bi·∫øn d·ª±a tr√™n file model.py\n",
    "vqvae_model = RVQVAE(\n",
    "    args=vq_args,\n",
    "    input_width=264,       # BEAT dataset th∆∞·ªùng l√† 264\n",
    "    nb_code=512,           # Ki·ªÉm tra l·∫°i config train c≈© n·∫øu l·ªói\n",
    "    code_dim=512, \n",
    "    output_emb_width=512, \n",
    "    down_t=3, \n",
    "    stride_t=2, \n",
    "    width=512, \n",
    "    depth=3, \n",
    "    dilation_growth_rate=3,\n",
    "    activation='relu',\n",
    "    norm=None\n",
    ")\n",
    "vqvae_model.load_state_dict(vqvae_dict, strict=True)\n",
    "vqvae_model.to(opt.device).eval()\n",
    "\n",
    "# ==========================================\n",
    "# 4. Inference\n",
    "# ==========================================\n",
    "trainer = DDPMTrainer(opt, encoder)\n",
    "\n",
    "# Inject mean/std (Quan tr·ªçng cho qu√° tr√¨nh decode cu·ªëi c√πng)\n",
    "trainer.mean = checkpoint['mean']\n",
    "trainer.std = checkpoint['std']\n",
    "\n",
    "print(\"üöÄ Starting Inference...\")\n",
    "os.makedirs(opt.result_dir, exist_ok=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    caption = [\"the first thing i like to do on weekends is relaxing and i'll go shopping if i'm not that tired\"]\n",
    "    \n",
    "    # ƒê·ªô d√†i Latent (45)\n",
    "    m_lens = torch.LongTensor([45]).to(opt.device) \n",
    "    \n",
    "    # 1. Sinh Latent (Diffusion) -> Output: (Batch, Length, Dim) = (1, 45, 512)\n",
    "    pred_latent_list = trainer.generate(caption, m_lens, dim_pose=512)\n",
    "    pred_latent = pred_latent_list[0]\n",
    "\n",
    "    if pred_latent.dim() == 2:\n",
    "        pred_latent = pred_latent.unsqueeze(0)  # Th√™m batch dim n·∫øu c·∫ßn\n",
    "\n",
    "    print(f\"   Latent generated shape: {pred_latent.shape}\")\n",
    "\n",
    "    # 2. Decode b·∫±ng RVQVAE\n",
    "    # RVQVAE Decoder c·∫ßn input: (Batch, Channel, Length) -> C·∫ßn permute\n",
    "    latent_input = pred_latent.permute(0, 2, 1) # -> (1, 512, 45)\n",
    "    \n",
    "    print(\"   Decoding with RVQVAE...\")\n",
    "    # G·ªçi tr·ª±c ti·∫øp decoder (b·ªè qua quantizer v√¨ Diffusion ƒë√£ sinh ra latent r·ªìi)\n",
    "    decoded_motion = vqvae_model.decoder(latent_input)\n",
    "    \n",
    "    # 3. Post-process (Permute l·∫°i v·ªÅ: Batch, Length, Channel)\n",
    "    # H√†m postprocess trong model.py: (B, C, T) -> (B, T, C)\n",
    "    motion = vqvae_model.postprocess(decoded_motion).cpu().numpy()\n",
    "\n",
    "    if motion.shape[1] == 264 and motion.shape[2] == 360:\n",
    "        motion = motion.transpose(0, 2, 1)\n",
    "\n",
    "    # motion = motion.cpu().numpy()\n",
    "    \n",
    "    # 4. Denormalize (Gi·∫£i chu·∫©n h√≥a)\n",
    "    # Output c·ªßa VQVAE th∆∞·ªùng v·∫´n l√† normalized data\n",
    "    mean = checkpoint['mean']\n",
    "    std = checkpoint['std']\n",
    "    \n",
    "    # ƒê·∫£m b·∫£o shape kh·ªõp ƒë·ªÉ broadcast\n",
    "    # Motion: (1, 360, 264), Mean: (264,), Std: (264,)\n",
    "    motion = motion * std + mean\n",
    "    \n",
    "    print(f\"üéâ Final Motion Shape: {motion.shape}\")\n",
    "\n",
    "# L∆∞u k·∫øt qu·∫£\n",
    "save_path = os.path.join(opt.result_dir, 'inference-latest.npy')\n",
    "np.save(save_path, motion)\n",
    "print(f\"üíæ Saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95738b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "\n",
    "# Setup path\n",
    "PYOM_DIR = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/pymo\"\n",
    "if PYOM_DIR not in sys.path:\n",
    "    sys.path.insert(0, PYOM_DIR)\n",
    "\n",
    "from pymo.parsers import BVHParser\n",
    "from pymo.preprocessing import *\n",
    "from pymo.viz_tools import draw_stickfigure, draw_stickfigure3d\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "\n",
    "# ===== CREATE PIPELINE WITH META =====\n",
    "print(\"\\nüî® Creating pipeline from scratch...\")\n",
    "\n",
    "# 1. Create empty pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('param', MocapParameterizer('position')),\n",
    "    ('rcpn', RootCentricPositionNormalizer()),\n",
    "    ('delta', RootTransformer('abdolute_translation_deltas')),\n",
    "    ('const', ConstantsRemover()),\n",
    "    ('np', Numpyfier()),\n",
    "    ('down', DownSampler(2)),\n",
    "    ('stdscale', ListStandardScaler())\n",
    "])\n",
    "\n",
    "# 2. Fit pipeline on sample BVH (to learn structure)\n",
    "bvh_sample_path = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/BEAT/1/1_wayne_0_1_1.bvh\"\n",
    "print(f\"   Fitting on BVH sample: {os.path.basename(bvh_sample_path)}\")\n",
    "\n",
    "parser = BVHParser()\n",
    "parsed_data = parser.parse(bvh_sample_path)\n",
    "pipeline.fit([parsed_data])\n",
    "print(\"   ‚úÖ Pipeline fitted (structure learned)\")\n",
    "\n",
    "# 3. Load mean/std from meta directory\n",
    "ckpt_path = \"/home/serverai/ltdoanh/Motion_Diffusion/checkpoints/beat/vq_diffusion/model/best_model.pt\"  \n",
    "\n",
    "try:\n",
    "    # 2. Load checkpoint\n",
    "    # map_location='cpu' gi√∫p tr√°nh l·ªói n·∫øu m√°y b·∫°n kh√¥ng c√≥ GPU gi·ªëng l√∫c train\n",
    "    checkpoint = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
    "\n",
    "    # 3. Ki·ªÉm tra v√† l·∫•y mean, std\n",
    "    if 'mean' in checkpoint and 'std' in checkpoint:\n",
    "        mean = checkpoint['mean']\n",
    "        std = checkpoint['std']\n",
    "\n",
    "        print(\"--- ƒê√£ t√¨m th·∫•y Mean v√† Std ---\")\n",
    "        print(f\"Shape c·ªßa Mean: {mean.shape}\")\n",
    "        print(f\"Shape c·ªßa Std: {std.shape}\")\n",
    "        \n",
    "        # In th·ª≠ v√†i gi√° tr·ªã ƒë·∫ßu\n",
    "        print(f\"Mean (5 gi√° tr·ªã ƒë·∫ßu): {mean[:5]}\")\n",
    "        print(f\"Std (5 gi√° tr·ªã ƒë·∫ßu): {std[:5]}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Kh√¥ng t√¨m th·∫•y key 'mean' ho·∫∑c 'std' trong file .pt n√†y.\")\n",
    "        print(\"C√°c keys hi·ªán c√≥:\", checkpoint.keys())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"C√≥ l·ªói x·∫£y ra: {e}\")\n",
    "\n",
    "# 4. Override pipeline's mean/std with meta values\n",
    "print(\"\\nüîß Overriding pipeline statistics with meta values...\")\n",
    "pipeline.named_steps['stdscale'].data_mean_ = mean\n",
    "pipeline.named_steps['stdscale'].data_std_ = std\n",
    "print(\"   ‚úÖ Pipeline updated with meta statistics!\")\n",
    "\n",
    "# ===== VISUALIZE MOTION =====\n",
    "npy_path = \"/home/serverai/ltdoanh/Motion_Diffusion/results/inference-latest.npy\"\n",
    "frame = 50\n",
    "\n",
    "print(f\"\\nüé¨ Visualizing: {os.path.basename(npy_path)}\")\n",
    "print(f\"   Frame: {frame}\")\n",
    "\n",
    "# Load motion data\n",
    "motion_data = np.load(npy_path)\n",
    "print(f\"   Motion shape: {motion_data.shape}\")\n",
    "if motion_data.ndim == 3:\n",
    "    motion_data = motion_data[0] # L·∫•y m·∫´u ƒë·∫ßu ti√™n -> (360, 264)\n",
    "    print(f\"   Squeezed Motion shape: {motion_data.shape}\")\n",
    "\n",
    "# Inverse transform\n",
    "print(\"   Performing inverse transform...\")\n",
    "reconstructed = pipeline.inverse_transform([motion_data])\n",
    "print(f\"   ‚úÖ Reconstructed shape: {reconstructed[0].values.shape}\")\n",
    "\n",
    "# Visualize 2D\n",
    "print(\"\\n   Creating 2D visualization...\")\n",
    "fig1 = plt.figure(figsize=(10, 8))\n",
    "draw_stickfigure(reconstructed[0], frame=frame)\n",
    "plt.title(f\"2D Stick Figure (Meta) - Frame {frame}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"   ‚úÖ 2D plot displayed!\")\n",
    "\n",
    "# Visualize 3D\n",
    "print(\"\\n   Creating 3D visualization...\")\n",
    "fig2 = plt.figure(figsize=(10, 8))\n",
    "draw_stickfigure3d(reconstructed[0], frame=frame)\n",
    "plt.title(f\"3D Stick Figure (Meta) - Frame {frame}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"   ‚úÖ 3D plot displayed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5eef55",
   "metadata": {},
   "source": [
    "### Inference DDIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01fee62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. SETUP & IMPORTS\n",
    "# ==========================================\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add project root to path\n",
    "ROOT = \"/home/serverai/ltdoanh/Motion_Diffusion\"\n",
    "if ROOT not in sys.path:\n",
    "    sys.path.insert(0, ROOT)\n",
    "\n",
    "PYMO_DIR = os.path.join(ROOT, 'datasets', 'pymo')\n",
    "if PYMO_DIR not in sys.path:\n",
    "    sys.path.insert(0, PYMO_DIR)\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# ==========================================\n",
    "# 2. CONFIGURATION\n",
    "# ==========================================\n",
    "class Config:\n",
    "    # Device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Paths\n",
    "    checkpoints_dir = \"/home/serverai/ltdoanh/Motion_Diffusion/checkpoints\"\n",
    "    dataset_name = 'beat'\n",
    "    vqvae_name = 'VQVAE_BEAT'\n",
    "    diffusion_name = 'vq_diffusion'\n",
    "    checkpoint_name = 'best_model.pt'\n",
    "    result_dir = \"/home/serverai/ltdoanh/Motion_Diffusion/results\"\n",
    "    scale_factor = 7.53435087  # Scale factor used during training\n",
    "    \n",
    "    # Model config\n",
    "    latent_dim = 512\n",
    "    num_layers = 8\n",
    "    num_heads = 8\n",
    "    ff_size = 1024\n",
    "    dropout = 0.1\n",
    "    no_eff = False\n",
    "    freeze_vqvae = True\n",
    "    \n",
    "    # Diffusion\n",
    "    diffusion_steps = 1000\n",
    "    noise_schedule = 'cosine'\n",
    "    \n",
    "    # Sampling\n",
    "    sampler = 'ddim'  # 'ddpm' or 'ddim'\n",
    "    ddim_eta = 0.0\n",
    "    num_samples = 1\n",
    "\n",
    "args = Config()\n",
    "\n",
    "# ==========================================\n",
    "# 3. LOAD MODEL\n",
    "# ==========================================\n",
    "from models.vq_diffusion import create_vq_latent_diffusion, VQLatentDiffusionWrapper\n",
    "from models.gaussian_diffusion import (\n",
    "    GaussianDiffusion, get_named_beta_schedule,\n",
    "    ModelMeanType, ModelVarType, LossType\n",
    ")\n",
    "\n",
    "print(\"\\nüì¶ Loading model...\")\n",
    "\n",
    "# Create model\n",
    "model = create_vq_latent_diffusion(\n",
    "    dataset_name=args.dataset_name,\n",
    "    vqvae_name=args.vqvae_name,\n",
    "    checkpoints_dir=args.checkpoints_dir,\n",
    "    device=args.device,\n",
    "    freeze_vqvae=args.freeze_vqvae,\n",
    "    scale_factor=args.scale_factor,\n",
    "    latent_dim=args.latent_dim,\n",
    "    num_layers=args.num_layers,\n",
    "    num_heads=args.num_heads,\n",
    "    ff_size=args.ff_size,\n",
    "    dropout=args.dropout,\n",
    "    no_eff=args.no_eff\n",
    ")\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint_path = os.path.join(\n",
    "    args.checkpoints_dir,\n",
    "    args.dataset_name,\n",
    "    args.diffusion_name,\n",
    "    'model',\n",
    "    args.checkpoint_name\n",
    ")\n",
    "\n",
    "print(f\"Loading checkpoint: {checkpoint_path}\")\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)\n",
    "\n",
    "try:\n",
    "    model.load_state_dict(checkpoint['model_state_dict'], strict=True)\n",
    "    print(\"‚úì Checkpoint loaded (strict)\")\n",
    "except:\n",
    "    model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "    print(\"‚úì Checkpoint loaded (loose)\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Create diffusion\n",
    "betas = get_named_beta_schedule(args.noise_schedule, args.diffusion_steps)\n",
    "diffusion = GaussianDiffusion(\n",
    "    betas=betas,\n",
    "    model_mean_type=ModelMeanType.EPSILON,\n",
    "    model_var_type=ModelVarType.FIXED_SMALL,\n",
    "    loss_type=LossType.MSE,\n",
    "    rescale_timesteps=False\n",
    ")\n",
    "\n",
    "print(\"‚úì Model and diffusion ready\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. LOAD STATS\n",
    "# ==========================================\n",
    "stats_path = os.path.join(ROOT, 'global_pipeline.pkl')\n",
    "pipeline = joblib.load(stats_path)\n",
    "scaler = pipeline.named_steps['stdscale']\n",
    "mean = scaler.data_mean_\n",
    "std = scaler.data_std_\n",
    "\n",
    "print(f\"‚úì Stats loaded (dim={len(mean)})\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. GENERATE MOTION\n",
    "# ==========================================\n",
    "@torch.no_grad()\n",
    "def generate_motion(text_prompts, lengths):\n",
    "    \"\"\"Generate motion from text\"\"\"\n",
    "    \n",
    "    print(f\"\\nüé¨ Generating motion...\")\n",
    "    print(f\"Prompts: {text_prompts}\")\n",
    "    \n",
    "    wrapped_model = VQLatentDiffusionWrapper(model)\n",
    "    \n",
    "    B = len(text_prompts)\n",
    "    T_latent = model.num_frames\n",
    "    code_dim = model.vqvae.code_dim\n",
    "    \n",
    "    shape = (B, T_latent, code_dim)\n",
    "    \n",
    "    model_kwargs = {\n",
    "        'y': {\n",
    "            'text': text_prompts,\n",
    "            'length': lengths,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Sample\n",
    "    if args.sampler == 'ddim':\n",
    "        print(f\"Sampling with DDIM (eta={args.ddim_eta})...\")\n",
    "        latent_samples = diffusion.ddim_sample_loop(\n",
    "            wrapped_model, shape,\n",
    "            clip_denoised=False,\n",
    "            model_kwargs=model_kwargs,\n",
    "            device=args.device,\n",
    "            progress=True,\n",
    "            eta=args.ddim_eta\n",
    "        )\n",
    "    else:\n",
    "        print(\"Sampling with DDPM...\")\n",
    "        latent_samples = diffusion.p_sample_loop(\n",
    "            wrapped_model, shape,\n",
    "            clip_denoised=False,\n",
    "            model_kwargs=model_kwargs,\n",
    "            device=args.device,\n",
    "            progress=True\n",
    "        )\n",
    "    \n",
    "    print(f\"Latent shape: {latent_samples.shape}\")\n",
    "    \n",
    "    # Decode\n",
    "    print(\"Decoding to motion...\")\n",
    "    motion = model.decode_from_latent(latent=latent_samples)\n",
    "    \n",
    "    print(f\"Motion shape: {motion.shape}\")\n",
    "    \n",
    "    # Denormalize\n",
    "    motion_np = motion.cpu().numpy()\n",
    "    if motion_np.shape[1] == 264:\n",
    "        motion_np = motion_np.transpose(0, 2, 1)\n",
    "    \n",
    "    motion_denorm = motion_np * std + mean\n",
    "    \n",
    "    print(f\"‚úì Generation complete: {motion_denorm.shape}\")\n",
    "    \n",
    "    return motion_denorm\n",
    "\n",
    "# ==========================================\n",
    "# 6. INFERENCE\n",
    "# ==========================================\n",
    "# Define your prompts here\n",
    "text_prompts = [\n",
    "    \"for example people being shifted from the center of the frame to the left side of the frame can make a different video when singing contest with the background\"\n",
    "    # \"how are you\"\n",
    "]\n",
    "\n",
    "# Latent lengths (45 for BEAT = 360 original frames)\n",
    "lengths = [45] * len(text_prompts)\n",
    "\n",
    "# Generate!\n",
    "motion = generate_motion(text_prompts, lengths)\n",
    "\n",
    "# ==========================================\n",
    "# 7. SAVE RESULTS\n",
    "# ==========================================\n",
    "os.makedirs(args.result_dir, exist_ok=True)\n",
    "\n",
    "save_path = os.path.join(args.result_dir, 'motion_0005.npy')\n",
    "np.save(save_path, motion)\n",
    "print(f\"üíæ Saved to: {save_path}\")\n",
    "\n",
    "# Save prompts\n",
    "with open(os.path.join(args.result_dir, 'prompts.txt'), 'w') as f:\n",
    "    for i, prompt in enumerate(text_prompts):\n",
    "        f.write(f\"{i}: {prompt}\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INFERENCE COMPLETED!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Motion saved to: {args.result_dir}\")\n",
    "print(\"\\nTo create more visualizations, run:\")\n",
    "print(f\"python tools/visualize_motion.py --motion_path {save_path}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed791742",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# Setup path\n",
    "PYOM_DIR = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/pymo\"\n",
    "if PYOM_DIR not in sys.path:\n",
    "    sys.path.insert(0, PYOM_DIR)\n",
    "\n",
    "from pymo.parsers import BVHParser\n",
    "from pymo.viz_tools import draw_stickfigure, draw_stickfigure3d\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "\n",
    "# ==========================================\n",
    "# 1. LOAD & FIX PIPELINE (ADVANCED FIX)\n",
    "# ==========================================\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline_path = \"/home/serverai/ltdoanh/Motion_Diffusion/global_pipeline.pkl\"\n",
    "print(f\"üì¶ Loading pipeline from: {pipeline_path}\")\n",
    "pipeline = joblib.load(pipeline_path)\n",
    "\n",
    "# Load Reference BVH\n",
    "ref_bvh_path = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/BEAT/1/1_wayne_0_1_1.bvh\"\n",
    "\n",
    "if os.path.exists(ref_bvh_path):\n",
    "    print(f\"üîß Processing reference BVH: {os.path.basename(ref_bvh_path)}\")\n",
    "    parser = BVHParser()\n",
    "    ref_data = parser.parse(ref_bvh_path)\n",
    "    \n",
    "    # [QUAN TR·ªåNG] T√¨m v·ªã tr√≠ c·ªßa b∆∞·ªõc 'np' (Numpyfier)\n",
    "    # Ch√∫ng ta c·∫ßn ch·∫°y d·ªØ li·ªáu qua t·∫•t c·∫£ c√°c b∆∞·ªõc TR∆Ø·ªöC b∆∞·ªõc 'np'\n",
    "    # ƒë·ªÉ bi·∫øn ƒë·ªïi d·ªØ li·ªáu th√¥ (228) th√†nh d·ªØ li·ªáu model (264)\n",
    "    \n",
    "    steps_before_np = []\n",
    "    np_found = False\n",
    "    \n",
    "    for name, step in pipeline.steps:\n",
    "        if name == 'np': # T√™n c·ªßa Numpyfier trong pipeline\n",
    "            np_found = True\n",
    "            break\n",
    "        steps_before_np.append((name, step))\n",
    "    \n",
    "    if np_found and steps_before_np:\n",
    "        print(f\"   Running reference data through pre-processing steps: {[n for n, _ in steps_before_np]}...\")\n",
    "        \n",
    "        # T·∫°o pipeline con ch·ªâ ch·ª©a c√°c b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω\n",
    "        pre_pipeline = Pipeline(steps_before_np)\n",
    "        \n",
    "        # Bi·∫øn ƒë·ªïi reference data: (Time, 228) -> (Time, 264)\n",
    "        # fit_transform c·∫ßn list c√°c MocapData\n",
    "        processed_ref = pre_pipeline.fit_transform([ref_data])\n",
    "        \n",
    "        # L·∫•y k·∫øt qu·∫£ (ƒë√£ l√† DataFrame ƒë√∫ng chu·∫©n 264 c·ªôt)\n",
    "        template_data = processed_ref[0]\n",
    "        \n",
    "        print(f\"   Template shape after preprocessing: {template_data.values.shape}\") \n",
    "        # K·ª≥ v·ªçng output: (Time, 264)\n",
    "        \n",
    "        if template_data.values.shape[1] == 264:\n",
    "            # G√°n template ƒë√∫ng shape v√†o Numpyfier\n",
    "            pipeline.named_steps['np'].org_mocap_ = template_data\n",
    "            print(\"‚úÖ Skeleton structure injected with CORRECT dimensions (264)!\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Warning: Processed shape is {template_data.values.shape[1]}, but model output is 264.\")\n",
    "            print(\"   Visualization might still fail.\")\n",
    "            \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Could not isolate steps before Numpyfier.\")\n",
    "        # Fallback c≈© (s·∫Ω l·ªói)\n",
    "        pipeline.named_steps['np'].org_mocap_ = ref_data\n",
    "\n",
    "else:\n",
    "    print(f\"‚ùå Reference BVH not found at {ref_bvh_path}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ==========================================\n",
    "# 2. LOAD & PREPARE DATA\n",
    "# ==========================================\n",
    "npy_path = \"/home/serverai/ltdoanh/Motion_Diffusion/results/generated_motion_best_model.npy\"\n",
    "print(f\"\\nüìÇ Loading motion: {npy_path}\")\n",
    "motion_data = np.load(npy_path)\n",
    "\n",
    "# X·ª≠ l√Ω dimension: (1, 360, 264) -> (360, 264)\n",
    "if motion_data.ndim == 3:\n",
    "    motion_data = motion_data[0]\n",
    "\n",
    "print(f\"   Raw Input Shape: {motion_data.shape}\")\n",
    "print(f\"   Raw Input Range: {motion_data.min():.2f} to {motion_data.max():.2f}\")\n",
    "\n",
    "# [FIX 2] X·ª≠ l√Ω v·∫•n ƒë·ªÅ \"Gi√° tr·ªã kh·ªïng l·ªì\" (9 tri·ªáu)\n",
    "# N·∫øu gi√° tr·ªã > 1000, ch·∫Øc ch·∫Øn file npy n√†y ƒê√É L√Ä real-world scale (ƒë√£ denormalize).\n",
    "# Nh∆∞ng pipeline.inverse_transform() l·∫°i mong ƒë·ª£i input l√† Normalized (Mean~0, Std~1).\n",
    "# Ta ph·∫£i \"Re-normalize\" th·ªß c√¥ng b·∫±ng Mean/Std c·ªßa pipeline.\n",
    "\n",
    "scaler = pipeline.named_steps['stdscale']\n",
    "mean = scaler.data_mean_\n",
    "std = scaler.data_std_\n",
    "\n",
    "# Ki·ªÉm tra xem c√≥ c·∫ßn re-normalize kh√¥ng\n",
    "if np.abs(motion_data).max() > 100:\n",
    "    print(\"‚ö†Ô∏è Detected large values (Real-scale data). Re-normalizing for pipeline compatibility...\")\n",
    "    # C√¥ng th·ª©c: (X - Mean) / Std\n",
    "    motion_data = (motion_data - mean) / std\n",
    "    print(f\"   Normalized Range: {motion_data.min():.2f} to {motion_data.max():.2f}\")\n",
    "else:\n",
    "    print(\"‚úÖ Data seems to be already normalized.\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. INVERSE TRANSFORM & VISUALIZE\n",
    "# ==========================================\n",
    "print(\"\\nüîÑ Performing inverse transform...\")\n",
    "try:\n",
    "    # Inverse transform tr·∫£ v·ªÅ list c√°c MocapData\n",
    "    reconstructed = pipeline.inverse_transform([motion_data])\n",
    "    mocap_data = reconstructed[0]\n",
    "    \n",
    "    print(\"   ‚úÖ Inverse transform successful!\")\n",
    "    \n",
    "    # Ki·ªÉm tra gi√° tr·ªã sau khi t√°i t·∫°o (ph·∫£i t√≠nh b·∫±ng cm/m)\n",
    "    df = mocap_data.values\n",
    "    print(f\"   Final Motion Range (cm): {df.min().min():.2f} to {df.max().max():.2f}\")\n",
    "    \n",
    "    # Visualize\n",
    "    frame = 0\n",
    "    print(f\"\\nüé¨ Visualizing Frame {frame}...\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    draw_stickfigure(mocap_data, frame=frame)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title(f\"Reconstructed Motion - Frame {frame}\")\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå ERROR during visualization: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b6dba30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ B·∫Øt ƒë·∫ßu t·∫°o indices...\n",
      "‚öôÔ∏è ƒêang tr√≠ch xu·∫•t t√™n features...\n",
      "‚úÖ T√¨m th·∫•y 264 features (Kh·ªõp 264).\n",
      "   Sample features: ['Spine_Xposition', 'Spine_Yposition', 'Spine_Zposition', 'Spine1_Xposition', 'Spine1_Yposition']\n",
      "\n",
      "üîç ƒêang ph√¢n lo·∫°i...\n",
      "\n",
      "üíæ ƒê√£ l∆∞u th√†nh c√¥ng:\n",
      "   - /home/serverai/ltdoanh/Motion_Diffusion/datasets/BEAT_indices/hand_indices.npy (174 chi·ªÅu)\n",
      "   - /home/serverai/ltdoanh/Motion_Diffusion/datasets/BEAT_indices/body_indices.npy (90 chi·ªÅu)\n",
      "   - T·ªïng c·ªông: 264 indices.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# # 1. C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n\n",
    "# ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "# if ROOT not in sys.path:\n",
    "#     sys.path.insert(0, ROOT)\n",
    "\n",
    "PYMO_DIR = os.path.join(ROOT, 'datasets', 'pymo')\n",
    "if PYMO_DIR not in sys.path:\n",
    "    sys.path.insert(0, PYMO_DIR)\n",
    "\n",
    "from pymo.parsers import BVHParser\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n (S·ª≠a l·∫°i n·∫øu c·∫ßn)\n",
    "pipeline_path = os.path.join(ROOT, \"global_pipeline.pkl\")\n",
    "ref_bvh_path = os.path.join(ROOT, \"datasets/BEAT/1/1_wayne_0_1_1.bvh\")\n",
    "save_dir = os.path.join(ROOT, \"datasets/BEAT_indices\")\n",
    "\n",
    "print(f\"üöÄ B·∫Øt ƒë·∫ßu t·∫°o indices...\")\n",
    "\n",
    "# 2. Load Pipeline & L·∫•y t√™n c·ªôt\n",
    "if not os.path.exists(ref_bvh_path):\n",
    "    print(f\"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file BVH m·∫´u t·∫°i {ref_bvh_path}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "pipeline = joblib.load(pipeline_path)\n",
    "parser = BVHParser()\n",
    "ref_data = parser.parse(ref_bvh_path)\n",
    "\n",
    "# T√¨m b∆∞·ªõc 'np' (Numpyfier)\n",
    "np_step_idx = -1\n",
    "for i, (name, step) in enumerate(pipeline.steps):\n",
    "    if name == 'np':\n",
    "        np_step_idx = i\n",
    "        break\n",
    "\n",
    "# Ch·∫°y pipeline con ƒë·ªÉ fit d·ªØ li·ªáu (B·∫Øt bu·ªôc ch·∫°y ƒë·ªÉ c·∫≠p nh·∫≠t tr·∫°ng th√°i b√™n trong)\n",
    "print(\"‚öôÔ∏è ƒêang tr√≠ch xu·∫•t t√™n features...\")\n",
    "sub_pipeline = Pipeline(pipeline.steps[:np_step_idx+1])\n",
    "\n",
    "# Ta v·∫´n ch·∫°y fit_transform, nh∆∞ng KH√îNG d√πng k·∫øt qu·∫£ tr·∫£ v·ªÅ ƒë·ªÉ l·∫•y t√™n c·ªôt\n",
    "_ = sub_pipeline.fit_transform([ref_data]) \n",
    "\n",
    "# [FIX] L·∫•y t√™n c·ªôt t·ª´ thu·ªôc t√≠nh n·ªôi b·ªô c·ªßa b∆∞·ªõc Numpyfier\n",
    "try:\n",
    "    # Truy c·∫≠p v√†o b∆∞·ªõc 'np' trong pipeline ƒë√£ fit\n",
    "    numpyfier_step = sub_pipeline.named_steps['np']\n",
    "    \n",
    "    # Ki·ªÉm tra xem org_mocap_ l∆∞u DataFrame tr·ª±c ti·∫øp hay b·ªçc trong MocapData\n",
    "    if hasattr(numpyfier_step.org_mocap_, 'columns'):\n",
    "        # Tr∆∞·ªùng h·ª£p 1: org_mocap_ l√† DataFrame\n",
    "        feature_names = list(numpyfier_step.org_mocap_.columns)\n",
    "    elif hasattr(numpyfier_step.org_mocap_, 'values'):\n",
    "        # Tr∆∞·ªùng h·ª£p 2: org_mocap_ l√† MocapData (pymo th∆∞·ªùng d√πng c√°ch n√†y)\n",
    "        feature_names = list(numpyfier_step.org_mocap_.values.columns)\n",
    "    else:\n",
    "        raise ValueError(\"Kh√¥ng t√¨m th·∫•y thu·ªôc t√≠nh columns trong org_mocap_\")\n",
    "\n",
    "    print(f\"‚úÖ T√¨m th·∫•y {len(feature_names)} features (Kh·ªõp 264).\")\n",
    "    \n",
    "    # In th·ª≠ 5 t√™n ƒë·∫ßu ti√™n ƒë·ªÉ ki·ªÉm tra\n",
    "    print(f\"   Sample features: {feature_names[:5]}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå L·ªói khi tr√≠ch xu·∫•t t·ª´ Numpyfier: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# 3. Logic t√°ch Hand vs Body (Gi·ªØ nguy√™n)\n",
    "hand_keywords = ['Hand', 'Finger', 'Thumb', 'Index', 'Middle', 'Ring', 'Pinky', 'Carpal']\n",
    "\n",
    "hand_indices = []\n",
    "body_indices = []\n",
    "\n",
    "print(\"\\nüîç ƒêang ph√¢n lo·∫°i...\")\n",
    "for idx, name in enumerate(feature_names):\n",
    "    # N·∫øu t√™n kh·ªõp ch·ª©a t·ª´ kh√≥a tay\n",
    "    is_hand = any(k.lower() in name.lower() for k in hand_keywords)\n",
    "    # Lo·∫°i tr·ª´ c√°nh tay/vai (ForeArm, Arm, Shoulder thu·ªôc v·ªÅ Body)\n",
    "    if 'ForeArm' in name or 'Arm' in name or 'Shoulder' in name:\n",
    "        is_hand = False\n",
    "        \n",
    "    if is_hand:\n",
    "        hand_indices.append(idx)\n",
    "    else:\n",
    "        body_indices.append(idx)\n",
    "\n",
    "# 4. L∆∞u file (Gi·ªØ nguy√™n)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(os.path.join(save_dir, 'hand_indices.npy'), np.array(hand_indices))\n",
    "np.save(os.path.join(save_dir, 'body_indices.npy'), np.array(body_indices))\n",
    "\n",
    "print(f\"\\nüíæ ƒê√£ l∆∞u th√†nh c√¥ng:\")\n",
    "print(f\"   - {os.path.join(save_dir, 'hand_indices.npy')} ({len(hand_indices)} chi·ªÅu)\")\n",
    "print(f\"   - {os.path.join(save_dir, 'body_indices.npy')} ({len(body_indices)} chi·ªÅu)\")\n",
    "print(f\"   - T·ªïng c·ªông: {len(hand_indices) + len(body_indices)} indices.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "091eccc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ B·∫ÆT ƒê·∫¶U SANITY CHECK VQ-VAE...\n",
      "‚úÖ ƒê√£ load Hand/Body Indices.\n",
      "üì¶ Loading Model...\n",
      "[INFO] Using scale_factor=1.000000 for latent normalization\n",
      "Loading VQ-VAE from ./checkpoints/beat/VQVAE_BEAT/model/best_model.tar\n",
      "VQ-VAE loaded successfully\n",
      "VQ-VAE frozen\n",
      "VQLatentDiffusion initialized:\n",
      "  - Input features: 512\n",
      "  - Latent seq length: 45\n",
      "  - Transformer latent dim: 512\n",
      "  - Scale factor: 1.000000\n",
      "‚ö†Ô∏è ƒê√£ pad d·ªØ li·ªáu t·ª´ 164 -> 168 frames (cho chia h·∫øt 8)\n",
      "\n",
      "üé¨ Input Shape (Normalized & Padded): torch.Size([1, 168, 264])\n",
      "\n",
      "üìä K·∫æT QU·∫¢ ƒê√ÅNH GI√Å VQ-VAE:\n",
      "üîπ MSE T·ªïng th·ªÉ (C√†ng th·∫•p c√†ng t·ªët): 0.018086\n",
      "üîπ MSE Body: 0.050603\n",
      "üîπ MSE Hand: 0.001267\n",
      "üëâ T·ª∑ l·ªá L·ªói Tay / L·ªói Body: 0.03 l·∫ßn\n",
      "\n",
      "‚úÖ VQ-VAE t√°i t·∫°o tay T·ªêT.\n",
      "   -> V·∫•n ƒë·ªÅ n·∫±m ·ªü Diffusion Model ch∆∞a h·ªçc ƒë∆∞·ª£c ho·∫∑c m·∫•t Gradient.\n",
      "\n",
      "üíæ ƒê√£ l∆∞u file t√°i t·∫°o t·∫°i: vqvae_sanity_check.npy\n",
      "   H√£y visualize file n√†y. N·∫øu tay x·∫•u -> 100% l·ªói do VQ-VAE.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# # 1. Setup Paths\n",
    "# ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "# if ROOT not in sys.path:\n",
    "#     sys.path.insert(0, ROOT)\n",
    "\n",
    "from models.vq_diffusion import create_vq_latent_diffusion\n",
    "\n",
    "# ==========================================\n",
    "# C·∫§U H√åNH KI·ªÇM TRA\n",
    "# ==========================================\n",
    "class Config:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # ƒê∆∞·ªùng d·∫´n quan tr·ªçng\n",
    "    checkpoints_dir = \"./checkpoints\"\n",
    "    dataset_name = 'beat'\n",
    "    vqvae_name = 'VQVAE_BEAT'\n",
    "    \n",
    "    # File motion th·∫≠t ƒë·ªÉ test (Ch·ªçn 1 file b·∫•t k·ª≥ trong t·∫≠p train/val)\n",
    "    # File n√†y ph·∫£i CH∆ØA qua x·ª≠ l√Ω (Raw .npy 264 chi·ªÅu)\n",
    "    test_motion_path = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/BEAT_numpy/npy/1/1_wayne_0_1_1_sentence_000.npy\" \n",
    "    \n",
    "    # Load l·∫°i Mean/Std/Indices\n",
    "    stats_path = \"./global_pipeline.pkl\"\n",
    "    hand_indices_path = \"./datasets/BEAT_indices/hand_indices.npy\"\n",
    "    body_indices_path = \"./datasets/BEAT_indices/body_indices.npy\"\n",
    "\n",
    "args = Config()\n",
    "\n",
    "print(\"üöÄ B·∫ÆT ƒê·∫¶U SANITY CHECK VQ-VAE...\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. LOAD T√ÄI NGUY√äN\n",
    "# ==========================================\n",
    "# A. Load Stats (Mean/Std) ƒë·ªÉ Normalize\n",
    "if not os.path.exists(args.stats_path):\n",
    "    print(f\"‚ùå Thi·∫øu file stats: {args.stats_path}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "pipeline = joblib.load(args.stats_path)\n",
    "scaler = pipeline.named_steps['stdscale']\n",
    "mean = torch.from_numpy(scaler.data_mean_).float().to(args.device)\n",
    "std = torch.from_numpy(scaler.data_std_).float().to(args.device)\n",
    "\n",
    "# B. Load Indices (ƒê·ªÉ ƒëo l·ªói ri√™ng bi·ªát)\n",
    "if os.path.exists(args.hand_indices_path):\n",
    "    hand_indices = torch.from_numpy(np.load(args.hand_indices_path)).long().to(args.device)\n",
    "    body_indices = torch.from_numpy(np.load(args.body_indices_path)).long().to(args.device)\n",
    "    print(\"‚úÖ ƒê√£ load Hand/Body Indices.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y Indices, s·∫Ω ch·ªâ t√≠nh l·ªói t·ªïng.\")\n",
    "    hand_indices = None\n",
    "\n",
    "# C. Load Model (VQ-VAE n·∫±m b√™n trong)\n",
    "print(\"üì¶ Loading Model...\")\n",
    "model = create_vq_latent_diffusion(\n",
    "    dataset_name=args.dataset_name,\n",
    "    vqvae_name=args.vqvae_name,\n",
    "    checkpoints_dir=args.checkpoints_dir,\n",
    "    device=args.device,\n",
    "    freeze_vqvae=True, # Ch·ªâ test, kh√¥ng train\n",
    "    scale_factor=1.0   # Scale kh√¥ng quan tr·ªçng khi test recon (nh√¢n r·ªìi chia s·∫Ω h·∫øt)\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# ==========================================\n",
    "# 3. CH·∫†Y KI·ªÇM TRA (RECONSTRUCTION) - FIXED\n",
    "# ==========================================\n",
    "# Load motion th·∫≠t\n",
    "if not os.path.exists(args.test_motion_path):\n",
    "    print(f\"‚ùå Kh√¥ng t√¨m th·∫•y file motion: {args.test_motion_path}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "raw_np = np.load(args.test_motion_path)\n",
    "if raw_np.ndim == 2:\n",
    "    raw_np = raw_np[np.newaxis, ...] # (1, T, 264)\n",
    "\n",
    "# [FIX 1] Padding cho chia h·∫øt s·ªë n√©n (Downsampling factor = 8 v·ªõi BEAT)\n",
    "# 164 % 8 != 0 -> G√¢y l·ªói Conv1D\n",
    "B, T, D = raw_np.shape\n",
    "down_t = 3 # Config c·ªßa BEAT\n",
    "stride = 2 ** down_t # = 8\n",
    "remainder = T % stride\n",
    "\n",
    "if remainder != 0:\n",
    "    pad_len = stride - remainder\n",
    "    # Padding th√™m v√†o cu·ªëi chu·ªói th·ªùi gian b·∫±ng c√°ch l·∫∑p l·∫°i frame cu·ªëi (edge)\n",
    "    raw_np = np.pad(raw_np, ((0,0), (0, pad_len), (0,0)), mode='edge')\n",
    "    print(f\"‚ö†Ô∏è ƒê√£ pad d·ªØ li·ªáu t·ª´ {T} -> {raw_np.shape[1]} frames (cho chia h·∫øt {stride})\")\n",
    "\n",
    "# Chuy·ªÉn sang Tensor & Normalize\n",
    "real_motion = torch.from_numpy(raw_np).float().to(args.device)\n",
    "real_motion_norm = (real_motion - mean) / std\n",
    "\n",
    "print(f\"\\nüé¨ Input Shape (Normalized & Padded): {real_motion_norm.shape}\")\n",
    "\n",
    "# [FIX 2] G·ªçi tr·ª±c ti·∫øp VQ-VAE ƒë·ªÉ tr√°nh Assert ƒë·ªô d√†i c·ªßa Diffusion Wrapper\n",
    "with torch.no_grad():\n",
    "    # A. Encode tr·ª±c ti·∫øp b·∫±ng VQ-VAE\n",
    "    # H√†m encode tr·∫£ v·ªÅ: code_idx, all_codes (Q, B, D, T_latent)\n",
    "    code_idx, all_codes = model.vqvae.encode(real_motion_norm)\n",
    "    \n",
    "    # B. L·∫•y Latent Sum (Continuous)\n",
    "    # Shape l√∫c n√†y l√† (B, D, T_latent) - ƒë√∫ng chu·∫©n ƒë·ªÉ ƒë∆∞a v√†o Decoder\n",
    "    latent_sum = all_codes.sum(dim=0) \n",
    "    \n",
    "    # C. Decode tr·ª±c ti·∫øp\n",
    "    # Kh√¥ng d√πng model.decode_from_latent v√¨ n√≥ y√™u c·∫ßu shape (B, T, D) v√† scale_factor\n",
    "    # Ta ƒë∆∞a th·∫≥ng v√†o decoder g·ªëc c·ªßa VQ-VAE\n",
    "    recon_motion_norm = model.vqvae.decoder(latent_sum)\n",
    "\n",
    "    # C·∫Øt b·ªè ph·∫ßn padding th·ª´a l√∫c n√£y (n·∫øu mu·ªën ch√≠nh x√°c tuy·ªát ƒë·ªëi)\n",
    "    if remainder != 0:\n",
    "        recon_motion_norm = recon_motion_norm[:, :T, :]\n",
    "        real_motion_norm = real_motion_norm[:, :T, :]\n",
    "\n",
    "# ==========================================\n",
    "# 4. T√çNH TO√ÅN SAI S·ªê (ERROR METRICS)\n",
    "# ==========================================\n",
    "# T√≠nh MSE tr√™n kh√¥ng gian Normalized\n",
    "diff = (real_motion_norm - recon_motion_norm) ** 2\n",
    "mse_total = diff.mean().item()\n",
    "\n",
    "print(\"\\nüìä K·∫æT QU·∫¢ ƒê√ÅNH GI√Å VQ-VAE:\")\n",
    "print(f\"üîπ MSE T·ªïng th·ªÉ (C√†ng th·∫•p c√†ng t·ªët): {mse_total:.6f}\")\n",
    "\n",
    "if hand_indices is not None:\n",
    "    # T√≠nh MSE ri√™ng cho tay\n",
    "    mse_hand = diff[:, :, hand_indices].mean().item()\n",
    "    mse_body = diff[:, :, body_indices].mean().item()\n",
    "    \n",
    "    print(f\"üîπ MSE Body: {mse_body:.6f}\")\n",
    "    print(f\"üîπ MSE Hand: {mse_hand:.6f}\")\n",
    "    \n",
    "    ratio = mse_hand / mse_body if mse_body > 0 else 0\n",
    "    print(f\"üëâ T·ª∑ l·ªá L·ªói Tay / L·ªói Body: {ratio:.2f} l·∫ßn\")\n",
    "    \n",
    "    if ratio > 5.0:\n",
    "        print(\"\\n‚ö†Ô∏è  C·∫¢NH B√ÅO: L·ªói ·ªü tay cao g·∫•p 5 l·∫ßn body!\")\n",
    "        print(\"   -> CH√çNH X√ÅC: VQ-VAE ƒëang n√©n m·∫•t chi ti·∫øt tay.\")\n",
    "        print(\"   -> L·ªùi khuy√™n: ƒê·ª´ng c·ªë ch·ªânh Diffusion n·ªØa. VQ-VAE l√† th·ªß ph·∫°m.\")\n",
    "    elif mse_hand < 0.1: \n",
    "        print(\"\\n‚úÖ VQ-VAE t√°i t·∫°o tay T·ªêT.\")\n",
    "        print(\"   -> V·∫•n ƒë·ªÅ n·∫±m ·ªü Diffusion Model ch∆∞a h·ªçc ƒë∆∞·ª£c ho·∫∑c m·∫•t Gradient.\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è VQ-VAE t√°i t·∫°o tay TRUNG B√åNH.\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. L∆ØU K·∫æT QU·∫¢\n",
    "# ==========================================\n",
    "recon_motion = recon_motion_norm * std + mean\n",
    "recon_np = recon_motion.cpu().numpy().squeeze()\n",
    "\n",
    "save_path = \"vqvae_sanity_check.npy\"\n",
    "np.save(save_path, recon_np)\n",
    "print(f\"\\nüíæ ƒê√£ l∆∞u file t√°i t·∫°o t·∫°i: {save_path}\")\n",
    "print(f\"   H√£y visualize file n√†y. N·∫øu tay x·∫•u -> 100% l·ªói do VQ-VAE.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motion_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
