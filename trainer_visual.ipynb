{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af10b203",
   "metadata": {},
   "source": [
    "### Chu·∫©n b·ªã data\n",
    "\n",
    "```text\n",
    "datasets/\n",
    "‚îú‚îÄ‚îÄ BEAT/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 1/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 2/\n",
    "|   ‚îú‚îÄ‚îÄ ...\n",
    "|   ‚îî‚îÄ‚îÄ ...\n",
    "‚îú‚îÄ‚îÄ BEAT_numpy/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ npy/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ txt/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9b0d3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/BEAT\"\n",
    "out_dir = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/BEAT_numpy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a287d95",
   "metadata": {},
   "source": [
    "### T√≠nh mean/std cho to√†n b·ªô dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d812d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/step1_fit_scaler.py\" --parent-dir \"{base_dir}\" --start 1 --end 30 --mode hybrid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3acb60",
   "metadata": {},
   "source": [
    "### Chuy·ªÉn dataset bvh sang npy theo t·ª´ng segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb50064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# for i in range(6, 31):\n",
    "#     print(\"ƒêang ch·∫°y preprocess_data.py...\")\n",
    "#     command2 = f'python \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/preprocess_data.py\" --parent-dir \"{base_dir}\" --out-root \"{out_dir}\" --folders \"{i}\"'\n",
    "#     os.system(command2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98171b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/preprocess_data.py\" --parent-dir \"{base_dir}\" --out-root \"{out_dir}\" --folders \"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df8ec042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from textgrid import TextGrid\n",
    "\n",
    "# tg = TextGrid.fromFile(\"/home/serverai/ltdoanh/Motion_Diffusion/datasets/BEAT/5/5_stewart_0_82_82.TextGrid\")\n",
    "# print(f\"Number of tiers: {len(tg)}\")\n",
    "# for i, tier in enumerate(tg):\n",
    "#     print(f\"Tier {i}: {tier.name}, intervals: {len(tier.intervals)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7d2957",
   "metadata": {},
   "source": [
    "### Train MotionDiffuse nh∆∞ b√¨nh th∆∞·ªùng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d6c81c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python \"/home/serverai/ltdoanh/Motion_Diffusion/tools/train.py\" --dataset_name beat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf77ac5",
   "metadata": {},
   "source": [
    "### Train VQ-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed13924",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python \"/home/serverai/ltdoanh/Motion_Diffusion/tools/train_vq.py\" --dataset_name beat --codebook_size 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab74aaf",
   "metadata": {},
   "source": [
    "### Train MotionDiffuse tr√™n Latent Space do VQ-VAE ·ªü tr√™n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf31078",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python \"/home/serverai/ltdoanh/Motion_Diffusion/tools/train_vq_diffusion.py\" --dataset_name beat --vqvae_name VQVAE_BEAT --sampler ddim --max_epoch 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e1052a",
   "metadata": {},
   "source": [
    "### Evaluation - ƒêang fix l·ªói ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca744de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python \"/home/serverai/ltdoanh/Motion_Diffusion/run_evaluation.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9950bdf",
   "metadata": {},
   "source": [
    "### Visual data b·∫±ng mean/std chu·∫©n t√≠nh t·ª´ b·ªô d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9f0d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Setup path\n",
    "PYOM_DIR = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/pymo\"\n",
    "if PYOM_DIR not in sys.path:\n",
    "    sys.path.insert(0, PYOM_DIR)\n",
    "\n",
    "from pymo.parsers import BVHParser\n",
    "from pymo.preprocessing import *\n",
    "from pymo.viz_tools import draw_stickfigure, draw_stickfigure3d\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "\n",
    "# ===== CELL 2: Load Pipeline =====\n",
    "pipeline_path = \"/home/serverai/ltdoanh/Motion_Diffusion/global_pipeline.pkl\"\n",
    "\n",
    "print(f\"üì¶ Loading pipeline from: {pipeline_path}\")\n",
    "\n",
    "pipeline = joblib.load(pipeline_path)\n",
    "print(f\"‚úÖ Pipeline loaded!\")\n",
    "print(f\"   Mean shape: {pipeline.named_steps['stdscale'].data_mean_.shape}, First 5 values: {pipeline.named_steps['stdscale'].data_mean_[:5]}\")\n",
    "print(f\"   Std shape: {pipeline.named_steps['stdscale'].data_std_.shape},  First 5 values: {pipeline.named_steps['stdscale'].data_std_[:5]}\")\n",
    "\n",
    "# ===== CELL 3: Load and Visualize Motion =====\n",
    "npy_path = \"/home/serverai/ltdoanh/Motion_Diffusion/results/motion_inference-02.npy\"\n",
    "frame = 50\n",
    "\n",
    "print(f\"\\nüé¨ Visualizing: {os.path.basename(npy_path)}\")\n",
    "print(f\"   Frame: {frame}\")\n",
    "\n",
    "# Load motion data\n",
    "motion_data = np.load(npy_path)\n",
    "print(f\"   Motion shape: {motion_data.shape}\")\n",
    "\n",
    "# Inverse transform\n",
    "print(\"   Performing inverse transform...\")\n",
    "reconstructed = pipeline.inverse_transform([motion_data])\n",
    "print(f\"   ‚úÖ Reconstructed shape: {reconstructed[0].values.shape}\")\n",
    "\n",
    "# Visualize 2D\n",
    "print(\"\\n   Creating 2D visualization...\")\n",
    "fig1 = plt.figure(figsize=(10, 8))\n",
    "draw_stickfigure(reconstructed[0], frame=frame)\n",
    "plt.title(f\"2D Stick Figure - Frame {frame}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"   ‚úÖ 2D plot displayed!\")\n",
    "\n",
    "# Visualize 3D\n",
    "print(\"\\n   Creating 3D visualization...\")\n",
    "fig2 = plt.figure(figsize=(10, 8))\n",
    "draw_stickfigure3d(reconstructed[0], frame=frame)\n",
    "plt.title(f\"3D Stick Figure - Frame {frame}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"   ‚úÖ 3D plot displayed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9286cdc0",
   "metadata": {},
   "source": [
    "### Visual data b·∫±ng mean/std t·ª´ model ƒë∆∞·ª£c hu·∫•n luy·ªán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65c2f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "\n",
    "# Setup path\n",
    "PYOM_DIR = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/pymo\"\n",
    "if PYOM_DIR not in sys.path:\n",
    "    sys.path.insert(0, PYOM_DIR)\n",
    "\n",
    "from pymo.parsers import BVHParser\n",
    "from pymo.preprocessing import *\n",
    "from pymo.viz_tools import draw_stickfigure, draw_stickfigure3d\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "\n",
    "# ===== CREATE PIPELINE WITH META =====\n",
    "print(\"\\nüî® Creating pipeline from scratch...\")\n",
    "\n",
    "# 1. Create empty pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('param', MocapParameterizer('position')),\n",
    "    ('rcpn', RootCentricPositionNormalizer()),\n",
    "    ('delta', RootTransformer('abdolute_translation_deltas')),\n",
    "    ('const', ConstantsRemover()),\n",
    "    ('np', Numpyfier()),\n",
    "    ('down', DownSampler(2)),\n",
    "    ('stdscale', ListStandardScaler())\n",
    "])\n",
    "\n",
    "# 2. Fit pipeline on sample BVH (to learn structure)\n",
    "bvh_sample_path = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/BEAT/1/1_wayne_0_1_1.bvh\"\n",
    "print(f\"   Fitting on BVH sample: {os.path.basename(bvh_sample_path)}\")\n",
    "\n",
    "parser = BVHParser()\n",
    "parsed_data = parser.parse(bvh_sample_path)\n",
    "pipeline.fit([parsed_data])\n",
    "print(\"   ‚úÖ Pipeline fitted (structure learned)\")\n",
    "\n",
    "# 3. Load mean/std from meta directory\n",
    "# meta_dir = \"/home/serverai/ltdoanh/Motion_Diffusion/checkpoints/beat/test/meta\"\n",
    "# mean_path = os.path.join(meta_dir, \"mean.npy\")\n",
    "# std_path = os.path.join(meta_dir, \"std.npy\")\n",
    "ckpt_path = \"/home/serverai/ltdoanh/Motion_Diffusion/checkpoints/beat/vq_diffusion/model/latest.pt\"  # Thay ƒë∆∞·ªùng d·∫´n file c·ªßa b·∫°n v√†o ƒë√¢y\n",
    "\n",
    "try:\n",
    "    # 2. Load checkpoint\n",
    "    # map_location='cpu' gi√∫p tr√°nh l·ªói n·∫øu m√°y b·∫°n kh√¥ng c√≥ GPU gi·ªëng l√∫c train\n",
    "    checkpoint = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
    "\n",
    "    # 3. Ki·ªÉm tra v√† l·∫•y mean, std\n",
    "    if 'mean' in checkpoint and 'std' in checkpoint:\n",
    "        mean = checkpoint['mean']\n",
    "        std = checkpoint['std']\n",
    "\n",
    "        print(\"--- ƒê√£ t√¨m th·∫•y Mean v√† Std ---\")\n",
    "        print(f\"Shape c·ªßa Mean: {mean.shape}\")\n",
    "        print(f\"Shape c·ªßa Std: {std.shape}\")\n",
    "        \n",
    "        # In th·ª≠ v√†i gi√° tr·ªã ƒë·∫ßu\n",
    "        print(f\"Mean (5 gi√° tr·ªã ƒë·∫ßu): {mean[:5]}\")\n",
    "        print(f\"Std (5 gi√° tr·ªã ƒë·∫ßu): {std[:5]}\")\n",
    "        \n",
    "        # 4. (T√πy ch·ªçn) L∆∞u l·∫°i ra file .npy ƒë·ªÉ d√πng vi·ªác kh√°c n·∫øu c·∫ßn\n",
    "        # np.save('mean.npy', mean)\n",
    "        # np.save('std.npy', std)\n",
    "        # print(\"ƒê√£ l∆∞u ra file .npy\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Kh√¥ng t√¨m th·∫•y key 'mean' ho·∫∑c 'std' trong file .pt n√†y.\")\n",
    "        print(\"C√°c keys hi·ªán c√≥:\", checkpoint.keys())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"C√≥ l·ªói x·∫£y ra: {e}\")\n",
    "\n",
    "# 4. Override pipeline's mean/std with meta values\n",
    "print(\"\\nüîß Overriding pipeline statistics with meta values...\")\n",
    "pipeline.named_steps['stdscale'].data_mean_ = mean\n",
    "pipeline.named_steps['stdscale'].data_std_ = std\n",
    "print(\"   ‚úÖ Pipeline updated with meta statistics!\")\n",
    "\n",
    "# ===== VISUALIZE MOTION =====\n",
    "npy_path = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/BEAT_numpy/npy/2/2_scott_0_2_2_sentence_000.npy\"\n",
    "frame = 50\n",
    "\n",
    "print(f\"\\nüé¨ Visualizing: {os.path.basename(npy_path)}\")\n",
    "print(f\"   Frame: {frame}\")\n",
    "\n",
    "# Load motion data\n",
    "motion_data = np.load(npy_path)\n",
    "print(f\"   Motion shape: {motion_data.shape}\")\n",
    "\n",
    "# Inverse transform\n",
    "print(\"   Performing inverse transform...\")\n",
    "reconstructed = pipeline.inverse_transform([motion_data])\n",
    "print(f\"   ‚úÖ Reconstructed shape: {reconstructed[0].values.shape}\")\n",
    "\n",
    "# Visualize 2D\n",
    "print(\"\\n   Creating 2D visualization...\")\n",
    "fig1 = plt.figure(figsize=(10, 8))\n",
    "draw_stickfigure(reconstructed[0], frame=frame)\n",
    "plt.title(f\"2D Stick Figure (Meta) - Frame {frame}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"   ‚úÖ 2D plot displayed!\")\n",
    "\n",
    "# Visualize 3D\n",
    "print(\"\\n   Creating 3D visualization...\")\n",
    "fig2 = plt.figure(figsize=(10, 8))\n",
    "draw_stickfigure3d(reconstructed[0], frame=frame)\n",
    "plt.title(f\"3D Stick Figure (Meta) - Frame {frame}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"   ‚úÖ 3D plot displayed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83329b36",
   "metadata": {},
   "source": [
    "### Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c3d1766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading checkpoint: /home/serverai/ltdoanh/Motion_Diffusion/checkpoints/beat/vq_diffusion/model/best_model.pt\n",
      "üîç Detected num_quantizers: 10\n",
      "üîß Initializing MotionTransformer...\n",
      "üîß Initializing RVQVAE...\n",
      "üöÄ Starting Inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:25<00:00, 39.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Latent generated shape: torch.Size([1, 45, 512])\n",
      "   Decoding with RVQVAE...\n",
      "üéâ Final Motion Shape: (1, 360, 264)\n",
      "üíæ Saved to: /home/serverai/ltdoanh/Motion_Diffusion/results/motion_inference-04.npy\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from models import MotionTransformer\n",
    "from trainers import DDPMTrainer\n",
    "from models.vq.model import RVQVAE \n",
    "\n",
    "# ==========================================\n",
    "# 1. C·∫•u h√¨nh Inference\n",
    "# ==========================================\n",
    "class InferenceConfig:\n",
    "    def __init__(self):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        self.is_train = False\n",
    "        self.schedule_sampler = 'uniform'   \n",
    "\n",
    "        # --- C·∫•u h√¨nh Diffusion ---\n",
    "        self.input_feats = 512     # Latent Dimension\n",
    "        self.num_frames = 45      # Latent Length (360 / 8)\n",
    "        self.num_layers = 8\n",
    "        self.latent_dim = 512\n",
    "        self.ff_size = 1024\n",
    "        self.num_heads = 8\n",
    "        self.dropout = 0.1\n",
    "        self.activation = \"gelu\"\n",
    "        self.dataset_name = 'beat' \n",
    "        self.do_denoise = True\n",
    "        self.noise_schedule = 'cosine'\n",
    "        self.diffusion_steps = 1000\n",
    "        self.no_clip = False\n",
    "        self.no_eff = False\n",
    "        self.result_dir = \"/home/serverai/ltdoanh/Motion_Diffusion/results\"\n",
    "\n",
    "# Class gi·∫£ l·∫≠p args cho RVQVAE\n",
    "class VQArgs:\n",
    "    def __init__(self):\n",
    "        # C√°c gi√° tr·ªã m·∫∑c ƒë·ªãnh, s·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t logic b√™n d∆∞·ªõi\n",
    "        self.num_quantizers = 1 \n",
    "        self.shared_codebook = False\n",
    "        self.quantize_dropout_prob = 0.0\n",
    "        self.mu = 0.99 # Cho QuantizerEMA\n",
    "\n",
    "opt = InferenceConfig()\n",
    "vq_args = VQArgs()\n",
    "\n",
    "# ==========================================\n",
    "# 2. Load Checkpoint & T√°ch Weights\n",
    "# ==========================================\n",
    "ckpt_path = \"/home/serverai/ltdoanh/Motion_Diffusion/checkpoints/beat/vq_diffusion/model/best_model.pt\"\n",
    "print(f\"üìÇ Loading checkpoint: {ckpt_path}\")\n",
    "\n",
    "# Load to√†n b·ªô checkpoint\n",
    "checkpoint = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
    "state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "# --- T·ª± ƒë·ªông ph√°t hi·ªán s·ªë l∆∞·ª£ng Quantizers t·ª´ Checkpoint ---\n",
    "# ƒêi·ªÅu n√†y gi√∫p tr√°nh l·ªói sai l·ªách key khi kh·ªüi t·∫°o VQ-VAE\n",
    "max_layer_idx = 0\n",
    "for k in state_dict.keys():\n",
    "    if \"vqvae.quantizer.layers.\" in k:\n",
    "        # Parse t√¨m s·ªë l·ªõn nh·∫•t trong 'layers.X.'\n",
    "        try:\n",
    "            parts = k.split('.')\n",
    "            layer_idx = int(parts[parts.index('layers') + 1])\n",
    "            if layer_idx > max_layer_idx:\n",
    "                max_layer_idx = layer_idx\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "vq_args.num_quantizers = max_layer_idx + 1\n",
    "print(f\"üîç Detected num_quantizers: {vq_args.num_quantizers}\")\n",
    "\n",
    "# --- T√°ch Dictionary ---\n",
    "trans_dict = {}\n",
    "vqvae_dict = {}\n",
    "\n",
    "for k, v in state_dict.items():\n",
    "    if k.startswith('transformer.'):\n",
    "        trans_dict[k[12:]] = v  \n",
    "    elif k.startswith('vqvae.'):\n",
    "        vqvae_dict[k[6:]] = v   \n",
    "\n",
    "# ==========================================\n",
    "# 3. Kh·ªüi t·∫°o Models\n",
    "# ==========================================\n",
    "\n",
    "# A. Motion Transformer\n",
    "print(\"üîß Initializing MotionTransformer...\")\n",
    "encoder = MotionTransformer(\n",
    "    input_feats=opt.input_feats,\n",
    "    num_frames=opt.num_frames,\n",
    "    num_layers=opt.num_layers,\n",
    "    latent_dim=opt.latent_dim,\n",
    "    num_heads=opt.num_heads,\n",
    "    ff_size=opt.ff_size,\n",
    "    no_clip=opt.no_clip,\n",
    "    no_eff=opt.no_eff\n",
    ")\n",
    "encoder.load_state_dict(trans_dict, strict=True)\n",
    "encoder.to(opt.device).eval()\n",
    "\n",
    "# B. RVQVAE\n",
    "print(\"üîß Initializing RVQVAE...\")\n",
    "# L∆∞u √Ω: C√°c tham s·ªë d∆∞·ªõi ƒë√¢y ph·∫£i kh·ªõp v·ªõi file config l√∫c train VQVAE c·ªßa b·∫°n.\n",
    "# T√¥i ƒëang ƒë·ªÉ c√°c gi√° tr·ªã ph·ªï bi·∫øn d·ª±a tr√™n file model.py\n",
    "vqvae_model = RVQVAE(\n",
    "    args=vq_args,\n",
    "    input_width=264,       # BEAT dataset th∆∞·ªùng l√† 264\n",
    "    nb_code=512,           # Ki·ªÉm tra l·∫°i config train c≈© n·∫øu l·ªói\n",
    "    code_dim=512, \n",
    "    output_emb_width=512, \n",
    "    down_t=3, \n",
    "    stride_t=2, \n",
    "    width=512, \n",
    "    depth=3, \n",
    "    dilation_growth_rate=3,\n",
    "    activation='relu',\n",
    "    norm=None\n",
    ")\n",
    "vqvae_model.load_state_dict(vqvae_dict, strict=True)\n",
    "vqvae_model.to(opt.device).eval()\n",
    "\n",
    "# ==========================================\n",
    "# 4. Inference\n",
    "# ==========================================\n",
    "trainer = DDPMTrainer(opt, encoder)\n",
    "\n",
    "# Inject mean/std (Quan tr·ªçng cho qu√° tr√¨nh decode cu·ªëi c√πng)\n",
    "trainer.mean = checkpoint['mean']\n",
    "trainer.std = checkpoint['std']\n",
    "\n",
    "print(\"üöÄ Starting Inference...\")\n",
    "os.makedirs(opt.result_dir, exist_ok=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    caption = [\"the one thing i miss when i'm away from home is my mum for example when i'm studying in the university which is 1400 miles away from my birthplace the food of my school's cafeteria is awful\"]\n",
    "    \n",
    "    # ƒê·ªô d√†i Latent (45)\n",
    "    m_lens = torch.LongTensor([45]).to(opt.device) \n",
    "    \n",
    "    # 1. Sinh Latent (Diffusion) -> Output: (Batch, Length, Dim) = (1, 45, 512)\n",
    "    pred_latent_list = trainer.generate(caption, m_lens, dim_pose=512)\n",
    "    pred_latent = pred_latent_list[0]\n",
    "\n",
    "    if pred_latent.dim() == 2:\n",
    "        pred_latent = pred_latent.unsqueeze(0)  # Th√™m batch dim n·∫øu c·∫ßn\n",
    "\n",
    "    print(f\"   Latent generated shape: {pred_latent.shape}\")\n",
    "\n",
    "    # 2. Decode b·∫±ng RVQVAE\n",
    "    # RVQVAE Decoder c·∫ßn input: (Batch, Channel, Length) -> C·∫ßn permute\n",
    "    latent_input = pred_latent.permute(0, 2, 1) # -> (1, 512, 45)\n",
    "    \n",
    "    print(\"   Decoding with RVQVAE...\")\n",
    "    # G·ªçi tr·ª±c ti·∫øp decoder (b·ªè qua quantizer v√¨ Diffusion ƒë√£ sinh ra latent r·ªìi)\n",
    "    decoded_motion = vqvae_model.decoder(latent_input)\n",
    "    \n",
    "    # 3. Post-process (Permute l·∫°i v·ªÅ: Batch, Length, Channel)\n",
    "    # H√†m postprocess trong model.py: (B, C, T) -> (B, T, C)\n",
    "    motion = vqvae_model.postprocess(decoded_motion).cpu().numpy()\n",
    "\n",
    "    if motion.shape[1] == 264 and motion.shape[2] == 360:\n",
    "        motion = motion.transpose(0, 2, 1)\n",
    "\n",
    "    # motion = motion.cpu().numpy()\n",
    "    \n",
    "    # 4. Denormalize (Gi·∫£i chu·∫©n h√≥a)\n",
    "    # Output c·ªßa VQVAE th∆∞·ªùng v·∫´n l√† normalized data\n",
    "    mean = checkpoint['mean']\n",
    "    std = checkpoint['std']\n",
    "    \n",
    "    # ƒê·∫£m b·∫£o shape kh·ªõp ƒë·ªÉ broadcast\n",
    "    # Motion: (1, 360, 264), Mean: (264,), Std: (264,)\n",
    "    motion = motion * std + mean\n",
    "    \n",
    "    print(f\"üéâ Final Motion Shape: {motion.shape}\")\n",
    "\n",
    "# L∆∞u k·∫øt qu·∫£\n",
    "save_path = os.path.join(opt.result_dir, 'motion_inference-04.npy')\n",
    "np.save(save_path, motion)\n",
    "print(f\"üíæ Saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cabdd052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading stats form: /home/serverai/ltdoanh/Motion_Diffusion/global_pipeline.pkl\n",
      "üîß Initializing Model & Diffusion...\n",
      "Loading VQ-VAE from /home/serverai/ltdoanh/Motion_Diffusion/checkpoints/beat/VQVAE_BEAT/model/best_model.tar\n",
      "VQ-VAE loaded successfully\n",
      "VQ-VAE frozen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/serverai/motion_env/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VQLatentDiffusion initialized:\n",
      "  - Input features (latent): 512\n",
      "  - Latent sequence length: 45\n",
      "  - Transformer latent dim: 512\n",
      "  - Use continuous latent: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/serverai/motion_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded manually from /home/serverai/ltdoanh/Motion_Diffusion/checkpoints/beat/vq_diffusion/model/best_model.pt\n",
      "üöÄ Starting Inference...\n",
      "Sampling with ddim (eta=0.0)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:34<00:00, 28.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated shape: (1, 360, 264)\n",
      "üéâ Final Motion Shape: (1, 360, 264)\n",
      "üíæ Saved to: /home/serverai/ltdoanh/Motion_Diffusion/results/motion_inference_trainer-01.npy\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "\n",
    "from models.vq_diffusion import create_vq_latent_diffusion\n",
    "from models.gaussian_diffusion import (\n",
    "    GaussianDiffusion, get_named_beta_schedule,\n",
    "    ModelMeanType, ModelVarType, LossType\n",
    ")\n",
    "from tools.train_vq_diffusion import VQDiffusionTrainer\n",
    "\n",
    "# ==========================================\n",
    "# 1. C·∫•u h√¨nh Inference (Gi·∫£ l·∫≠p args)\n",
    "# ==========================================\n",
    "class InferenceConfig:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.gpu_id = 0\n",
    "        \n",
    "        self.checkpoints_dir = \"/home/serverai/ltdoanh/Motion_Diffusion/checkpoints\"\n",
    "        self.dataset_name = 'beat'\n",
    "        self.name = 'vq_diffusion'\n",
    "        \n",
    "        self.save_root = os.path.join(self.checkpoints_dir, self.dataset_name, self.name)\n",
    "        self.model_dir = os.path.join(self.save_root, 'model')\n",
    "        self.log_dir = os.path.join(self.save_root, 'logs')\n",
    "        \n",
    "        self.vqvae_name = 'VQVAE_BEAT'\n",
    "        self.freeze_vqvae = True\n",
    "        self.latent_dim = 512\n",
    "        self.num_layers = 8\n",
    "        self.num_heads = 8\n",
    "        self.ff_size = 1024\n",
    "        self.dropout = 0.1\n",
    "        self.no_eff = False\n",
    "        self.no_clip = False \n",
    "        \n",
    "        # Diffusion Config\n",
    "        self.diffusion_steps = 1000\n",
    "        self.noise_schedule = 'cosine'\n",
    "        self.schedule_sampler = 'uniform'\n",
    "        \n",
    "        self.sampler = 'ddim' \n",
    "        self.ddim_eta = 0.0\n",
    "        \n",
    "        self.lr = 2e-4\n",
    "        self.weight_decay = 0.01\n",
    "        self.max_epoch = 100 # Dummy value\n",
    "\n",
    "        # Result dir\n",
    "        self.result_dir = \"/home/serverai/ltdoanh/Motion_Diffusion/results\"\n",
    "\n",
    "args = InferenceConfig()\n",
    "\n",
    "# ==========================================\n",
    "# 2. Load Stats (Mean/Std)\n",
    "# ==========================================\n",
    "stats_file_path = \"/home/serverai/ltdoanh/Motion_Diffusion/global_pipeline.pkl\"\n",
    "print(f\"üìÇ Loading stats form: {stats_file_path}\")\n",
    "try:\n",
    "    pipeline = joblib.load(stats_file_path)\n",
    "    scaler = pipeline.named_steps['stdscale']\n",
    "    args.mean = np.array(scaler.data_mean_)\n",
    "    args.std = np.array(scaler.data_std_)\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Warning: Could not load stats. Using dummy stats. Error: {e}\")\n",
    "    args.mean = np.zeros(264) \n",
    "    args.std = np.ones(264)\n",
    "\n",
    "# ==========================================\n",
    "# 3. Kh·ªüi t·∫°o Model & Diffusion\n",
    "# ==========================================\n",
    "print(\"üîß Initializing Model & Diffusion...\")\n",
    "\n",
    "model = create_vq_latent_diffusion(\n",
    "    dataset_name=args.dataset_name,\n",
    "    vqvae_name=args.vqvae_name,\n",
    "    checkpoints_dir=args.checkpoints_dir,\n",
    "    device=args.device,\n",
    "    freeze_vqvae=args.freeze_vqvae,\n",
    "    latent_dim=args.latent_dim,\n",
    "    num_layers=args.num_layers,\n",
    "    num_heads=args.num_heads,\n",
    "    ff_size=args.ff_size,\n",
    "    dropout=args.dropout,\n",
    "    no_eff=args.no_eff\n",
    ")\n",
    "\n",
    "betas = get_named_beta_schedule(args.noise_schedule, args.diffusion_steps)\n",
    "diffusion = GaussianDiffusion(\n",
    "    betas=betas,\n",
    "    model_mean_type=ModelMeanType.EPSILON,\n",
    "    model_var_type=ModelVarType.FIXED_SMALL,\n",
    "    loss_type=LossType.MSE,\n",
    "    rescale_timesteps=False\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 4. Kh·ªüi t·∫°o Trainer & Load Checkpoint\n",
    "# ==========================================\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "dummy_data = torch.zeros(1, 45, 512) \n",
    "dummy_dataset = TensorDataset(dummy_data)\n",
    "dummy_loader = DataLoader(dummy_dataset, batch_size=1)\n",
    "\n",
    "trainer = VQDiffusionTrainer(\n",
    "    args=args,\n",
    "    model=model,\n",
    "    diffusion=diffusion,\n",
    "    data_loader=dummy_loader,  \n",
    "    val_loader=None\n",
    ")\n",
    "\n",
    "ckpt_path = \"/home/serverai/ltdoanh/Motion_Diffusion/checkpoints/beat/vq_diffusion/model/best_model.pt\" \n",
    "checkpoint = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
    "state_dict = checkpoint['model_state_dict']\n",
    "print(f\"‚úÖ Loaded manually from {ckpt_path}\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. Inference (S·ª≠ d·ª•ng h√†m sample c·ªßa Trainer)\n",
    "# ==========================================\n",
    "print(\"üöÄ Starting Inference...\")\n",
    "os.makedirs(args.result_dir, exist_ok=True)\n",
    "\n",
    "prompts = [\"the one thing i miss when i'm away from home is my mum for example when i'm studying in the university which is 1400 miles away from my birthplace the food of my school's cafeteria is awful\"]\n",
    "lengths = torch.LongTensor([45]).to(args.device) \n",
    "\n",
    "motion_output = trainer.sample(prompts, lengths)\n",
    "\n",
    "# ==========================================\n",
    "# 6. Post-processing & Save\n",
    "# ==========================================\n",
    "\n",
    "motion = motion_output.detach().cpu().numpy()\n",
    "\n",
    "print(f\"Generated shape: {motion.shape}\")\n",
    "\n",
    "if motion.shape[1] == 264 and motion.shape[2] != 264:\n",
    "     motion = motion.transpose(0, 2, 1)\n",
    "\n",
    "motion = motion * args.std + args.mean\n",
    "\n",
    "print(f\"üéâ Final Motion Shape: {motion.shape}\")\n",
    "\n",
    "save_path = os.path.join(args.result_dir, 'motion_inference_trainer-01.npy')\n",
    "np.save(save_path, motion)\n",
    "print(f\"üíæ Saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95738b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "\n",
    "# Setup path\n",
    "PYOM_DIR = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/pymo\"\n",
    "if PYOM_DIR not in sys.path:\n",
    "    sys.path.insert(0, PYOM_DIR)\n",
    "\n",
    "from pymo.parsers import BVHParser\n",
    "from pymo.preprocessing import *\n",
    "from pymo.viz_tools import draw_stickfigure, draw_stickfigure3d\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "\n",
    "# ===== CREATE PIPELINE WITH META =====\n",
    "print(\"\\nüî® Creating pipeline from scratch...\")\n",
    "\n",
    "# 1. Create empty pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('param', MocapParameterizer('position')),\n",
    "    ('rcpn', RootCentricPositionNormalizer()),\n",
    "    ('delta', RootTransformer('abdolute_translation_deltas')),\n",
    "    ('const', ConstantsRemover()),\n",
    "    ('np', Numpyfier()),\n",
    "    ('down', DownSampler(2)),\n",
    "    ('stdscale', ListStandardScaler())\n",
    "])\n",
    "\n",
    "# 2. Fit pipeline on sample BVH (to learn structure)\n",
    "bvh_sample_path = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/BEAT/1/1_wayne_0_1_1.bvh\"\n",
    "print(f\"   Fitting on BVH sample: {os.path.basename(bvh_sample_path)}\")\n",
    "\n",
    "parser = BVHParser()\n",
    "parsed_data = parser.parse(bvh_sample_path)\n",
    "pipeline.fit([parsed_data])\n",
    "print(\"   ‚úÖ Pipeline fitted (structure learned)\")\n",
    "\n",
    "# 3. Load mean/std from meta directory\n",
    "ckpt_path = \"/home/serverai/ltdoanh/Motion_Diffusion/checkpoints/beat/vq_diffusion/model/best_model.pt\"  \n",
    "\n",
    "try:\n",
    "    # 2. Load checkpoint\n",
    "    # map_location='cpu' gi√∫p tr√°nh l·ªói n·∫øu m√°y b·∫°n kh√¥ng c√≥ GPU gi·ªëng l√∫c train\n",
    "    checkpoint = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
    "\n",
    "    # 3. Ki·ªÉm tra v√† l·∫•y mean, std\n",
    "    if 'mean' in checkpoint and 'std' in checkpoint:\n",
    "        mean = checkpoint['mean']\n",
    "        std = checkpoint['std']\n",
    "\n",
    "        print(\"--- ƒê√£ t√¨m th·∫•y Mean v√† Std ---\")\n",
    "        print(f\"Shape c·ªßa Mean: {mean.shape}\")\n",
    "        print(f\"Shape c·ªßa Std: {std.shape}\")\n",
    "        \n",
    "        # In th·ª≠ v√†i gi√° tr·ªã ƒë·∫ßu\n",
    "        print(f\"Mean (5 gi√° tr·ªã ƒë·∫ßu): {mean[:5]}\")\n",
    "        print(f\"Std (5 gi√° tr·ªã ƒë·∫ßu): {std[:5]}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Kh√¥ng t√¨m th·∫•y key 'mean' ho·∫∑c 'std' trong file .pt n√†y.\")\n",
    "        print(\"C√°c keys hi·ªán c√≥:\", checkpoint.keys())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"C√≥ l·ªói x·∫£y ra: {e}\")\n",
    "\n",
    "# 4. Override pipeline's mean/std with meta values\n",
    "print(\"\\nüîß Overriding pipeline statistics with meta values...\")\n",
    "pipeline.named_steps['stdscale'].data_mean_ = mean\n",
    "pipeline.named_steps['stdscale'].data_std_ = std\n",
    "print(\"   ‚úÖ Pipeline updated with meta statistics!\")\n",
    "\n",
    "# ===== VISUALIZE MOTION =====\n",
    "npy_path = \"/home/serverai/ltdoanh/Motion_Diffusion/results/motion_inference-04.npy\"\n",
    "frame = 50\n",
    "\n",
    "print(f\"\\nüé¨ Visualizing: {os.path.basename(npy_path)}\")\n",
    "print(f\"   Frame: {frame}\")\n",
    "\n",
    "# Load motion data\n",
    "motion_data = np.load(npy_path)\n",
    "print(f\"   Motion shape: {motion_data.shape}\")\n",
    "if motion_data.ndim == 3:\n",
    "    motion_data = motion_data[0] # L·∫•y m·∫´u ƒë·∫ßu ti√™n -> (360, 264)\n",
    "    print(f\"   Squeezed Motion shape: {motion_data.shape}\")\n",
    "\n",
    "# Inverse transform\n",
    "print(\"   Performing inverse transform...\")\n",
    "reconstructed = pipeline.inverse_transform([motion_data])\n",
    "print(f\"   ‚úÖ Reconstructed shape: {reconstructed[0].values.shape}\")\n",
    "\n",
    "# Visualize 2D\n",
    "print(\"\\n   Creating 2D visualization...\")\n",
    "fig1 = plt.figure(figsize=(10, 8))\n",
    "draw_stickfigure(reconstructed[0], frame=frame)\n",
    "plt.title(f\"2D Stick Figure (Meta) - Frame {frame}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"   ‚úÖ 2D plot displayed!\")\n",
    "\n",
    "# Visualize 3D\n",
    "print(\"\\n   Creating 3D visualization...\")\n",
    "fig2 = plt.figure(figsize=(10, 8))\n",
    "draw_stickfigure3d(reconstructed[0], frame=frame)\n",
    "plt.title(f\"3D Stick Figure (Meta) - Frame {frame}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"   ‚úÖ 3D plot displayed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motion_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
