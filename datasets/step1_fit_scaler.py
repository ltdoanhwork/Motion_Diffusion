import sys, os
import joblib
import numpy as np
from tqdm import tqdm
import multiprocessing
import argparse
import re

PYOM_DIR = "/home/serverai/ltdoanh/Motion_Diffusion/datasets/pymo"
if PYOM_DIR not in sys.path:
    sys.path.insert(0, PYOM_DIR)

from pymo.parsers import BVHParser
from pymo.preprocessing import *
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin


class OnlineStatsCalculator:
    """
    T√≠nh mean/std theo c√°ch streaming (Welford's algorithm)
    ƒë·ªÉ tr√°nh load to√†n b·ªô data v√†o RAM.
    """
    def __init__(self):
        self.n = 0
        self.mean = None
        self.M2 = None
    
    def update(self, batch_data):
        """
        Update statistics v·ªõi m·ªôt batch m·ªõi.
        batch_data: numpy array shape (n_samples, n_features)
        """
        if batch_data.size == 0:
            return
            
        # Flatten n·∫øu l√† 3D (batch, time, features) -> (n_samples, features)
        if batch_data.ndim == 3:
            batch_data = batch_data.reshape(-1, batch_data.shape[-1])
        
        for sample in batch_data:
            self.n += 1
            if self.mean is None:
                self.mean = np.zeros_like(sample, dtype=np.float64)
                self.M2 = np.zeros_like(sample, dtype=np.float64)
            
            delta = sample - self.mean
            self.mean += delta / self.n
            delta2 = sample - self.mean
            self.M2 += delta * delta2
    
    def finalize(self):
        """Tr·∫£ v·ªÅ mean v√† std cu·ªëi c√πng."""
        if self.n < 2:
            return self.mean, np.ones_like(self.mean)
        
        variance = self.M2 / self.n
        std = np.sqrt(variance) + 1e-8  # Th√™m epsilon ƒë·ªÉ tr√°nh chia 0
        return self.mean, std


class FixedDownSampler(BaseEstimator, TransformerMixin):
    """DownSampler t∆∞∆°ng th√≠ch v·ªõi MocapData."""
    def __init__(self, rate):
        self.rate = rate
    
    def fit(self, X, y=None):
        return self
    
    def transform(self, X, y=None):
        Q = []
        for track in X:
            new_track = track.clone()
            new_track.values = track.values.iloc[::self.rate]
            Q.append(new_track)
        return Q
    
    def inverse_transform(self, X, copy=None):
        return X


# --- WORKER CHO MULTIPROCESSING ---
g_parser = None
g_partial_pipe = None

def init_worker():
    """Kh·ªüi t·∫°o parser v√† pipeline cho worker."""
    global g_parser, g_partial_pipe
    g_parser = BVHParser()
    g_partial_pipe = Pipeline([
        ('param', MocapParameterizer('position')),
        ('rcpn', RootCentricPositionNormalizer()),
        ('delta', RootTransformer('absolute_translation_deltas')),
    ])

def worker_parse_and_transform(bvh_path):
    """Worker: Parse + transform BVH file."""
    global g_parser, g_partial_pipe
    try:
        parsed_data = g_parser.parse(bvh_path)
        processed = g_partial_pipe.transform([parsed_data])[0]
        return processed
    except Exception:
        return None


def process_files_streaming(bvh_paths, stats_calculator, const_remover, 
                            downsampler, batch_size=100):
    """
    X·ª≠ l√Ω files theo batch v√† update statistics streaming.
    Kh√¥ng gi·ªØ to√†n b·ªô data trong RAM.
    
    Args:
        bvh_paths: List ƒë∆∞·ªùng d·∫´n files BVH
        stats_calculator: OnlineStatsCalculator instance
        const_remover: ConstantsRemover ƒë√£ fit
        downsampler: DownSampler instance
        batch_size: S·ªë files x·ª≠ l√Ω m·ªói l·∫ßn
    """
    num_cores = min(multiprocessing.cpu_count(), 8)  # Gi·ªõi h·∫°n cores
    
    for i in tqdm(range(0, len(bvh_paths), batch_size), 
                  desc="Processing batches", ncols=100):
        batch_paths = bvh_paths[i:i + batch_size]
        
        # Multiprocessing cho batch n√†y
        with multiprocessing.Pool(processes=num_cores, 
                                 initializer=init_worker) as pool:
            results = pool.map(worker_parse_and_transform, batch_paths)
        
        # L·ªçc k·∫øt qu·∫£ h·ª£p l·ªá
        valid_results = [r for r in results if r is not None]
        
        if not valid_results:
            continue
        
        # Apply ConstantsRemover
        after_const = const_remover.transform(valid_results)
        
        # Apply DownSampler
        downsampled = downsampler.transform(after_const)
        
        # Convert sang numpy v√† update stats
        batch_arrays = [track.values.values for track in downsampled]
        if batch_arrays:
            batch_data = np.concatenate(batch_arrays, axis=0)  # (total_frames, features)
            stats_calculator.update(batch_data)
        
        # X√≥a ƒë·ªÉ gi·∫£i ph√≥ng RAM
        del results, valid_results, after_const, downsampled, batch_arrays
        import gc
        gc.collect()


def main_fit_scaler_efficient(parent_dir, folders=None, start=None, end=None, 
                              batch_size=100):
    """
    Fit scaler v·ªõi memory-efficient approach.
    
    CHI·∫æN L∆Ø·ª¢C:
    1. Pass ƒë·∫ßu: Fit ConstantsRemover (c·∫ßn to√†n b·ªô data ƒë·ªÉ x√°c ƒë·ªãnh constant dims)
    2. Pass hai: T√≠nh mean/std theo streaming (kh√¥ng c·∫ßn load to√†n b·ªô v√†o RAM)
    """
    
    # --- THU TH·∫¨P FOLDERS ---
    if folders:
        to_process = folders
    else:
        entries = [d for d in os.listdir(parent_dir) 
                  if os.path.isdir(os.path.join(parent_dir, d))]
        numeric = [d for d in entries if re.fullmatch(r"\d+", d)]
        numeric_sorted = sorted(numeric, key=lambda x: int(x))
        
        if start is not None or end is not None:
            s = int(start) if start is not None else None
            e = int(end) if end is not None else None
            to_process = [d for d in numeric_sorted 
                         if (s is None or int(d) >= s) and (e is None or int(d) <= e)]
        else:
            to_process = numeric_sorted

    if not to_process:
        print("‚ùå No folders found")
        return

    print(f"\n{'='*60}")
    print(f"üéØ MEMORY-EFFICIENT MODE")
    print(f"üìÇ Folders to process: {len(to_process)}")
    print(f"{'='*60}")

    # --- THU TH·∫¨P T·∫§T C·∫¢ FILES ---
    all_bvh_paths = []
    for folder in to_process:
        folder_path = os.path.join(parent_dir, folder)
        if not os.path.isdir(folder_path):
            continue
        
        bvh_files = [
            os.path.join(folder_path, fname)
            for fname in os.listdir(folder_path)
            if fname.endswith(".bvh")
        ]
        all_bvh_paths.extend(bvh_files)

    if not all_bvh_paths:
        print("‚ùå No BVH files found")
        return

    print(f"‚úÖ Found {len(all_bvh_paths)} BVH files")

    # ============================================================
    # PASS 1: FIT ConstantsRemover (c·∫ßn sample nh·ªè, kh√¥ng ph·∫£i to√†n b·ªô)
    # ============================================================
    print(f"\n{'='*60}")
    print(f"üîç PASS 1: Fitting ConstantsRemover (sampling 1000 files)")
    print(f"{'='*60}")
    
    # L·∫•y sample ƒë·ªÉ fit ConstantsRemover (kh√¥ng c·∫ßn to√†n b·ªô data)
    sample_size = min(1000, len(all_bvh_paths))
    sample_paths = np.random.choice(all_bvh_paths, sample_size, replace=False)
    
    num_cores = min(multiprocessing.cpu_count(), 8)
    with multiprocessing.Pool(processes=num_cores, 
                             initializer=init_worker) as pool:
        sample_results = list(tqdm(
            pool.imap(worker_parse_and_transform, sample_paths),
            total=len(sample_paths),
            desc="Sampling for ConstantsRemover",
            ncols=100
        ))
    
    sample_mocap = [r for r in sample_results if r is not None]
    
    if not sample_mocap:
        print("‚ùå Failed to process sample data")
        return
    
    # Fit ConstantsRemover
    const_remover = ConstantsRemover()
    const_remover.fit(sample_mocap)
    print(f"‚úÖ ConstantsRemover fitted on {len(sample_mocap)} samples")
    print(f"   Constant columns: {len(const_remover.const_dims_)}")
    
    del sample_results, sample_mocap
    import gc
    gc.collect()

    # ============================================================
    # PASS 2: T√çNH MEAN/STD THEO STREAMING
    # ============================================================
    print(f"\n{'='*60}")
    print(f"üìä PASS 2: Computing mean/std (streaming mode)")
    print(f"   Batch size: {batch_size} files")
    print(f"{'='*60}")
    
    downsampler = FixedDownSampler(2)
    stats_calculator = OnlineStatsCalculator()
    
    process_files_streaming(
        all_bvh_paths, 
        stats_calculator, 
        const_remover,
        downsampler,
        batch_size=batch_size
    )
    
    # Finalize statistics
    mean, std = stats_calculator.finalize()
    
    print(f"\n‚úÖ Statistics computed!")
    print(f"   Mean shape: {mean.shape}")
    print(f"   Std shape: {std.shape}")

    # ============================================================
    # PASS 3: T·∫†O V√Ä L∆ØU PIPELINE
    # ============================================================
    print(f"\nüíæ Creating complete pipeline...")
    
    # T·∫°o scaler t·ª´ mean/std ƒë√£ t√≠nh
    from pymo.preprocessing import ListStandardScaler
    scaler = ListStandardScaler()
    scaler.data_mean_ = mean
    scaler.data_std_ = std
    
    # T·∫°o Numpyfier (c·∫ßn fit m·ªôt l·∫ßn)
    numpyfier = Numpyfier()
    # Fit v·ªõi sample nh·ªè
    sample_for_numpyfier = const_remover.transform(
        [sample_mocap[0]] if 'sample_mocap' in locals() else []
    )
    if sample_for_numpyfier:
        numpyfier.fit(sample_for_numpyfier)
    
    full_pipeline = Pipeline([
        ('param', MocapParameterizer('position')),
        ('rcpn', RootCentricPositionNormalizer()),
        ('delta', RootTransformer('absolute_translation_deltas')),
        ('const', const_remover),
        ('np', numpyfier),
        ('down', DownSampler(2)),
        ('stdscale', scaler)
    ])
    
    output_filename = "global_pipeline.pkl"
    joblib.dump(full_pipeline, output_filename)
    
    print(f"\n{'='*60}")
    print(f"üéâ SUCCESS!")
    print(f"{'='*60}")
    print(f"   Saved: {output_filename}")
    print(f"   Files processed: {len(all_bvh_paths)}")
    print(f"   Final features: {mean.shape[0]}")
    print(f"   Memory usage: STREAMING (no full load)")
    print(f"{'='*60}")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description='Fit global scaler with streaming (memory-efficient)'
    )
    parser.add_argument('--parent-dir', type=str, required=True,
                       help='Parent directory containing numbered folders')
    parser.add_argument('--folders', type=str,
                       help='Comma-separated list of specific folders')
    parser.add_argument('--start', type=int,
                       help='Start folder number (inclusive)')
    parser.add_argument('--end', type=int,
                       help='End folder number (inclusive)')
    parser.add_argument('--batch-size', type=int, default=100,
                       help='Number of files to process per batch (default: 100)')
    
    args = parser.parse_args()

    folders_list = None
    if args.folders:
        folders_list = [f.strip() for f in args.folders.split(',') if f.strip()]

    main_fit_scaler_efficient(
        args.parent_dir,
        folders=folders_list,
        start=args.start,
        end=args.end,
        batch_size=args.batch_size
    )