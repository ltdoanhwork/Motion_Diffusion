import sys, os
PYOM_DIR = "/home/serverai/ltdoanh/Motion_Diffusion/datasets/pymo"
if PYOM_DIR not in sys.path:
    sys.path.insert(0, PYOM_DIR)

import joblib
import pandas as pd
from pymo.parsers import BVHParser
from pymo.preprocessing import *
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin
import re
import argparse
import numpy as np
from tqdm import tqdm
import multiprocessing

# --- CUSTOM DOWNSAMPLER (Fix bug trong pymo) ---
class FixedDownSampler(BaseEstimator, TransformerMixin):
    """DownSampler hoáº¡t Ä‘á»™ng Ä‘Ãºng vá»›i MocapData objects"""
    def __init__(self, rate):
        self.rate = rate
    
    def fit(self, X, y=None):
        return self
    
    def transform(self, X, y=None):
        Q = []
        for track in X:
            # Clone vÃ  downsample DataFrame
            new_track = track.clone()
            new_track.values = track.values.iloc[::self.rate]
            Q.append(new_track)
        return Q
    
    def inverse_transform(self, X, copy=None):
        return X

# --- BIáº¾N TOÃ€N Cá»¤C ---
g_parser = None
g_partial_pipe = None

def init_worker():
    """
    Khá»Ÿi táº¡o worker: parse + transform qua cÃ¡c bÆ°á»›c TRÆ¯á»šC ConstantsRemover
    Tráº£ vá» MocapData object (KHÃ”NG pháº£i numpy) Ä‘á»ƒ cÃ³ thá»ƒ fit ConstantsRemover sau
    """
    global g_parser, g_partial_pipe

    g_parser = BVHParser()
    
    # Pipeline chá»‰ tá»›i TRÆ¯á»šC ConstantsRemover
    g_partial_pipe = Pipeline([
        ('param', MocapParameterizer('position')),
        ('rcpn', RootCentricPositionNormalizer()),
        ('delta', RootTransformer('absolute_translation_deltas')),
        # Dá»ªNG á»ž ÄÃ‚Y - chÆ°a cÃ³ ConstantsRemover, Numpyfier, DownSampler
    ])

def worker_parse_and_transform(bvh_path):
    """
    Worker: Parse + transform tá»›i trÆ°á»›c ConstantsRemover
    Tráº£ vá»: MocapData object (DataFrame)
    """
    global g_parser, g_partial_pipe
    
    try:
        # Parse
        parsed_data = g_parser.parse(bvh_path)
        
        # Transform qua cÃ¡c bÆ°á»›c Ä‘áº§u
        # Káº¿t quáº£ váº«n lÃ  MocapData object vá»›i DataFrame
        processed = g_partial_pipe.transform([parsed_data])[0]
        
        return processed
        
    except Exception as e:
        print(f"    âŒ {os.path.basename(bvh_path)}: {e}")
        return None

def main_fit_scaler(parent_dir, folders=None, start=None, end=None):
    """
    CHIáº¾N LÆ¯á»¢C Tá»I Æ¯U:
    1. Multiprocessing: Parse + transform Táº¤T Cáº¢ tá»›i TRÆ¯á»šC ConstantsRemover
       â†’ Káº¿t quáº£: List[MocapData] vá»›i DataFrame
    2. Fit ConstantsRemover trÃªn Táº¤T Cáº¢ MocapData objects (nhanh vÃ¬ khÃ´ng parse)
    3. Apply cÃ¡c bÆ°á»›c cÃ²n láº¡i + fit Scaler
    """
    
    # --- THU THáº¬P FILE BVH ---
    print(f"\nðŸ” Scanning: {parent_dir}")
    
    if folders:
        to_process = folders
    else:
        entries = [d for d in os.listdir(parent_dir) if os.path.isdir(os.path.join(parent_dir, d))]
        numeric = [d for d in entries if re.fullmatch(r"\d+", d)]
        numeric_sorted = sorted(numeric, key=lambda x: int(x))
        
        if start is not None or end is not None:
            s = int(start) if start is not None else None
            e = int(end) if end is not None else None
            to_process = [d for d in numeric_sorted 
                         if (s is None or int(d) >= s) and (e is None or int(d) <= e)]
        else:
            to_process = numeric_sorted

    if not to_process:
        print("âŒ No folders found")
        return

    all_bvh_paths = []
    for folder in to_process:
        src = os.path.join(parent_dir, folder)
        if not os.path.isdir(src):
            continue
        print(f"  Scanning folder {folder}...")
        for fname in os.listdir(src):
            if fname.endswith(".bvh"):
                all_bvh_paths.append(os.path.join(src, fname))

    if not all_bvh_paths:
        print("âŒ No BVH files found")
        return

    print(f"\nâœ… Found {len(all_bvh_paths)} BVH files")

    # --- BÆ¯á»šC 1: MULTIPROCESSING PARSE + PARTIAL TRANSFORM ---
    print(f"\nðŸš€ PASS 1: Multiprocessing parse + transform (to before ConstantsRemover)...")
    
    num_cores = multiprocessing.cpu_count()
    print(f"   Using {num_cores} CPU cores")
    
    with multiprocessing.Pool(processes=num_cores, initializer=init_worker) as pool:
        results = list(tqdm(
            pool.imap(worker_parse_and_transform, all_bvh_paths),
            total=len(all_bvh_paths),
            desc="Pass 1: Parsing & transforming"
        ))
    
    # Lá»c káº¿t quáº£ há»£p lá»‡ (MocapData objects)
    mocap_objects = [r for r in results if r is not None]
    
    if not mocap_objects:
        print("âŒ No valid data after Pass 1")
        return
    
    print(f"âœ… Pass 1 done: {len(mocap_objects)}/{len(all_bvh_paths)} files successful")
    print(f"   Sample shape: {mocap_objects[0].values.shape}")
    
    # --- BÆ¯á»šC 2: FIT ConstantsRemover TRÃŠN Táº¤T Cáº¢ Dá»® LIá»†U ---
    print(f"\nðŸ“Š PASS 2: Fitting ConstantsRemover on ALL {len(mocap_objects)} files...")
    print("   (This is fast - no parsing, just computing std on DataFrames)")
    
    const_remover = ConstantsRemover()
    const_remover.fit(mocap_objects)
    
    print(f"âœ… ConstantsRemover fitted!")
    print(f"   Constant columns found: {len(const_remover.const_dims_)}")
    if const_remover.const_dims_:
        print(f"   Examples: {list(const_remover.const_dims_)[:5]}")
    
    # --- BÆ¯á»šC 3: APPLY CÃC BÆ¯á»šC CÃ’N Láº I + FIT SCALER ---
    print(f"\nðŸ“ˆ PASS 3: Applying remaining steps + fitting Scaler...")
    
    # Apply ConstantsRemover
    print("   Applying ConstantsRemover...")
    after_const = const_remover.transform(mocap_objects)
    print(f"   Shape after removing constants: {after_const[0].values.shape}")
    
    # Apply DownSampler (dÃ¹ng FixedDownSampler)
    print("   Downsampling by factor of 2...")
    downsampler = FixedDownSampler(2)
    downsampled_mocap = downsampler.transform(after_const)
    print(f"   Shape after downsampling: {downsampled_mocap[0].values.shape}")
    
    # Convert to numpy arrays
    print("   Converting to numpy arrays...")
    numpyfier = Numpyfier()
    numpyfier.fit(downsampled_mocap)  # Cáº§n fit Ä‘á»ƒ lÆ°u org_mocap_
    
    # Convert manually to list of arrays (khÃ´ng stack)
    numpy_arrays = [track.values.values for track in downsampled_mocap]
    print(f"   Converted {len(numpy_arrays)} files")
    
    # Fit Scaler
    print("   Fitting ListStandardScaler...")
    scaler = ListStandardScaler()
    scaler.fit(numpy_arrays)
    
    print(f"âœ… Scaler fitted!")
    print(f"   Mean shape: {scaler.data_mean_.shape}")
    print(f"   Std shape: {scaler.data_std_.shape}")
    
    # --- BÆ¯á»šC 4: Táº O VÃ€ LÆ¯U PIPELINE HOÃ€N CHá»ˆNH ---
    print(f"\nðŸ’¾ Creating complete pipeline...")
    
    # Táº¡o DownSampler wrapper Ä‘á»ƒ pipeline hoÃ n chá»‰nh
    # (Trong thá»±c táº¿, downsampling Ä‘Ã£ Ä‘Æ°á»£c Ã¡p dá»¥ng thá»§ cÃ´ng á»Ÿ trÃªn)
    downsampler = DownSampler(2)
    
    full_pipeline = Pipeline([
        ('param', MocapParameterizer('position')),
        ('rcpn', RootCentricPositionNormalizer()),
        ('delta', RootTransformer('absolute_translation_deltas')),
        ('const', const_remover),      # âœ… Fitted
        ('np', numpyfier),              # âœ… Fitted
        ('down', downsampler),          # âš ï¸ Wrapper (manual downsampling applied)
        ('stdscale', scaler)            # âœ… Fitted
    ])
    
    output_filename = "global_pipeline.pkl"
    joblib.dump(full_pipeline, output_filename)
    
    print(f"\nðŸŽ‰ SUCCESS!")
    print(f"   Saved: {output_filename}")
    print(f"   Files processed: {len(mocap_objects)}/{len(all_bvh_paths)}")
    print(f"   Input features: {mocap_objects[0].values.shape[1]}")
    print(f"   After ConstantsRemover: {after_const[0].values.shape[1]}")
    print(f"   Final features: {scaler.data_mean_.shape[0]}")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description='Fit global scaler (OPTIMIZED multiprocessing)'
    )
    parser.add_argument('--parent-dir', type=str, required=True)
    parser.add_argument('--folders', type=str)
    parser.add_argument('--start', type=int)
    parser.add_argument('--end', type=int)
    args = parser.parse_args()

    folders_list = None
    if args.folders:
        folders_list = [f.strip() for f in args.folders.split(',') if f.strip()]

    main_fit_scaler(
        args.parent_dir,
        folders=folders_list,
        start=args.start,
        end=args.end
    )