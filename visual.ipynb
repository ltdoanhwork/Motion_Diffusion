{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1159fde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_factor_path = \"./checkpoints/beat/VQKL_Enhanced_BEAT/scale_factor.txt\" \n",
    "scale_val = None\n",
    "\n",
    "print(f\"Äang Ä‘á»c giÃ¡ trá»‹ tá»« {scale_factor_path}...\")\n",
    "\n",
    "if os.path.exists(scale_factor_path):\n",
    "    with open(scale_factor_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if line.strip().startswith(\"scale_factor\"):\n",
    "                parts = line.split('=')\n",
    "                if len(parts) > 1:\n",
    "                    scale_val = parts[1].strip() \n",
    "                    print(f\"--> TÃ¬m tháº¥y scale_factor: {scale_val}\")\n",
    "                break\n",
    "else:\n",
    "    print(f\"Lá»–I: KhÃ´ng tÃ¬m tháº¥y file {scale_factor_path}. Vui lÃ²ng kiá»ƒm tra láº¡i Ä‘Æ°á»ng dáº«n.\")\n",
    "    exit(1) \n",
    "\n",
    "if scale_val is None:\n",
    "    print(\"Lá»–I: KhÃ´ng Ä‘á»c Ä‘Æ°á»£c giÃ¡ trá»‹ scale_factor trong file.\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b33088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "PYMO_DIR = os.path.join('/home/serverai/ltdoanh/Motion_Diffusion', 'datasets', 'pymo')\n",
    "if PYMO_DIR not in sys.path:\n",
    "    sys.path.insert(0, PYMO_DIR)\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "class Config:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    checkpoints_dir = \"./checkpoints\"\n",
    "    dataset_name = 'beat'\n",
    "    \n",
    "    vqkl_name = 'VQKL_Enhanced_BEAT'  \n",
    "    \n",
    "    diffusion_name = 'vqkl_diff_sobolev_phys' \n",
    "    \n",
    "    checkpoint_name = 'best_model.pt' \n",
    "    result_dir = \"./results\"\n",
    "\n",
    "    scale_factor = scale_val\n",
    "\n",
    "    latent_dim = 512\n",
    "    num_layers = 8\n",
    "    num_heads = 8\n",
    "    ff_size = 1024\n",
    "    dropout = 0.2    \n",
    "    no_eff = False  \n",
    "    freeze_vqkl = True\n",
    "\n",
    "    use_kl_posterior = True \n",
    "\n",
    "    diffusion_steps = 1000\n",
    "    noise_schedule = 'linear' \n",
    "    \n",
    "    sampler = 'ddim'\n",
    "    ddim_eta = 0.0 \n",
    "    \n",
    "    guidance_scale = 5.0\n",
    "\n",
    "class ClassifierFreeGuidanceModel(torch.nn.Module):\n",
    "    def __init__(self, model, guidance_scale):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.guidance_scale = guidance_scale\n",
    "\n",
    "    def forward(self, x, t, y=None):\n",
    "        if self.guidance_scale == 1.0:\n",
    "            return self.model(x, t, y)\n",
    "\n",
    "        texts = y['text']\n",
    "        lengths = y['length']\n",
    "        batch_size = len(texts)\n",
    "\n",
    "        uncond_texts = [\"\"] * batch_size\n",
    "\n",
    "        x_in = torch.cat([x, x], dim=0)\n",
    "\n",
    "        t_in = torch.cat([t, t], dim=0)\n",
    "\n",
    "        y_in = {\n",
    "            'text': texts + uncond_texts,  \n",
    "            'length': lengths + lengths   \n",
    "        }\n",
    "\n",
    "        model_output = self.model(x_in, t_in, y=y_in)\n",
    "\n",
    "        eps_cond, eps_uncond = model_output.chunk(2, dim=0)\n",
    "\n",
    "        eps = eps_uncond + self.guidance_scale * (eps_cond - eps_uncond)\n",
    "\n",
    "        return eps\n",
    "\n",
    "from models.vq_diffusion import create_vqkl_latent_diffusion, VQKLLatentDiffusionWrapper\n",
    "from models.gaussian_diffusion import (\n",
    "    GaussianDiffusion, get_named_beta_schedule,\n",
    "    ModelMeanType, ModelVarType, LossType\n",
    ")\n",
    "\n",
    "def load_models(args):\n",
    "    print(\"\\nLoading model...\")\n",
    "\n",
    "    model = create_vqkl_latent_diffusion(\n",
    "        dataset_name=args.dataset_name,\n",
    "        vqkl_name=args.vqkl_name,\n",
    "        checkpoints_dir=args.checkpoints_dir,\n",
    "        device=args.device,\n",
    "        freeze_vqkl=args.freeze_vqkl,\n",
    "        scale_factor=args.scale_factor,\n",
    "\n",
    "        use_kl_posterior=args.use_kl_posterior, \n",
    "        \n",
    "        latent_dim=args.latent_dim,\n",
    "        num_layers=args.num_layers,\n",
    "        num_heads=args.num_heads,\n",
    "        ff_size=args.ff_size,\n",
    "        dropout=args.dropout,\n",
    "        no_eff=args.no_eff\n",
    "    )\n",
    "\n",
    "    checkpoint_path = os.path.join(\n",
    "        args.checkpoints_dir,\n",
    "        args.dataset_name,\n",
    "        args.diffusion_name, \n",
    "        'model',\n",
    "        args.checkpoint_name\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "\n",
    "    print(f\"Loading checkpoint: {checkpoint_path}\")\n",
    "    \n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    except Exception:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)\n",
    "\n",
    "    try:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'], strict=True)\n",
    "        print(\"Checkpoint loaded (strict)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Strict loading failed: {e}\")\n",
    "        print(\"Retrying with strict=False (Missing keys might be acceptable usually VQ parts)...\")\n",
    "        model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "        print(\"Checkpoint loaded (loose)\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    betas = get_named_beta_schedule(args.noise_schedule, args.diffusion_steps)\n",
    "    diffusion = GaussianDiffusion(\n",
    "        betas=betas,\n",
    "        model_mean_type=ModelMeanType.EPSILON, \n",
    "        model_var_type=ModelVarType.FIXED_SMALL,\n",
    "        loss_type=LossType.RESCALED_L1, \n",
    "        rescale_timesteps=False\n",
    "    )\n",
    "\n",
    "    print(\"Model and diffusion ready\")\n",
    "    return model, diffusion\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_motion(model, diffusion, text_prompts, lengths, args, mean, std):\n",
    "    \"\"\"Generate motion from text with CFG\"\"\"\n",
    "    \n",
    "    print(f\"\\n Generating motion for {len(text_prompts)} prompts...\")\n",
    "    print(f\"   Guidance Scale: {args.guidance_scale}\")\n",
    "\n",
    "    raw_wrapped_model = VQKLLatentDiffusionWrapper(model)\n",
    "    cfg_model = ClassifierFreeGuidanceModel(raw_wrapped_model, args.guidance_scale)\n",
    "    \n",
    "    B = len(text_prompts)\n",
    "    T_latent = model.num_frames\n",
    "    latent_dim = getattr(model.vqkl, 'embed_dim', model.vqkl.code_dim)\n",
    "    \n",
    "    shape = (B, T_latent, latent_dim)\n",
    "    \n",
    "    model_kwargs = {\n",
    "        'y': {\n",
    "            'text': text_prompts,\n",
    "            'length': lengths,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if args.sampler == 'ddim':\n",
    "        print(f\"Sampling with DDIM (eta={args.ddim_eta})...\")\n",
    "        latent_samples = diffusion.ddim_sample_loop(\n",
    "            cfg_model,\n",
    "            shape,\n",
    "            clip_denoised=False,\n",
    "            model_kwargs=model_kwargs,\n",
    "            device=args.device,\n",
    "            progress=True,\n",
    "            eta=args.ddim_eta\n",
    "        )\n",
    "    else:\n",
    "        print(\"Sampling with DDPM...\")\n",
    "        latent_samples = diffusion.p_sample_loop(\n",
    "            cfg_model, \n",
    "            shape,\n",
    "            clip_denoised=False,\n",
    "            model_kwargs=model_kwargs,\n",
    "            device=args.device,\n",
    "            progress=True\n",
    "        )\n",
    "    \n",
    "    print(f\"Latent shape: {latent_samples.shape}\")\n",
    "    \n",
    "    print(\"Decoding to motion...\")\n",
    "    motion = model.decode_from_latent(latent=latent_samples)\n",
    "    \n",
    "    print(f\"Motion shape: {motion.shape}\")\n",
    "    \n",
    "    motion_np = motion.cpu().numpy()\n",
    "    if motion_np.shape[1] == 264:\n",
    "        motion_np = motion_np.transpose(0, 2, 1)\n",
    "    \n",
    "    motion_denorm = motion_np * std + mean\n",
    "    \n",
    "    print(f\" Generation complete: {motion_denorm.shape}\")\n",
    "    \n",
    "    return motion_denorm\n",
    "\n",
    "def main():\n",
    "    args = Config()\n",
    "    stats_path = os.path.join('/home/serverai/ltdoanh/Motion_Diffusion', 'global_pipeline.pkl')\n",
    "    \n",
    "    print(f\"Loading stats from: {stats_path}\")\n",
    "    pipeline = joblib.load(stats_path)\n",
    "    scaler = pipeline.named_steps['stdscale']\n",
    "    mean = scaler.data_mean_\n",
    "    std = scaler.data_std_\n",
    "    print(f\" Stats loaded (dim={len(mean)})\")\n",
    "\n",
    "    model, diffusion = load_models(args)\n",
    "    \n",
    "    text_prompts = [\n",
    "        \"the first thing i like to do on weekends is relaxing and i'll go shopping if i'm not that tired\",\n",
    "    ]\n",
    "    \n",
    "    lengths = [45] * len(text_prompts)\n",
    "    \n",
    "    motion = generate_motion(model, diffusion, text_prompts, lengths, args, mean, std)\n",
    "    \n",
    "    os.makedirs(args.result_dir, exist_ok=True)\n",
    "    save_path = os.path.join(args.result_dir, f'vqkl_diffusion_hierarchical_v3.npy')\n",
    "    np.save(save_path, motion)\n",
    "    \n",
    "    print(f\" Saved to: {save_path}\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"INFERENCE COMPLETED!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dce4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# Setup path\n",
    "PYOM_DIR = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/pymo\"\n",
    "if PYOM_DIR not in sys.path:\n",
    "    sys.path.insert(0, PYOM_DIR)\n",
    "\n",
    "from pymo.parsers import BVHParser\n",
    "from pymo.viz_tools import draw_stickfigure, draw_stickfigure3d\n",
    "\n",
    "print(\"âœ… Imports successful!\")\n",
    "\n",
    "# ==========================================\n",
    "# 1. LOAD & FIX PIPELINE (ADVANCED FIX)\n",
    "# ==========================================\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline_path = \"/home/serverai/ltdoanh/Motion_Diffusion/global_pipeline.pkl\"\n",
    "print(f\"ðŸ“¦ Loading pipeline from: {pipeline_path}\")\n",
    "pipeline = joblib.load(pipeline_path)\n",
    "\n",
    "# Load Reference BVH\n",
    "ref_bvh_path = \"/home/serverai/ltdoanh/Motion_Diffusion/datasets/2_scott_0_55_55.bvh\"\n",
    "\n",
    "if os.path.exists(ref_bvh_path):\n",
    "    print(f\"ðŸ”§ Processing reference BVH: {os.path.basename(ref_bvh_path)}\")\n",
    "    parser = BVHParser()\n",
    "    ref_data = parser.parse(ref_bvh_path)\n",
    "    \n",
    "    steps_before_np = []\n",
    "    np_found = False\n",
    "    \n",
    "    for name, step in pipeline.steps:\n",
    "        if name == 'np': \n",
    "            np_found = True\n",
    "            break\n",
    "        steps_before_np.append((name, step))\n",
    "    \n",
    "    if np_found and steps_before_np:\n",
    "        print(f\"   Running reference data through pre-processing steps: {[n for n, _ in steps_before_np]}...\")\n",
    "        \n",
    "        pre_pipeline = Pipeline(steps_before_np)\n",
    "        \n",
    "        processed_ref = pre_pipeline.fit_transform([ref_data])\n",
    "        \n",
    "        template_data = processed_ref[0]\n",
    "        \n",
    "        print(f\"   Template shape after preprocessing: {template_data.values.shape}\") \n",
    "        \n",
    "        if template_data.values.shape[1] == 264:\n",
    "            # GÃ¡n template Ä‘Ãºng shape vÃ o Numpyfier\n",
    "            pipeline.named_steps['np'].org_mocap_ = template_data\n",
    "            print(\"  Skeleton structure injected with CORRECT dimensions (264)!\")\n",
    "        else:\n",
    "            print(f\" Warning: Processed shape is {template_data.values.shape[1]}, but model output is 264.\")\n",
    "            print(\"   Visualization might still fail.\")\n",
    "            \n",
    "    else:\n",
    "        print(\" Could not isolate steps before Numpyfier.\")\n",
    "        pipeline.named_steps['np'].org_mocap_ = ref_data\n",
    "\n",
    "else:\n",
    "    print(f\" Reference BVH not found at {ref_bvh_path}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5c8a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "npy_path = \"/home/serverai/ltdoanh/Motion_Diffusion/results/vqkl_diffusion_hierarchical_v3.npy\"\n",
    "print(f\"\\n Loading motion: {npy_path}\")\n",
    "motion_data = np.load(npy_path)\n",
    "\n",
    "if motion_data.ndim == 3:\n",
    "    motion_data = motion_data[0]\n",
    "\n",
    "print(f\"   Raw Input Shape: {motion_data.shape}\")\n",
    "print(f\"   Raw Input Range: {motion_data.min():.2f} to {motion_data.max():.2f}\")\n",
    "\n",
    "scaler = pipeline.named_steps['stdscale']\n",
    "mean = scaler.data_mean_\n",
    "std = scaler.data_std_\n",
    "\n",
    "if np.abs(motion_data).max() > 100:\n",
    "    print(\" Detected large values (Real-scale data). Re-normalizing for pipeline compatibility...\")\n",
    "    motion_data = (motion_data - mean) / std\n",
    "    print(f\"   Normalized Range: {motion_data.min():.2f} to {motion_data.max():.2f}\")\n",
    "else:\n",
    "    print(\" Data seems to be already normalized.\")\n",
    "\n",
    "print(\"\\n Performing inverse transform...\")\n",
    "try:\n",
    "    reconstructed = pipeline.inverse_transform([motion_data])\n",
    "    mocap_data = reconstructed[0]\n",
    "    \n",
    "    print(\"    Inverse transform successful!\")\n",
    "    \n",
    "    df = mocap_data.values\n",
    "    print(f\"   Final Motion Range (cm): {df.min().min():.2f} to {df.max().max():.2f}\")\n",
    "    \n",
    "    frame = 0\n",
    "    print(f\"\\n Visualizing Frame {frame}...\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 8)) \n",
    "    draw_stickfigure(mocap_data, frame=frame)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title(f\"Reconstructed Motion - Frame {frame}\")\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n ERROR during visualization: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
